[{"content":"24岁的一点感想 总结自己总是离不开几件事，生活，友谊，工作，感情，健康和对人类的贡献\u0026hellip;\u0026hellip;每个人心中的排序都不同，同龄人里，有的忙于工作， 有的还在读书，甚至有的已经在家庭和婚姻中历练自我。\n而我的2024年过的太冗长，杂乱且难以整理。\n感情方面上并没有诞生出什么恋情。 想到四叔说的，一直一个人就犹如建立了一个个人的王国， 这个王国的律法系统且保守，国王专制，接受他人往往带着审视和批判，是一个精神上的独裁者，最后走向一个人的朝圣。 而随着年龄增大，周围的朋友开始有自己的生活，开始谈恋爱，见家长，考公考事业单位，努力赚钱，自己就会有一种走入孤独的感觉。 因为一旦加入到这样一个集体，必然要交付一部分个性，而这个交付的过程是一个允许自己主体性被侵犯的过程。 而大部分人选择的谈恋爱来对抗孤独的方式，也正是如此。 虽然来到了一个新的城市，但是好在还有几个交心的老朋友，他们也不属于以上的集体，虽然分别甚远，但也能消解寂寞。\n今年的工作也许是最颠簸的一年吧。新的工作说不上喜欢，但也说不上讨厌。 离开了之前的象牙塔，没有了之前的部长，同事，才发现之前一直被保护的太好，一个人的探险之路并不简单。 没有太过幸福的事情发生，也没有遇到什么毁灭性打击，我只能去捏造一些有趣的事情让自己略有活力地活着，来逃避荒谬的生活。\n下半年也不怎么玩游戏了，今年也就玩了山河旅探，bangbang，重温了两部刺客信条。买了一年的王国之泪到现在也没通关。 被生活的疲惫感渐渐侵蚀，为了抵抗这种虚无，为了心里的那一点愿望，买了一部四块二，开始了吉他之旅，买了一辆手波小车，开始驾驶之路。 在干这些事的时候，物理意义上扎实的接触，操作与协调感，能对抗虚无，抽离和格格不入。\nAnyway，今年还是做了很多不一样的事情，一个人去日本出差，旅行，给好朋友介绍工作，一个人跑到广州上班，开始了新的生活 也做了很多没想到的事，比如撞车了，比如买车了。 也看了不少精彩的演出，去年年初不晚封箱演出时世界一分为二的感动，美好药店的新奇，平野绫的回忆满满，藤井风的精彩，还和新认识的同事看了康姆士。\n我的心境从大三开始确确实实改变，到现在扎扎实实地坚定下来，现在的我不允许任何无意义的事情对我制造焦虑干扰我的生活，也不会在这样不知要流向何处的工作里白白消耗自己的才能和激情。 新的一岁在狭窄的出租屋里码字，但是无妨，麻雀虽小但五脏俱全，又何陋之有呢。\n","permalink":"https://fishdel.github.io/posts/2024%E5%B9%B4%E7%BB%88%E5%BF%8F%E6%82%94%E5%BD%95/","summary":"\u003ch3 id=\"24岁的一点感想\"\u003e24岁的一点感想\u003c/h3\u003e\n\u003cp\u003e总结自己总是离不开几件事，生活，友谊，工作，感情，健康和对人类的贡献\u0026hellip;\u0026hellip;每个人心中的排序都不同，同龄人里，有的忙于工作，\n有的还在读书，甚至有的已经在家庭和婚姻中历练自我。\u003c/p\u003e","title":"2024年终忏悔录"},{"content":" 生成可执行文件的主要流程：预编译、编译（C—\u0026gt;汇编—\u0026gt;binary）、链接。\n编译只是把各个.c和.s文件编译成对应的.o文件。然后需要链接器将各个.o文件链接为一个可执行文件。\n.lsl,.ld文件：属于一种linkfile，规定如何把输入文件内的section放到输出文件，以及控制输出文件内各部分在程序地址空间内的布局。\n.text：是程序代码段，用于存放函数代码\n.data: 全局变量并且初始值不为0\n.bss：未初始化的全局变量或者初始化为0的全局变量。\n标准C语言的 section 前缀主要包括： .bss/.data/.rodata/.text。 英飞凌芯片对应的将数据划分为 near类型/far类型。对应的前缀为：\n1）带 “z”的near类型数据：.zbss/.zdata/.zrodata 2）标准的 far 类型数据： .bss/.data/.rodata\n16位系统中会有近指针，远指针。\nnear定义的标号表示段内近跳转，近调用的地址。near指针的长度是16位的，所以可指向的地址范围是64K字节，通常说near指针的寻址范围是64K。\nfar定义的标号表示段间远跳转，远调用的地址。far指针的长度是32位，含有一个16位的基地址和16位的偏移量，将基地址乘以16后再与偏移量相加，实际上far指针是20位，取值范围为0x00000～0xFFFFF。所以far指针的寻址范围是1M字节，超过了一个段64K的容量。\n编译器将.c文件编译成了对应的.o文件后，每个.o文件中会包含了数量不同的段，.text段、.data段、.bss段、.vectors段、.ram_code段。链接器将各个.o文件中的代码按照不同的段，链接成一个文件。所有的.text链接到一起，所有的.data链接到一起，所有的.bss段链接到一起，所有的.vectors段链接到一起，所有的.ram_code段链接到一起。\nENTRY(_START) ，ENTRY语法说明，用户程序最先从START处开始运行，定义应用程序的入口点，相当于告诉连接器的启动地址，即输出文件中的第一条可执行指令，_START一般在.s文件定义。\nFlash地址分配可以用链接脚本来控制，代码共分为三个部分，Tricore的APP，HSM的BootLoader和HSM的APP，三个工程的链接脚本要相互配合好，这样编译出来的代码不会互相覆盖。\n例子：\n代码内声明为Static类型的变量，其运行空间在RAM区域加载空间在ROM区域，编译后属性为.data/.bss,其运行空间可通过以下方式设定去运行地址与加载地址。\n设定运行地址\ngroup P_REW_DIS_TSK2MS (ordered, align = 4, run_addr=P_REW_DIS_TSK2MS_ORG) { section \u0026#34;P_REW_DIS_TSK2MS\u0026#34; (size=P_REW_DIS_TSK2MS_LEN, attributes=r, fill=0x00) { select \u0026#34;.text.*.P_REW_DIS_TSK2MS.text\u0026#34;; } } 设定加载地址\ngroup P_REW_EXE_EXD (ordered, contiguous, align=4, fill=0, load_addr=P_REW_EXE_EXD_ORG) { group (ordered, contiguous, align=4) { section \u0026#34;P_REW_EXE_EXD\u0026#34; (size=P_REW_EXE_EXD_LEN, attributes=r, fill=0x00) { select \u0026#34;.text.*.P_REW_EXE_EXD.text\u0026#34;; } } } ","permalink":"https://fishdel.github.io/posts/%E5%B5%8C%E5%85%A5%E5%BC%8F.ld.lsl%E7%AD%89%E9%93%BE%E6%8E%A5%E6%96%87%E4%BB%B6%E4%BD%9C%E7%94%A8/","summary":"\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e生成可执行文件的主要流程：预编译、编译（C—\u0026gt;汇编—\u0026gt;binary）、链接。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e编译只是把各个.c和.s文件编译成对应的.o文件。然后需要链接器将各个.o文件链接为一个可执行文件。\u003c/p\u003e","title":"嵌入式.ld,.lsl等链接文件作用"},{"content":"函数指针与回调函数 函数指针 作用：硬件驱动程序和用户应用程序相互分开，硬件驱动程序提供API函数，用户应用程序将函数作为回调函数的方式进行使用。 回调机制的好处是，在程序执行期间可以动态更改被调用 回调函数：作为参数传递给另一个函数的函数，接受回调作为参数的函数预计会在某个时间点执行它。\n使用方法：函数名带括号就是函数指针，没括号是指针函数\n把函数A的地址赋给一个函数指针p，以p为参数，赋值给函数B，函数B通过p调用A\n函数指针是一个指向函数的指针变量，32位单片机中存放大小为4字节的地址\nint *(*pfunc)(int,int*,void*)*; typedef int *(*pfunc)(int,int*,void*)*; 一般用typedef定义函数指针类型。 typedef和define区别：typedef 语句是在编译过程中被解析的，而#define是在编译之前的预处理过程中被解析的\ntypedef uint8_t (*func_ptr) (void);，就相当于把uint8_t (*) (void); 定义成了另一个别名 func_ptr了。这个func_ptr就表示了函数指针类型。\ntypedef int *(*pfunc)(int,int*,void*)*;相当于把int（*）(int,int*,void*)；定义成别名pfunc\n相当于把 int(*)(int，int*，void*)取一个别名pfunc\n回调函数 作用: 虽然只是函数指针的应用，但通过函数指针的当时区分传入不同的函数入口地址去执行不同的函数，可以节省单片机的ram和rom的开支。\n实现： 例如将callback函数看做底层函数，main函数看成上层应用函数，现在有上层方法A,上层方法B,在使用底层callback函数进行功能设计时，转回去拿到具体的方法A,方法B，再把结果返回main,此时方法A,B就是回调函数。\n举例： EcuM_AL_Reset是回调函数，属于对象外的，该函数被调用后，会MCAL标准函数Mcu_PerformReset来重启CPU\n钩子函数 hook函数实际也是函数指针，因为也是用户定义的，也可以理解为回调函数，二者之间区别主要是回调函数主要是目的处理，hook函数主要是过程监控。\n定义函数fun1，fun2，再定义一个pfun函数指针，在main函数里通过pfun指向fun1，fun2，这个过程称为挂钩子。在不确定main函数的功能的情况下，留下函数指针作为接口，挂上不同的函数就可以完成不同的功能。\nHOOK函数相当于一个监视器，捕获消息队列中需要的内容去处理\n挂钩子的过程也称为注册。在注册函数中，使用者把自己编写的钩子函数挂在已经声明的函数指针上，这个注册函数的参数是要挂上的钩子函数的地址，即函数指针。\n举例：Autosar中的hook函数机制\n1）由操作系统调用，在特定的context中取决于操作系统的实现 2）高于所有task 3）不被第二类中断程序打断。 4）属于操作系统的一部分 5）可以由用户定义功能\n截获行为的函数调用（相当于监控器）。所有特定于应用程序的Hook函数（Startup, Shutdown and Error）必须返回（不接受阻塞或无限循环）。\n当应用程序或操作系统在出现严重错误时请求系统关闭时调用 (ShutdownHook)\n在Shutdown OS时，操作系统将调用钩子函数ShutdownHook，勾到EcuM_ShutDown那边，然后关闭（如下图： 多核系统关闭过程）。用户通常可以在ShutdownHook中自由定义任何系统行为\nFUNC(void, OS_SHUTDOWNHOOK_CODE) ShutdownHook(StatusType Fatalerror) { if( GetCoreID() == OS_CORE_ID_MASTER ) { EcuM_Shutdown(); } } 位操作 1. 移位操作 ①\u0026laquo; 左移 ：左移几位就把左边的数去掉几位，右边补0；相当于2^5-\u0026gt;2^6\n源操作数 * 2的N次方（N取决于移动的位数） = 移动后的结果。\n②\u0026raquo;右移：右移几位就把右边的数去掉几位，左边补0\n源操作数 / 2的N次方（N取决于移动的位数） = 移动后的结果(*只取整数部分*)\n算术左移和逻辑左移相同\n​ 算术右移符号位要一起移动，左边补符号位，11100算术右移一位为11110\n2.逻辑运算 \u0026amp;(与) 和0一起使用 可以清零 |(或) 可以置1 ^(异或) 3.举例 将特定数置1 0xf8 把第2位到第6位置1\n1111 _1000\n2到6 1_1111 =0x1f\n从第二位开始置1 0x1f\u0026laquo;2 = 111_1100\n第a位到第b位置1，（b-a+1）转为16进制\n从哪一位开始置1 则左移多少位，\u0026laquo;a\n把第4位到第8位和第23到25位同时置1\n((0x1f \u0026laquo;4) | (0x7\u0026laquo;23))\n将特定数置0 先置1 再按位取反\n~((0x1f \u0026laquo;3)\n操作 表达式 设置整型数a的bit4 a|=(1\u0026laquo;4) 设置整型数a的bit4~bit7 a|=（(0x1f）\u0026laquo;4） 清除整型数a的bit15 a\u0026amp;=((~a\u0026laquo;15)) 代码举例 清除特定位的值 VOID\tClear_data( const UCHAR indat ) { UCHAR\tbyte_id;\tUCHAR\tbit_dat;\tUCHAR\tbit_id;\tUCHAR\tbit_dat_inv;\t/* indat/8 取得其数组id*/ byte_id\t= (UCHAR)If_Shift_LR( (ULONG)indat, ZSHIFT3 ); /* indat%8 取得其bit位*/ bit_id\t= (UCHAR)( indat \u0026amp; ZMASK_07 );\tbit_dat\t= (UCHAR)If_Shift_LL( ZMASK_BIT0, (ULONG)bit_id ); bit_dat_inv = ~bit_dat; /*\u0026amp;= 清除该位*/ array[ byte_id ]\t\u0026amp;= bit_dat_inv;\t} memcpy函数 3.以2Byte为单位进行复制处理，从起始地址开始每2个Byte拷贝到目标地址，n是拷贝次数，如果16个字节，以2Byte为单位复写，n=8，以4Byte为单位复写，n=4 VOID\tIf_Memcpy2( VOID* const dst, const VOID* const src, SIZE_T n ) { USHORT*\tdst_p = (USHORT*)dst; const\tUSHORT*\tsrc_p = (const USHORT*)src; /*n = n/2 如果是4个byte为单位进行复写，IfCfc_Shift_LR( (ULONG)n, 2 );*/ n = (SIZE_T)If_Shift_LR( (ULONG)n, 1 ); if ( n != 0 ) { do { *(dst_p) = *(src_p); dst_p++; src_p++; n--; } while ( n \u0026gt; 0 ); } } 数据类型与关键字 enum枚举 注意点：\n1.在同一个作用域能不能出现重名的枚举常量名；\n虽然定义了两个枚举类型 enum1，enum2，但如果其成员常量名相同则会报错\n2.不可以定义相同的变量但是可以定义相同的值；\n`typedef enum { enumA=0, enumB=4, enumC=5,\tenmuD=6, enmuE=6, }A_enum;` union联合体 注意点\n内存空间相同但储存不同的数据类型，并不是同时储存，而是一次只能存储一种数据类型。 联合体的大小都相等，每个联合可以储存各种数据类型。共用体的长度为其最大的成员的长度。 用途： 数据的格式在不同场合下不同时，节省内存。 const关键字 可以保护被修饰的东西，防止意外修改，增强程序的健壮性\n编译器通常不为普通const常量分配存储空间，而是将它们保存在符号表中，这使得它成为一个编译期间的常量，没有了存储与读内存的操作，使得它的效率也很高。\ninline关键字 关键字inline 必须与函数定义体放在一起\n只适合函数体内代码简单的函数数使用\nstatic inline inline函数不能在两个不同的文件中出现，一个.h不能被两个不同的文件包含，一个inline在不同的.C里面生成了不同的实例\nc文件中的仅inline函数是不内联的，因为没有static，编译会认为它是全局的，因此像普通函数一样编译了。 加入static，这样内部调用函数时，会内联，而外部调用该函数时，则不会内联。\nstatic关键字 修饰变量：\n修饰全局变量：仅对当前文件可见，其他文件不可访问，其他文件可以定义与其同名的变量\n修饰局部变量：普通局部变量存储在栈空间，编译器不会初始化；修饰之后的局部变量，即使在声明时未赋初值，编译器也会把它初始化为0。且静态局部变量存储于进程的全局数据区，即使函数返回，它的值也会保持不变\n修饰函数：\n静态函数只能在声明它的文件中可见，其他文件不能引用该函数\nvolatile关键字 每次读取数据，必须在内存上取，而不是使用保存在寄存器或者cache里的备份（直接取内存原始地址）\n没有声明的话，对一个变量进行多次赋值，没有生成之间汇编代码，直接取最后的值寻址赋值给该变量；声明之后每个变量的赋值都形成了汇编代码，没有被优化。\n易变的 多线程的程序，共同访问内存时多个程序可以操纵这个变量；和外部设备的状态对应，通过驱动程序和中断改变该变量的值，程序并不知道\n用途：\n并行设备的硬件寄存器（状态寄存器），每次对它的读写都可能有不同意义\n一个中断服务子程序中会访问到的非自动变量；当变量在触发某中断程序中修改，对于编译器，主函数里没有修改这个变量，可能只执行一次从内存到某寄存器的读操作，而后每次只会从该寄存器中读取变量副本，使得中断程序的操作做了跟没做一样。\n多线程任务中的共享变量。\n一个参数可以既是const又是volatile——只读的状态寄存器。它是volatile因为它可能被意想不到地改变。它是const因为程序不应该试图去修改它。\n一个volatile的指针——当一个中断服务子程序修改一个指向一个主函数的指针时\n宏定义 用途\n防止头文件被重复包含\n重新定义一些类型\n得到指定地址\n得到指定地址上的一个字节\n#define MEM_B( x ) ( *( (byte *) (x) ) ) #define MEM_W( x ) ( *( (word *) (x) ) ) Translation unit 在函数块外部名字声明（函数和变量）若只能在一个已知的translation unit是可见的，称为内部链接。他们对于linker（连接器）是不可见。若声明的函数或变量对于其他的目标文件是看见的，则称为具有外部连接，对于linker是可见的。\n编译的基本单元是.c文件\n","permalink":"https://fishdel.github.io/posts/%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88%E4%BD%8D%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","summary":"\u003ch1 id=\"函数指针与回调函数\"\u003e函数指针与回调函数\u003c/h1\u003e\n\u003ch2 id=\"函数指针\"\u003e函数指针\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e作用\u003c/strong\u003e：硬件驱动程序和用户应用程序相互分开，硬件驱动程序提供API函数，用户应用程序将函数作为回调函数的方式进行使用。\n\u003cem\u003e回调机制的好处是，\u003cem\u003e\u003cstrong\u003e在程序执行期间可以动态更改被调用\u003c/strong\u003e\u003c/em\u003e\u003c/em\u003e\n\u003cem\u003e回调函数：作为参数传递给另一个函数的函数，接受回调作为参数的函数预计会在某个时间点执行它。\u003c/em\u003e\u003c/p\u003e","title":"函数指针，位操作和数据类型"},{"content":"CAN Interface (MCMCAN) 1.模块 有三个模块CAN0,CAN1,CAN2，一般只使用CAN0,CAN0的模块比较全面。\nCAN0有4个CAN node，Message RAM 一共32Kbyte，FIFO,buffer，register在此处开辟。\n每个node通过M_CAN来实现，并且都支持CAN FD；都有两个引角，TXD和RXD\n2.初始化 具体步骤\n设置CCCRi.INIT开始初始化（软硬件复位或通过bus off）此时配置寄存器不会被改变\n当该位被置起，即CCCRi.INIT (i=0-3) 置1，can总线之间的通信停止，CAN node停止收发，CAN总线输出TXD为隐性（高）。错误管理逻辑EML的计数器保持不变。\n重置 CCCRi (i=0-3).INIT 完成软件初始化\n只有当位CCCRi.INIT和CCCRy.CCE都设置为1时，才能启用对M_CAN配置寄存器的访问\nCCCRi.CCE只有在CCCRi.INIT=\u0026lsquo;1\u0026rsquo;时才能被set/reset。\n当CCCRi.INIT被清除时，CCCRi.CCE自动复位。\n3. 时钟控制单元 MCMCAN模块时钟输入连接到时钟控制单元（CCU）。并且MCMCAN有两种时钟源，fsyn用于寄存器和RAM接口，fasyn用在CAN FD上。CLC设置为全局模块寄存器提供其时钟。\n为了向M_CAN节点提供相应的时钟，必须设置MCR.CLKSELi寄存器。异步时钟以及每个M_CAN节点的同步时钟可以通过MCR.CLKSELi寄存器位字段打开/关闭。\n4. 中断 每个module都有16个中断线INT_O0~INT_O15\n中断分组已经设置，存在16个中断节点。中断组可以通过GRINT1i（i=0-3），**GRINT2i (i=0-3)**自由分配给节点\n中断脉冲的生成是基于 寄存器 (IRi (i=0-3) TTIR0) 和 (**IEi (i=0-3)**and TTIE0)，中断标志和中断使能之间的关系是\u0026amp;。中断flag会通过向 CANn_IRi bit.写‘1’进行reset。\n如果相应中断使能寄存器中的相关中断使能位使能（IEi（i＝0-3）、NTRTRi（i＝0-2）、TEIE和TTIE0），MCMCAN 模块里16条中断输出线INT_On的其中一条使用GRINT1i (i=0-3) 和 GRINT2i (i=0-3)可以产生中断脉冲。\n也就是说通过配置interrupt line选择对应的SRC节点（line和SRC_INT一一对应）配置CAN中断\nIfx_SRC_SRCR INT[16]; } Ifx_SRC_CAN_CAN; volatile Ifx_SRC_SRCR *IfxCan_getSrcPointer(Ifx_CAN *can, IfxCan_InterruptLine interruptLine) { IfxCan_Index canIndex = IfxCan_getIndex(can); Ifx_SRC_CAN_CAN *const srcCanBaseAddress[IFXCAN_NUM_MODULES] = { \u0026amp;MODULE_SRC.CAN.CAN[0], \u0026amp;MODULE_SRC.CAN.CAN[1], }; return \u0026amp;(srcCanBaseAddress[canIndex]-\u0026gt;INT[interruptLine]); } 5. CAN FD 区别：传输速率不同、数据长度不同、帧格式不同、ID长度不同\nCCCRi.FDOE (i=0-3) 是收发CAN FD frame功能启用位\n时间延迟补偿：为了实现比发射机延迟更短的数据相位比特时间，引入了延迟补偿。在没有变送器延迟补偿的情况下，CAN FD帧的数据阶段的比特率受到变送器延迟的限制\nCAN收发器的信号从TX出发到总线到RX有时延。CAN FD速率可变，BRS位进行控制，波特率更高\n6. CAN node的收发 接受到的Rx Frame以Rx buffer的形式存放在Message RAM，最多存放64个\n首地址通过寄存器进行配置\n\\#define SMCMCAN0_RXBC0_INIT ( 0x00000000UL )\ntypedef volatile struct _Ifx_CAN_N_RX { Ifx_CAN_N_RX_F0C F0C; Ifx_CAN_N_RX_F0S F0S; Ifx_CAN_N_RX_F0A F0A; Ifx_CAN_N_RX_BC BC; Ifx_CAN_N_RX_F1C F1C; Ifx_CAN_N_RX_F1S F1S; Ifx_CAN_N_RX_F1A F1A; Ifx_CAN_N_RX_ESC ESC; } Ifx_CAN_N_RX; 单个Rx Buffer进行管理时，每个Rx Buffer称为Dedicated Rx Buffer，当写入数据后就会被锁住，不会再从CAN总线上写入新数据，直到CPU访问完后解锁\n多个连续的Rx Buffer 可以组成Rx FIFO进行管理，一个CAN Node可以设置2个Rx FIFO,FIFO0和FIFO1.同样首地址通过寄存器进行配置。FIFO是一种先进先出的数据缓存区域。\n在节点的初始化过程中\n①对于Tx,\n（1）首先设置buffer大小\n（2）判断config-\u0026gt;txConfig.txMode的类型是FIFO还是QUEUE设置为对应的模式并且设置其buffers的大小\n（3）在所选中的buffer使能中断\n​\t②对于Rx，主要是设置其Rx buffer 的数据长度和Message RAM的起始地址，然后设置Rx FIFO的数据长度，Message RAM的起始地址，大小，操作模式和watermark level\n对于FIFO,可容纳的元素数量称为Deepth，每个元素大小为Size，进入full状态后写入会溢出，所以要设置水线（Watermark)，Watermark\u0026lt;Deepth,当已被读取的Element达到Watermark时降低读取速度或者提高写入速度 读写指针：总指向下一个要读取/当前要被读出的单元。\n读空：当读写指针相同时，表示FIFO为空， 复位操作时 或者当读指针读出FIFO中最后一个字 后，追赶上写指针时，此时读空信号有效\n写满：当读写指针再次相等时，表明FIFO为满，这种情况发生在，当写指针转了一圈折回来（wrapped around）又追上了读指针\n7. Message RAM CAN0 起始地址：0xF0200000\nCAN1起始地址：0xF0210000\nCAN Node会将准备发送的Tx Frame以Tx Buffer的形式存放在Message RAM中，最多可以存放32个Tx Buffer数据\n8. 工作模式 Restricted Operation Mode 固定工作模式\n模式进入：处理器无法从消息RAM读取数据\n模式退出：host CPU重置CCCRi.ASM位\n在固定工作模式下，节点能够接收数据和远程帧，并对有效帧进行确认，但不发送数据帧、远程帧、活动错误帧或过载帧。出现错误或者过载情况等待总线空闲，然后同步CAN通信，\nBus Monitoring Mode 总线监控模式\n模式进入：CCCRi.MON置1，或者error level S3，即(TTOST0.EL = “11”)\n在总线监控模式下，点能够接收数据和远程帧，但是不启动传输，可以分析CAN总线上信号量\nPower Down (Sleep Mode) 睡眠模式\n模式进入：输入时钟信号或者控制寄存器\n具体过程：时钟停止请求信号active，CCCRi.CSR读取为1；\n1.当所有pending的传输请求都已经完成，等待总线空闲状态。\n2.然后M_CAN将CCCRi.INIT设置为1，以防止任何进一步的CAN传输；\n3.通过将CCCRi（i=0-3）.CSA设置为1来确认其已准备好Power Down。在这种状态下，在时钟被关闭之前，可以进行进一步的寄存器访问。对CCCRi.INIT写访问将无效。\n4.时钟将被关闭\nTest Modes 测试模式\n测试模式应仅用于生产测试或自检。针脚TXD的软件控制会干扰所有CAN协议功能。不建议使用测试模式进行应用\n3）.CSA设置为1来确认其已准备好Power Down。在这种状态下，在时钟被关闭之前，可以进行进一步的寄存器访问。对CCCRi.INIT写访问将无效。\n4.时钟将被关闭\nTest Modes 测试模式\n测试模式应仅用于生产测试或自检。针脚TXD的软件控制会干扰所有CAN协议功能。不建议使用测试模式进行应用\n","permalink":"https://fishdel.github.io/posts/%E8%8B%B1%E9%A3%9E%E5%87%8C-tc3xx-can-interface-mcmcan-%E6%A8%A1%E5%9D%97/","summary":"\u003ch3 id=\"can-interface-mcmcan\"\u003e\u003cstrong\u003eCAN Interface (MCMCAN)\u003c/strong\u003e\u003c/h3\u003e\n\u003ch4 id=\"1模块\"\u003e1.模块\u003c/h4\u003e\n\u003cp\u003e有三个模块CAN0,CAN1,CAN2，一般只使用CAN0,CAN0的模块比较全面。\u003c/p\u003e\n\u003cp\u003eCAN0有4个CAN node，Message RAM 一共32Kbyte，FIFO,buffer，register在此处开辟。\u003c/p\u003e","title":"英飞凌 TC3XX CAN Interface (MCMCAN)模块"},{"content":"事件触发操作系统，通过定时器届满，错误检出等event触发os task的调度运行\n1.TASK种类与状态 task的种类分为两种，基本task和扩展的task，处理器在同一时间只能运行一个task指令，os会负责保存和恢复task的状态切换时的数据\n1.1.Basic task 状态：Running，Ready，Suspended 单次任务模型，当基本任务中止或被强占时才会释放处理器 自动开始任务在startOS（）时被自动激活，可以开启需要等待事件的扩展任务。基本task在进入中止状态前只执行依次\n1.2.Extended task 状态：Running，Ready，Waiting，Suspended 多一个waiting状态，适合于需要中间执行同步的功能，使任务具有同步点，（当处理过程中发现缺少信息时，会切换到Waiting状态）时间响应较快，但会一直占用Ram资源，相当于以空间换时间\n1.3.状态与栈工作的对应 1.Basic task：当进入running状态时直接依次入栈，加到栈顶，然后依次出栈；2.Extended task：最差情况：在waiting过程中其他低优先级的任务都被激活并全被打断，扩展task的入栈位置需要确定，即知道实际任务占用的栈空间当扩展任务的栈管理异常时，会进入shutdownOS（）\n抢占式调度会增加任务context的切换时间和内存消耗，不可抢占式会降低实时性，但是节省context切换的时间\n2.调度方式 2.1.任务调度方式 FirstComeFirstServed（FCFS） 2.2.切换机制与调度策略 抢占式并需要设置优先级\n静态调度：在调度前就已经配置好；\n动态调度：根据负载率自动去调度 autosar支持静态调度，单个处理器一次处理一个任务，不同任务通过alarm进行切换\n抢占策略的选择：如果应用程序并行的task比较少，可以选择全抢占式，如果有明确的执行时间的短期任务则是和混合抢占式。\n若有三个task，优先级task1\u0026lt;task2\u0026lt;task3,task1,3,可以被抢占，task2不可被抢占，所以在运行task2时，task3不会抢占task2，优先级其实是相同的。好处是可以让task2运行完，节省了一次任务切换时间，并且task3的栈不会压到task2上增加栈空间。\n2.3.资源管理 死锁 1.概念：当两个（进程）task同时访问一个资源，在无外力作用时会导致无法推进 2.产生原因：竞争了不可剥夺资源；进程间的非法推进 3.产生条件：互斥条件 请求和保持 不剥夺 环路等待 4.解决方法：通过os resources使用优先级上限协议机制 优先级反转 优先级反转：低优先级延迟了高优先级的执行顺序\n举例：假设有task1-task4，优先级递减，task1强占task4对信号x的访问，此时task1在等待访问信号x的过程中（等待信号x被task4释放），中途会执行优先级比task4更高的task2和task3，当执行完task2,3后才会执行task4，即释放信号x，所以task1的执行时间被task2和task3延迟，即优先级反转。\n优先级上限协议：即上文的静态分配\n内容：在系统初始化时对每个资源静态分配一个上限优先级，当任务使用这个资源时任务的优先级就自动提升到该资源的优先级上限，使得其他访问该资源的task的优先级都低。 作用：相当于获得这个资源的最高级别优先级，其他试图访问该进程的优先级都低于它，所以不会发生倒挂\n3.task的激活 启动操作系统后，所有任务默认均为Suspended状态，用户去触发然后激活后进入Ready状态，os根据task的优先级进行调度\n3.1.Task的激活方式： 1.ActivateTask()—直接激活\n2.Alarm届满：对Alarm指定任务，该任务在每次Alarm届满时激活—间接激活\n对于一些重复的事件，通过定时器进行设置\n3.2 event和alarm机制 Alarm作用：以设置定时周期的方式\n1.激活一个task（通过硬件定时器产生tick time ，使system counter++，加到预设的值触发task） 2.设置一个event（event仅仅提供给Extended task） event为扩展任务提供同步点，每个event可以关联多个task，一个扩展任务ye可以用很多event，但是该event只能由其接受的那个扩展任务进行清除。\n当某个task正在运行时突然需要一个event，此时task进入waiting状态，释放cpu资源，而后去执行优先级低的task，等event来了，再执行改优先级高的task。\n3.设置回调函数 4.task的登录 Runnables可运行实体,SWC中的函数，再被达芬奇生成的时候手动添加实际功能，可以被定时器或者操作调用以及接受数据触发\nRunnables需要os中的task作为载体，需要放在操作系统的任务中来执行这个函数\nRunnables到task的映射需要RTE实现\n5.中断 优先级 说明 0类中断 不受OS管理 Timing protect 防止时间失效导致死锁，阻塞，错误同步等 1类中断 不与OS内部交互，1类中断不能被2类中断打断，开销小 2类中断 其中断向量表指向OS内部，开销大 Task 中断的周期性比task更严格 1类中断和2类中断的区别在于当有两个不同优先级的task1，task2时，在执行低优先级的task1时，如果1类中断被触发，退出中断后仍然回到task1，如果2类中断被触发，会进行os调度，中断退出后执行高优先级的task2\nAUTOSAR有两种时间保护机制，一种是执行时间的保护，一种是在bsw里的看门狗的保护\n5.HOOK机制 类似中断的机制，在OS里一般使用在StartupHook（操作系统启动后并且在调度程序运行前），ShutdownHook（系统被应用或是操作系统出错要求关闭），调试以及出错管理\nHOOK程序的优先级比task高，因为属于操作系统的一部分，所以不会被2类中断打断\n6.OS运行流程 硬件代码初始化\ncall startOS\nOS执行操作初始化代码\nOS执行StartupHook，用户将初始化程序放置在这里，此时所有中断无效\n操作程序激活中断和调度程序\n执行用户的中断和 task。\n","permalink":"https://fishdel.github.io/posts/autosar-os%E5%8E%9F%E7%90%86/","summary":"\u003cp\u003e事件触发操作系统，通过定时器届满，错误检出等event触发os task的调度运行\u003c/p\u003e\n\u003ch4 id=\"1task种类与状态\"\u003e1.TASK种类与状态\u003c/h4\u003e\n\u003cp\u003etask的种类分为两种，基本task和扩展的task，处理器在同一时间只能运行一个task指令，os会负责保存和恢复task的状态切换时的数据\u003c/p\u003e","title":"Autosar OS原理"},{"content":"MCU系统结构 整体结构 因为学习的是STM32，所以按照手册进行理解。\n哈佛结构和冯诺依曼结构 首先我们在编写代码的时候，可以将代码分为两部分，一部分是逻辑代码部分，另一部分是定义的变量，逻辑代码是不用改变的，而变量会改变，哈佛结构和冯诺依曼结构就是对于这个两部分代码的存储方式有着一些区别。 冯诺依曼结构将程序存储器和数据存储器合并在一起的处理器架构设计，他的特点是使用同一个存储器，经由同一个总线传输。 哈佛结构将程序指令存储和数据存储分开存储，在嵌入式编程中一般使用这种方式，因为可以只修改数据不用修改逻辑代码。\nSTM32采用的就是这种哈佛结构 它分为四个驱动单元和四个从动单元 驱动单元：CM3，系统总线和数据总线，DMA 从动单元: SRAM，FLASH，FSMA，AHB到APB的桥以及连接的所有设备。\n可以从这个总线矩阵来看，它的前级是CM3和DMA，它的后级是从设备——也就是存储器以及各个外设控制器。\n各部分间联系 CM3内核 内核里面有NVIC嵌套中断向量控制器：ALU，寄存器组，内存保护单元，总线的内连接等。 NVIC:关于中断的内容可以看这一篇： 中断的概念与机制 寄存器组：其中寄存器组的R0-R12为通用寄存器。R13为堆栈指针寄存器，有2个寄存器，当时同一时间只能用一个，一个是主堆栈指针（MSP）：复位后缺省使用的堆栈指针，用于操作系统内核以及异常处理例程（包括中断服务例程）；另一个是进程堆栈指针（PSP）：由用户的应用程序代码使用。 R14为连接寄存器，当呼叫一个寄存器，R14存储返回地址；R15是程序计数器PC。 其中还有特殊功能寄存器组：\n程序状态字寄存器组（PSRs），记录ALU的标志。 中断屏蔽寄存器组（PRIMASK, FAULTMASK, BASEPRI） PRIMASK相当于中断总开关，关闭后只有NMI和硬fault才会响应；FAULTMASK打开后只有NMI才会响应； BASEPRI定义被屏蔽优先级的阈值。 控制寄存器（CONTROL）：选择堆栈的指针和线程模式。 总线 Icode指令总线：和FLASH的接口相连，用于取指令，它不经过总线矩阵，不需要切换，速度更快。 Dcode数据总线：主要用来读取存储在SRAM和Flash里的数据。 哈佛结构在这里我理解为SRAM是处理变量和堆栈记录，而Flash是我们烧进去的代码，SRAM是由锁存器构成，掉电之后内容就会没有，而Flash相当于逻辑代码。 DMA总线：可以访问Flash，SRAM，数据寄存器，然后三者间交换数据并且不占用CPU。 而总线矩阵在这里协调CM3内核和DMA的竞争。 系统总线：也叫外设总线，连接CM3的内核和外设。AHB通过桥接的方式进行了分频，APB2为72HZ上面搭载着GPIO口，ADC，TIM1等外设，而APB1为36HZ,上面挂着串口，看门狗等慢速设备。\n时钟 STM32F10x时钟源 HSI：（高速内部）RC振荡器，频率8MHz，精度不高 HSE：（高速外部）外接石英/陶瓷晶振（4MHz——16MHz） LSI：（低速内部）RC振荡器，频率40KHz，LSI是作为看门狗时钟源和RTC时钟源而独立使用 LSE：（低速外部）外接晶振，32.768KHz石英晶振 PLL：锁相环倍频输出，其时钟输入源可选择为HSI/2、HSE或者HSE/2。倍频可选择为2~16倍，但是其输出频率最大不得超过72MHz\n多个时钟源是因为兼容不同速度的外设，有些高速，有些低速，不同的时钟对应不同的模块。\n系统时钟SYSCLK可来源于三个时钟源：\nHSI振荡器时钟 HSE振荡器时钟 PLL时钟 系统时钟\u0026mdash;\u0026gt;AHB分频器\u0026mdash;\u0026gt;各个外设分频倍频器 \u0026mdash;\u0026gt; 外设时钟的设置\nstm32上电过程 1. 选择启动方式 STM32 上电复位后代码从0x00000000开始，选择不同的启动模式就是将不同的地址映射到0x00000000 从Flash启动，将0x80000000映射到0x00000000； 从系统存储器启动，一般是将stm32里带的Bootloader的代码映射到0x00000000，这个Bootloader就是将用户的代码通过串口下载到Flash，再从Flash启动； 从SRAM启动，将0x20000000映射到0x00000000。\n2. 根据复位中断向量表设置SP和PC 假定从Flash启动 向量表的存储位置是可以设置的，通过 NVIC 中的一个重定位寄存器来指出向量表的地址。在复位后，该寄存器的值为 0。因此，在地址 0 处必须包含一张向量表，用于初始时的异常分配。 起始地址存放堆顶指针，而第二个地址则必须存放复位中断入口向量地址，这样在Cortex-M3内核复位后，会自动从起始地址的下一个32位空间取出复位中断入口向量，跳转执行复位中断服务程序。\n3. 初始化系统时钟 在复位中断函数中调用 SystemInit 函数，初始化时钟，配置中断向量表等。\n4. 软件设置SP 在复位中断服务程序中会跳转__main函数，在这里面加载.data.bss,初始化栈区，在__main函数中调用C函数main。\n","permalink":"https://fishdel.github.io/posts/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0mcu%E7%BB%84%E6%88%90%E7%BB%93%E6%9E%84%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%E4%B8%89/","summary":"\u003ch1 id=\"mcu系统结构\"\u003eMCU系统结构\u003c/h1\u003e\n\u003ch2 id=\"整体结构\"\u003e整体结构\u003c/h2\u003e\n\u003cp\u003e因为学习的是STM32，所以按照手册进行理解。\u003c/p\u003e\n\u003ch3 id=\"哈佛结构和冯诺依曼结构\"\u003e哈佛结构和冯诺依曼结构\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e首先我们在编写代码的时候，可以将代码分为两部分，一部分是逻辑代码部分，另一部分是定义的变量，逻辑代码是不用改变的，而变量会改变，哈佛结构和冯诺依曼结构就是对于这个两部分代码的存储方式有着一些区别。\n\u003cstrong\u003e冯诺依曼结构\u003c/strong\u003e将程序存储器和数据存储器合并在一起的处理器架构设计，他的特点是使用同一个存储器，经由同一个总线传输。\n\u003cstrong\u003e哈佛结构\u003c/strong\u003e将程序指令存储和数据存储分开存储，在嵌入式编程中一般使用这种方式，因为可以只修改数据不用修改逻辑代码。\u003c/p\u003e","title":"嵌入式系统——MCU组成结构与程序运行机制"},{"content":"1. CPU的运行原理 1.1 CPU最基本的工作单元——MOSFET 二极管的工作原理 SI原子外层有4个电子，P原子外层有5个电子，B原子外层有3个电子；如果SI和P结合，就会多一个电子，导电性上升；如果SI和B结合，就会有一个空穴，会有电子过来，导电性也会上升；电子和空穴都叫载流子，载流子就是电流的载体。 增加空穴的掺杂——P型掺杂；增加电子的掺杂——N型掺杂。 如果在一块硅晶体左边和右边进行N型掺杂和P型掺杂，中间区域就会出现电子从N区扩散到P区和空穴结合；交界处N区域失去电子显正电，P区域得到电子带负电，中间产生一个电场，叫耗尽层。 如果在外面接一个电池，如果电池提供的电场和中间电场方向相反进行抵消，此时电路导通；如果反过来接，耗尽层加宽，则不能导通。\nMOSFET的工作原理 在一块纯硅的两个肩膀处进行N型掺杂，其余部分P型掺杂，交界处产生耗尽层。 在两个N区域中间下层接绝缘层上层放金属板，P区域也充当金属板（类似于一个电容），此时通电很多电子填到下层P区域的空穴中，然后下面又出现一个耗尽层，两个N区域被联通起来，这个区域称为N沟道。\n如果施加一个能使沟道产生的电压，电路被导通，这个电压叫做阈值电压，中间电极称为栅极，左边称为源极，右边称为漏极。\n高于阈值电压被导通，低于阈值电压不导通：NMOS； 如果NP掺杂时相反，高于阈值电压不导通，低于阈值电压导通：PMOS；\n把NMOS和PMOS的漏极连起来得到一个CMOS。 对于NMOS远离源头。 对于PMOS指向源头 1.2 逻辑门 门电路 以非门为例： 以或门为例： 假设A=0,B=1,那么A上的PMOS导通，B上的NMOS导通，PMOS不导通，中间的主线相当于接了VSS，为0，再经过非门，得到值为1。 之后以此类推。 所以，或非门就是不要后面的非门。 同样，与非门： 可以得出： 与非门+非门=与门 或非门+非门=或门\n异或门： 逻辑的组合 异或门+与门可以构成半加法器，但是不能进位（后面会解释） 所以两个半加器级联，构成全加法器，四个串联起来四位全加法器。\n时序的逻辑 D锁存器 C端如果是0，D端不起作用 C端为1的时候，Q和Q~被D刷新成新值 锁存器是一种存储数据形式，SRAM是用这种形式，所以掉电之后代码会丢失。\nD触发器 一对D锁存器构成，控制信号相反 clk为高，主关从开；clk为低，主开从关；低电平前面导通，高电平后面导通，从而实现一个节拍结构。\n1.3 CPU计算加法的原理 本位和（Sum），进位（Carry） 10进制 3+9=12里2是本位和，1是进位数； 2进制 1+1=10里0是本位和，1是进位数 A B S C 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 本位和S符合异或的逻辑，进位C符合与的逻辑，用这两个门就可以计算一位二进制数得到一个半加法器，但是它的输入只有A,B，就不能输入上一次计算的进位数，所以需要再来一个可以计算上一位进位数的加法器。 本位S还要和Cin再异或一次；进位条件是A,B,Cin有大于等于两个以上的1就行了,这三个分别做与运算，结果再分别做或运算。 四个全加法器Cin和Cout首位串起来，就是4位串行进位全加法器。\n1.4 总结 逻辑门由MOSFET组成，把它们刻蚀到芯片上，按电路图连接，CPU内部全是门电路构成。\n2. 概念CPU之微控制器MCU和ARM CPU是什么 CPU是计算机/微控制器的核心，进行算术逻辑运算，通用CPU需要大量外围辅助. MUC 微控制器：完成的计算机系统，单个芯片包含了处理器，存储器和所有外设I/O模块. 其组成包括： CPU 输入输出接口 外设接口 RAM ROM 时钟单元 MCU优势在于：小巧，低成本，低功耗。\nARM是什么 1 一家公司，全球领先的半导体知识产权(IP)提供商，ARM设计了大量高性价比、耗能低的RISC处理器、相关技术及软件。 2 ARM指的也是一门技术，具有性能高、成本低和能耗省的特点。在智能机、平板电脑、嵌入控制、多媒体数字等处理器领域拥有主导地位。 3 ARM还是一类微型处理器的统称，其微型处理器包含多个系列，每个系列各自的特点和应用领域。\n3.\tCPU的基本结构和运行机制 3.1 基本结构 运算逻辑单元（Arithmetic Logic Unit） 操作数operands 运算operation 结果\tresult 标志\tflag 寄存器组（register file） 作用：用于临时保存/获取操作数 程序状态寄存器（program status register/CCR） 特点：1. 执行单元产生的标志通常放在PSR中；2. 每执行一条指令，相应的状态位更新；3.\t每条指令影响的状态位不同 条件码：Zero，Negative，Overflow，Carry 寄存器与CPU的关系：\t任何CPU都包含通用/专用寄存器；\t寄存器的数目和宽度是衡量CPU的重要指标。 寄存器和内存的关系：内存并不在CPU上访问比较慢，寄存器相当于衣服上的口袋，内存相当于包包。在单片机中，寄存器相当于要操作的外设的别名，通过操作寄存器对外设进行控制。 ARM五个寄存器编程模型 xPSR——保存cpu各种状态（32位） 通过别名访问，只关注和访问特定的字段实现特定功能 APSR—高四位：N Z C V IPSR—后几位：发生异常时的中断号 EPSR—T：记录是否发生异常和中断 PRIMASK：PM 控制中断的总开关 FAULTMASK BASEPRI CONTROL 控制寄存器：实现控制堆栈指针的选择和切换到用户级 程序计数器（Program Counter） 作用：保存下一条待执行的指令 控制单元（Control Unit） 指令解析 分析该指令需要执行何种操作 原理：程序由指令序列构成，保存在程序存储器中，这些指令序列依次进入CPU执行 数据流向 3.2堆栈的概念 栈 概念：一段连续的存储空间 工作方式：后入先出，只能从顶部加入或取出数据 特点：堆栈能保持数据的顺序 操作方式：PUSH，PULL 栈的实际使用：分支调用，嵌套调用，顺次保存函数地址，逆序取用。 栈的作用\nC语言编译器使用堆栈来完成参数的传递和返回值传递 汇编语言使用堆栈来保存局部变量，寄存器值 CPU硬件使用堆栈来保存返回地址和寄存器上下文（中断） 栈与寄存器的关系 栈的顶端位置通过CPU内的堆栈指针寄存器确定（stack pointer） 初始位置由程序代码确定，指向预先划定的堆栈空间底部 地址 变量地址从低地址向高地址划分 堆栈空间从高地址向低地址增长 堆 概念：一个进程开启后，系统分配给它的一个全局的空间，系统中所有动态分配的对象（指针）都在这个空间分配。 注意：堆里的数据是有数据结构的，空间占用不连续 堆栈溢出：堆栈溢出的产生是由于过多的函数调用，导致调用堆栈无法容纳这些调用的返回地址，一般在递归中产生。\n3.3 运行机制 该程序的功能：A和B做对换 具体步骤 1 开始什么也不做，随后加载一个栈指针 2 A入栈，B入栈 3 跳到函数subfunc 3.1 do nothing 3.2 返回 4 A出栈，B出栈\n","permalink":"https://fishdel.github.io/posts/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0cpu%E4%B8%80/","summary":"\u003ch1 id=\"1-cpu的运行原理\"\u003e1. CPU的运行原理\u003c/h1\u003e\n\u003ch2 id=\"11-cpu最基本的工作单元mosfet\"\u003e1.1 CPU最基本的工作单元——MOSFET\u003c/h2\u003e\n\u003ch3 id=\"二极管的工作原理\"\u003e二极管的工作原理\u003c/h3\u003e\n\u003cp\u003eSI原子外层有4个电子，P原子外层有5个电子，B原子外层有3个电子；如果SI和P结合，就会多一个电子，导电性上升；如果SI和B结合，就会有一个空穴，会有电子过来，导电性也会上升；电子和空穴都叫载流子，载流子就是电流的载体。\n增加空穴的掺杂——P型掺杂；增加电子的掺杂——N型掺杂。\n如果在一块硅晶体左边和右边进行N型掺杂和P型掺杂，中间区域就会出现电子从N区扩散到P区和空穴结合；交界处N区域失去电子显正电，P区域得到电子带负电，中间产生一个电场，叫耗尽层。\n如果在外面接一个电池，如果电池提供的电场和中间电场方向相反进行抵消，此时电路导通；如果反过来接，耗尽层加宽，则不能导通。\u003c/p\u003e","title":"嵌入式系统——CPU"},{"content":"中断的概念和机制 中断与轮询 中断： 由硬件判断外部事件并通知CPU；专用的中断服务程序来处理事件 处理对响应要求非常高的事件 处理持续事件非常短的事件 低功耗的应用 程序设计复杂 通常把CPU内部的紧急时间叫做异常，比如地址访问越界； 把CPU外部的片上外设产生的紧急时间叫做中断,比如GPIO口引脚的电平变化。 中断和异常都是停下当前任务去执行紧急事件，所以一般统称位中断。\n轮询：周期/连续检查外部事件是否发生 消耗大量CPU处理时间 在CM3的内核中1~15号是系统异常，16 ~ 256是外部中断，有内核中的NVIC（嵌套向量中断控制器）\n中断控制器（NVIC） 作用\n中断管理 支持异常及中断向量化处理 支持嵌套中断 中断管理 全局中断控制 CRR寄存器中的一个特殊位 在复位和中断后该位置\nDedicated IE 在复位后所有中断被禁止\n中断标志位 IF 中断源对应的标志 引发请求 读写操作清除中断标志位\n寄存器AIRCR（中断和复位寄存器） 0-7位：设置抢占优先级和子优先级的级数 8-10位：优先级分组 中断先看抢占优先级，抢占优先级越高越先触发；抢占优先级相同，子优先级高的先触发。 抢占优先级高可以打断低的，即中断的嵌套，实现中断的嵌套\n中断优先级特点\n多个中断同时出现，高优先级中断先得到响应。 中断优先级可以是固定的或者是编程指定的 固定优先级：根据中断向量表顺序（比如S12内核） 设定优先级：每个中断都有优先级设置位（比如ARM Cortex M0+支持4个优先级） 每一个部件也有一个自己的中断控制器，而NVIC相当于总管家。\n中断和异常向量表 内部异常和外部中断按照优先级进行排列形成一张中断向量表，一般数字越小优先级越高。当发生中断和异常的时候，处理器将PC指向表中的相应地址，这个地址叫做异常向量。 前三个优先级是最高的。 中断向量表的特点：\n一段连续的存储空间 ； 复位后有默认起始位置 ； 每个中断在向量表中都有相应的表项，该表项的值为该中断对应的服务程序的地址； 中断向量表里的内容赋值给PC指针，程序相应的就会发生跳转 由程序代码确定中断向量表的每个表项； 中断向量表的位置是可以通过改写中断向量基址寄存器重新定位的； 在系统上电之后，即缺省情况下，会执行这样一张表，后面会讲到。 工作流程 中断的过程 一个中断的产生可以归纳为打开中断总开关，使能中断，配置发生条件：\nstep1 设置自己的控制单元； step2 在NVIC里打开总中断； step3 中断源送到CPU； step4 CPU查找中断向量表，找到处理函数的地址去执行函数。 以GPIO口电平引起的中断为例，有一个EXIT外部中断控制器，管理所有GPIO的中断。EXTI进行使能，将请求通知给NVIC，再通知CPU。\n堆栈情况 通过栈结构进行现场恢复的步骤\nstep1 主程序执行的时候，发生中断 step2 压栈保留一个现场执行中断函数 step3 中断函数没有执行完，又来了一个更高优先级的中断 step4 再次打断它，在保留一个CPU的现场 step5 再占一部分堆栈去执行更高优先级的中断 step6 依次退出 注意点\n中断的寄存器入栈由CPU硬件自动完成 在中断时寄存器在堆栈中的保存顺序是在其手册中指明的。 开发者需要手动的把发生变化的寄存器压入堆栈。 中断服务子程 中断服务子程也叫中断服务函数，在一些CPU中，中断服务子程不同于一般的C函数，函数退出时的返回汇编指令有所区别。\t在ARM Cortex M0+平台上，中断服务子程与一般C语言写法没有区别，使用同样的汇编返回指令即可\n中断子程序函数特点：\n被CPU硬件自动调用 在ISR执行前、后，CPU自动进行了堆栈出入等操作 写成C语言的参数和返回值都应该为void ","permalink":"https://fishdel.github.io/posts/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%AD%E6%96%AD%E4%BA%8C/","summary":"\u003ch2 id=\"中断的概念和机制\"\u003e中断的概念和机制\u003c/h2\u003e\n\u003ch3 id=\"中断与轮询\"\u003e中断与轮询\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e中断\u003c/strong\u003e： 由硬件判断外部事件并通知CPU；专用的中断服务程序来处理事件\n\u003cul\u003e\n\u003cli\u003e处理对响应要求非常高的事件\u003c/li\u003e\n\u003cli\u003e处理持续事件非常短的事件\u003c/li\u003e\n\u003cli\u003e低功耗的应用\u003c/li\u003e\n\u003cli\u003e程序设计复杂\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003e通常把CPU内部的紧急时间叫做异常，比如地址访问越界；\n把CPU外部的片上外设产生的紧急时间叫做中断,比如GPIO口引脚的电平变化。\n中断和异常都是停下当前任务去执行紧急事件，所以一般统称位中断。\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"嵌入式系统—中断"},{"content":"1.数据导入 df=pd.read_csv(\u0026#39;Pokemon.csv\u0026#39;,encoding=\u0026#34;ISO-8859-1\u0026#34;) df.head() 2.查看数据 df.shape df.shape[0] df.shape[1] df.columns #列名 df.index #行名 df.dtypes df.head() df.tail() df.sample() df.describe() pd.set_option(\u0026#39;max_colwidth\u0026#39;,8) #设置每一行的最大宽度，恢复原设置方法 pd.reset_option(\u0026#39;max_colwidth\u0026#39;) loc操作与iloc loc通过label定位；iloc通过position定位\ndf.loc[[0,5],[\u0026#39;名称\u0026#39;,\u0026#39;生命点数\u0026#39;]] df.iloc[0:10,[0,1]] df[:3] 3.数据筛选与操作 df[df[\u0026#39;综合能力\u0026#39;]\u0026gt;400].head() df[df[\u0026#39;世代数\u0026#39;]==1] df.insert(4,\u0026#39;能力600\u0026#39;,df[\u0026#39;综合能力\u0026#39;]\u0026gt;=600) 4.读取数据 pickle文件，可以将python中的数据类型进行序列化 compression参数指定了压缩类型，\u0026lsquo;zip\u0026rsquo;, \u0026lsquo;gzip\u0026rsquo;, \u0026lsquo;bz2\u0026rsquo;, \u0026lsquo;zstd\u0026rsquo;\npandas.read_pickle(filepath_or_buffer, compression=\u0026#39;infer\u0026#39;, storage_options=None) 5.处理数据 （1）取特定的行列 利用标签取列：\ndata[\u0026#39;xxx\u0026#39;] data.at #访问单个值 data.loc[\u0026#39;行名\u0026#39;,\u0026#39;列名\u0026#39;] #访问值组 data.loc[[\u0026#39;A行\u0026#39;,\u0026#39;B行\u0026#39;]] # 取多行 （2）对需要的行列进行处理 时间处理 #转换回日期格式,默认是毫秒 data[\u0026#39;time\u0026#39;] = pd.to_datetime(data[\u0026#39;time\u0026#39;],unit=\u0026#39;s\u0026#39;) #生成一个时间戳 now = pd.Timestamp.now() 某一列数据进行处理 data[\u0026#39;xxx\u0026#39;] = [\u0026#39;这里用一个列表表达式\u0026#39;] data[\u0026#39;xxx\u0026#39;] = data[\u0026#39;xxx\u0026#39;].apply(lamdba x: function(x)) 对于字符串的处理 #去掉首位的a data[\u0026#39;a\u0026#39;] = data[\u0026#39;a\u0026#39;].str.strip(\u0026#39;a\u0026#39;) # 替换 data[\u0026#39;a\u0026#39;] = data[\u0026#39;a\u0026#39;].str.replace(\u0026#39;a\u0026#39;，\u0026#39;b\u0026#39;) 将特定的列拆分后合并 data = pd.concat([data, data[\u0026#39;tempvaluearray\u0026#39;].str.split(\u0026#39;,\u0026#39;, expand=True)], axis=1).drop(\u0026#39;tempvaluearray\u0026#39;, axis=1) pd.concat()#可以按照指定的轴将dataframe或者series拼接，而merge只能拼接两个表 axis #0：上下拼接 1：左右拼接 #drop()的参数axis在删除特定的一行为1 批量修改列名称 data.rename(columns=dic)#可以写一个字典进行替换 计算时保留小数 \u0026#39;%.3f\u0026#39;%0.0065 #保留3位小数 lambda x:\u0026#39;%.3f\u0026#39;%((int(x)-1000)/1000)) 调整列顺序 #先取出来再插入 A = data[\u0026#39;A\u0026#39;] data.drop(labels=[\u0026#39;A\u0026#39;],axis = 1,inplace=True) data.insert(0, \u0026#39;A\u0026#39;, A) 一个字符串划分的函数 # 对于数据为[12，23，45，12]类似的数据，其中不能是字符串 def split_col(data, columns): \u0026#34;\u0026#34;\u0026#34;拆分成列 :param data: 原始数据 :param columns: 拆分的列名 :type data: pandas.core.frame.DataFrame :type columns: list \u0026#34;\u0026#34;\u0026#34; for c in columns: new_col = data.pop(c) max_len = max(list(map(len, new_col.values))) # 最大长度 new_col = new_col.apply(lambda x: x + [None]*(max_len - len(x))) # 补空值，None可换成np.nan new_col = np.array(new_col.tolist()).T # 转置 for i, j in enumerate(new_col): data[c + str(i)] = j 去重 df_unique = df.drop_duplicates([\u0026#39;A\u0026#39;], keep=\u0026#39;last\u0026#39;) # 根据A去重只留下最后一次出现的 隔行取数据 方法1：每隔20行取数,把每20行的id取出来\na=[] for i in range(0,len(df),20): a.append(i) new_df= df.iloc[a] 方法2：直接调用read_csv()里的参数skiprows\ndf = pd.read_csv(\u0026#39;test.csv\u0026#39;,header = None,skiprows=lambda x: x \u0026gt; 0 and x % 20 != 0) df 6.遍历和求值 遍历\nfor name,group in grouped_single: print(name) display(group.head()) 组内求均值，最大值，最小值等等\ndf.groupby(\u0026#39;A\u0026#39;)[\u0026#39;B\u0026#39;].mean() agg自定义内置函数 df.groupby(\u0026#39;A\u0026#39;)[\u0026#39;B\u0026#39;].agg(fun()) 整合（Aggregation）——即分组计算统计量（如求均值、求每组元素个数）\n变换（Transformation）——即分组对每个单元的数据进行操作（如元素标准化）\n过滤（Filtration）——即按照某些规则筛选出一些组（如选出组内某一指标小于50的组）\n7.举例：分组统计次数或者频数 ①先排序，再分组，加上first()就是取最大值。\n若B是种类，C是时间，就是先按种类分组，再按时间分组，取A最大的情况。\ndf1=df.sort_values(\u0026#39;A\u0026#39;, ascending=False).groupby([\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;], as_index=False).first() ②apply方法(groupby+apply这样的写法比之前写循环要快很多)\n先分组再对A属性进行数量统计，加上head（1）取最大值\n需要加上reset_index,时期扁平化，原来的index变成数据列，保留下来\ndf2=df.groupby(\u0026#39;B\u0026#39;)[\u0026#39;A\u0026#39;].apply(lambda x: x.value_counts(normalize=True).head(1)).to_frame().reset_index() 举个例子：统计数学成绩前五名的学生\ndf = df[\u0026#39;math\u0026#39;].groupby(\u0026#39;class\u0026#39;).apply(lambda x: x.sort_values(ascending = False)[:5])` 1. value_counts()方法 需要加上unstack()将其展开，否则是一个序列。\ndata.groupby(\u0026#39;A\u0026#39;)[\u0026#39;B\u0026#39;].value_counts().unstack() 2.DataFrame.plot函数 画在一个子图里\npd.DateFrame.plot(kind = \u0026lsquo;scatter\u0026rsquo;)\nax=v1.plot.scatter(x=\u0026#39;C\u0026#39;,y=\u0026#39;D\u0026#39;,label=\u0026#39;D\u0026#39;,title=i,alpha=0.4,) v1.plot.scatter(x=\u0026#39;C\u0026#39;,y=\u0026#39;E\u0026#39;,c=\u0026#39;r\u0026#39;,title=i,alpha=0.4,ax=ax,label=\u0026#39;E\u0026#39;) ","permalink":"https://fishdel.github.io/posts/pandas%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/","summary":"\u003ch3 id=\"1数据导入\"\u003e1.数据导入\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003epd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eread_csv\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;Pokemon.csv\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eencoding\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;ISO-8859-1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ehead\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2查看数据\"\u003e2.查看数据\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecolumns\u003c/span\u003e \u003cspan class=\"c1\"\u003e#列名\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eindex\u003c/span\u003e \u003cspan class=\"c1\"\u003e#行名\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edtypes\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ehead\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etail\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esample\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edescribe\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_option\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;max_colwidth\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"c1\"\u003e#设置每一行的最大宽度，恢复原设置方法\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereset_option\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;max_colwidth\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eloc操作与iloc\nloc通过label定位；iloc通过position定位\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eloc\u003c/span\u003e\u003cspan class=\"p\"\u003e[[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e],[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;名称\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;生命点数\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eiloc\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e,[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[:\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"3数据筛选与操作\"\u003e3.数据筛选与操作\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;综合能力\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"mi\"\u003e400\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ehead\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;世代数\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einsert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;能力600\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;综合能力\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;=\u003c/span\u003e\u003cspan class=\"mi\"\u003e600\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"4读取数据\"\u003e4.读取数据\u003c/h3\u003e\n\u003cp\u003epickle文件，可以将python中的数据类型进行序列化\ncompression参数指定了压缩类型，\u0026lsquo;zip\u0026rsquo;, \u0026lsquo;gzip\u0026rsquo;, \u0026lsquo;bz2\u0026rsquo;, \u0026lsquo;zstd\u0026rsquo;\u003c/p\u003e","title":"Pandas基本用法"},{"content":"基本概念 1.docker 应用容器引擎，用于运行容器\nimage：可执行程序\ncontainer：运行起来的进程\ndockerfile：image的源代码， 是一个用**来构建镜像的文本文件，**文本内容包含了一条条构建镜像所需的指令和说明\n部署流程 上传项目文件到服务器\n进去项目文件夹 使用 Dockerfile 创建镜像\ndocker build -t server:v1\n运行容器之前先停止老版本\ndocker stop server-container:v0\n基于镜像运行容器\ndocker run -d name server-container:v1 server:v1\n暂停一个容器\ndocker pause haicoder\n删除之前版本的容器\ndocker rmi server:v0\n若有问题恢复之前版本\ndocker stop server-container:v1\ndocker run -d --name server-container:v0 server:v0\n基于dockerfile创建实例 创建一个Dockerfile 创建一个文件夹mkdir TestDockerfile 进入文件夹cd TestDockerfile 创建一个文件touch Dockerfile 写入echo \u0026quot;hello world\u0026quot; \u0026gt; index.html创建一个测试文件 在相应的目录下 docker build -t = “beyond/test：v1” 镜像名为beyond/test，标签为v1 Dockerfile文件 FROM ubuntu：16.04 基于哪个镜像进行构建\nMAINTAINER beyond ubuntu ”111@qq.com\u0026quot;\nADD ./index.html /var/www/html/index.html把和Dockerfile文件在同一个目录里面的文件添加到要构建的镜像\nWORKDIR /var/www/html 设置工作目录\nrun和cmd的区别：run在构建镜像时运行，cmd在容器启动时运行，每次启动都执行一次\n常用命令 docker build：构建镜像。\n常用格式：docker build -t 镜像名 Dockerfile所在路径。 示例： dcoker build -t image-name .。 docker run：基于镜像启动容器。\n常用格式：docker run -id -t --name 容器名 镜像名/镜像ID。 示例：docker run -id -t --name container-name image-name。 docker ps：显示所有容器信息。\ndocker images：显示所有镜像信息。\ndocker stop：停止容器运行。\ndocker start：重新运行一个已停止的容器。\ndocker rm：删除容器，删除之前要先确保容器已经停止运行。可以指定多个容器。\n示例：docker rm container-name。 docker rmi：删除镜像，删除之前要确保没有基于该镜像的容器存在。可以指定多个镜像。\n示例：docker rmi image-name/image-id docker exec：在容器中执行命令。\n示例：docker exec -it container-name /bin/bash。这命令可以启动容器内的 bash，其中 -i 表示以交互的方式运行命令，-t 表示在终端（tty）中运行。 2.一些概念 集群 服务器集群，使多台服务器能够像一台服务器那样工作或者是看起来好像一台机器，提高数据处理能力以及服务能力。\n负载均衡 把工作分到多个服务器，防止哪个服务器宕机其他服务器能提供相同内容。\n负载均衡器：选择能正常做出响应的后端服务器。\n分布式 分散的物理和逻辑资源通过计算机网络实现信息的交换。\n例：计算1+2+\u0026hellip;+100 一台电脑处理1+2+3\u0026hellip;+50,另一台处理51+52\u0026hellip;+100，再进行汇总\n区别 集群是将几台服务器集合到一起，来实现同一业务，分布式是处理不同任务。\nElyra 一个AI项目：数据预处理— 特征抽取— 训练— 模型评估— 部署 提供一个pipeline可视化编辑器，将多个Notebook转换为批处理或工作流 kubernetes(k8s) 一个容器集群的管理系统\n快速部署应用\n快速扩展应用\n无缝对接新的应用功能\n节省资源，优化硬件资源的使用\nKubeflow kubernetes的机器学习工具包\nkubeflow的组建：\tPipeline Pipeline：（Pipelines是一个基于Argo实现了面向机器学习场景的流水线项目，提供机器学习流程的创建、编排调度和管理，提供了一个Web UI）提供一个ui来定义机器学习的过程，整个过程运行在k8s集群上。\npipeline是一个机器学习工作流的抽象概念，可以是一个函数过程，也可以是数据加载，变换，清洗等环节。\n在pipelines构建各流程组件前，需要将对应流程的业务代码打包成docker镜像文件（因为kubeflow中运行的代码均以容器方式实现）\n构建pipeline的步骤 安装专门的sdk：打包Docker镜像，镜像是组件的依赖，每一个组件运行就是一个容器。\npython定义好pipeline：python函数描述组件的输入输出信息，有几个节点输入有几个节点输出。\nsdk构建pipeline的包，通过ui上传\nk8s命令和kubeflow平台 查看版本kubectl version\n查看命名空间 kubectl get ns\n查看命名空间下的资源 kubectl get pods --namespece (名字)\n运行pipeline需新建experiment，即将同一类的pipeline 运行放在同一个experiment中（KFP）\n运行一个pipelines：上传yaml文件或者通过url上传\n","permalink":"https://fishdel.github.io/posts/%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%B9%8Bdocker+kubeflow/","summary":"\u003ch2 id=\"基本概念\"\u003e基本概念\u003c/h2\u003e\n\u003ch4 id=\"1docker\"\u003e1.docker\u003c/h4\u003e\n\u003cp\u003e应用容器引擎，用于运行容器\u003c/p\u003e\n\u003cp\u003eimage：可执行程序\u003c/p\u003e\n\u003cp\u003econtainer：运行起来的进程\u003c/p\u003e\n\u003cp\u003edockerfile：image的源代码， 是一个用**来构建镜像的文本文件，**文本内容包含了一条条构建镜像所需的指令和说明\u003c/p\u003e","title":"工程化之docker+kubeflow"},{"content":"LightGBM 介绍 LightGBM（Light Gradient Boosting Machine）：一个实现GBDT算法的框架，解决GBDT在海量数据遇到的问题。\n两大技术： （1）GOSS(Gradient-based One-Side Sampling)：减少样本数\n（2）EFB (Exclusive Feature Bundling ):减少特征数\nXGBoost的缺点：先预排序再找分割点，空间消耗大\nXGBoost与LightGBM的区别：\nlightGBM XGBoost 分裂方式 leaft-wise选择分裂收益最大的节点，要限制深度容易过拟合 level-wise无差别分裂 输入 lightgbm支持直接输入categorical 的feature 需要one-hot编码 时间复杂度 基于直方图的决策树算法，直方图的优化算法只需要计算K次，时间复杂度为O(Kfeature) 基于预排序的决策树算法，每遍历一个特征就需要计算一次特征的增益，时间复杂度为O(datafeature) 特征捆绑转化为图着色问题，减少特征数量 两种使用形式 sklearn接口形式 导包\nimport lightgbm as lgb from sklearn.metrics import mean_squared_error from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split gbm = lgb.LGBMRegressor(objective=\u0026#39;regression\u0026#39;, num_leaves=31, learning_rate=0.05, n_estimators=20) gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric=\u0026#39;l1\u0026#39;, early_stopping_rounds=5) 原生形式 数据集切分与转换\nlgb_train = lgb.Dataset(X_train, y_train) #If this is Dataset for validation, training data should be used as reference. lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train) 将参数写为字典形式\nparams = { \u0026#39;task\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;boosting_type\u0026#39;: \u0026#39;gbdt\u0026#39;, # 设置提升类型 \u0026#39;objective\u0026#39;: \u0026#39;regression\u0026#39;, # 目标函数 \u0026#39;metric\u0026#39;: {\u0026#39;l2\u0026#39;, \u0026#39;auc\u0026#39;}, # 评估函数 \u0026#39;num_leaves\u0026#39;: 31, # 叶子节点数 \u0026#39;learning_rate\u0026#39;: 0.05, # 学习速率 \u0026#39;feature_fraction\u0026#39;: 0.9, # 建树的特征选择比例 \u0026#39;bagging_fraction\u0026#39;: 0.8, # 建树的样本采样比例 \u0026#39;bagging_freq\u0026#39;: 5, # k 意味着每 k 次迭代执行bagging \u0026#39;verbose\u0026#39;: 1 # \u0026lt;0 显示致命的, =0 显示错误 (警告), \u0026gt;0 显示信息 } 交叉验证与预测评估\ngbm = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5) y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration) Epoch、Iteration、Batchsize相关理解 Epoch 一个完整的数据集通过了神经网络一次并且返回了一次。梯度下降的方法来优化学习过程，随着epoch\nIteration batch需要完成一个epoch的次数\n一个迭代=一个正向通过+一个反向通过。\nBatchsize 不能将数据一次性通过神经网络的时候，就需要将数据集分成几个batch。\nbatchsize的选择：Batch：数据集较小选择全批次\nmini batch：选定后以batch的大小输入网络，计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均\nstochastic：每次修正方向以各自样本的梯度方向修正，难收敛\n如果batchsize过小，训练数据难以收敛容易欠拟合，增加batchsize相对处理速度加快但是占用内存增加\n参数调整 预先固定的参数 调整策略 learning_rate 0.05~0.1 学习率较小比较稳定。默认0.1 n_estimators 100~1000。可以设置一个较大的值配合early_stopping_round来让模型根据性能自动选择最好的迭代次数。默认100 min_split_gain 执行节点分裂的最小增益。默认为0。不建议去调整。增大这个数值会得到相对浅的树深。可调整其他参数得到类似效果。 min_child_sample 一个叶子上的最小数据量。默认设置为20.数据量大适当增加 min_child_weight 一个叶子上的最小hessian和。默认设置为0.001，一般设置为1。不建议调整，增大数值会得到较浅的树深 通过算法来搜索的参数 调整策略 max_depth 3，4，5（过大容易过拟合） num_leaves 小于2^max_depth-1 subsample 大致的搜索范围[0.8, 0.9, 1.0] colsample_bytree 大致的搜索范围[0.8, 0.9, 1.0] reg_alpha 服务于L1正则化，一般取0-1000的范围。通过特征筛选该数值由大变小可以增加模型信心 reg_lambda 服务于L2正则化，一般0-1000的范围。如果有非常强势的特征，可以人为加大一些reg_lambda使得整体特征效果平均一些，一般会比reg_alpha的数值略大一些，但如果这个参数大的夸张也需要再查看一遍特征是否合理 代码实例 训练模型并求最优参数的函数定义 import lightgbm as lgb def cv_model(clf, train_x, train_y, test_x, clf_name): 划分100折并进行数据打乱\nfolds = 10 seed = 2022 kf = KFold(n_splits=folds, shuffle=True, random_state=seed) ​ 设置测试集的输出矩阵。每一组数据输出：[0,0,0,0]以概率值填入\ntest = np.zeros((test_x.shape[0],4)) #交叉验证分数 cv_scores = [] onehot_encoder = OneHotEncoder(sparse=False) 根据折数进行划分，i值代表第（i+1）折。每一个K折都进行「数据混乱：随机」操作 train_index：10折里9折在train_index valid_index：剩下1折样本索引值，作为验证集用于给出「训练误差」\nfor i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)): if i \u0026lt; 9: #打印第（i+1）个模型结果 print(\u0026#39;模型:\u0026#39;i+1) #将训练集分为：真正训练的数据（K-1折），和 训练集中的测试数据（1折） trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y.iloc[train_index], train_x.iloc[valid_index], train_y.iloc[valid_index] 模型的设置和调用 #LGB模型 if clf_name == \u0026#34;lgb\u0026#34;: #训练样本 train_matrix = clf.Dataset(trn_x, label=trn_y) #训练集中测试样本 valid_matrix = clf.Dataset(val_x, label=val_y) #参数设置 params = { \u0026#39;boosting_type\u0026#39;: \u0026#39;gbdt\u0026#39;, #boosting方式 \u0026#39;objective\u0026#39;: \u0026#39;multiclass\u0026#39;, #任务类型为「多分类」 \u0026#39;num_class\u0026#39;: 4, #类别个数 \u0026#39;num_leaves\u0026#39;: 2 ** 5, #最大的叶子数，树模型的复杂度 \u0026#39;feature_fraction\u0026#39;: 0.8, #每次迭代中随机选择特征的比例（0.5-0.9之间调整） \u0026#39;bagging_fraction\u0026#39;: 0.8, #不进行重采样的情况下随机选择部分数据（0.5-0.9之间调整） \u0026#39;bagging_freq\u0026#39;: 5, #每5次迭代，进行一次bagging（3-5之间调整） \u0026#39;learning_rate\u0026#39;: 0.05, #学习率 \u0026#39;seed\u0026#39;: seed, #seed值，保证模型复现 \u0026#39;nthread\u0026#39;: 28, \u0026#39;n_jobs\u0026#39;:24, #多线程 \u0026#39;verbose\u0026#39;: 1, \u0026#39;lambda_l1\u0026#39;: 0.4, # L1正则化 \u0026#39;lambda_l2\u0026#39;: 0.5, #L2正则化 \u0026#39;min_data_in_leaf\u0026#39;:100, #叶子可能具有的最小记录数 } #模型 model = clf.train(params, train_set=train_matrix, #训练样本 valid_sets=valid_matrix, #测试样本 num_boost_round=2000, #迭代次数 verbose_eval=100, early_stopping_rounds=200) #如果数据在200次内没有提高，停止计算 ​\nval_pred = model.predict(val_x, num_iteration=model.best_iteration) test_pred = model.predict(test_x, num_iteration=model.best_iteration) val_y = np.array(val_y).reshape(-1, 1) val_y = onehot_encoder.fit_transform(val_y) print(\u0026#39;预测的概率矩阵：\u0026#39;) print(test_pred) test += test_pred #验证集计算训练误差 score = loss(val_y, val_pred) cv_scores.append(score) print(cv_scores) print(\u0026#34;%s_scotrainre_list:\u0026#34; % clf_name, cv_scores) print(\u0026#34;%s_score_mean:\u0026#34; % clf_name, np.mean(cv_scores)) print(\u0026#34;%s_score_std:\u0026#34; % clf_name, np.std(cv_scores)) #i个模型输出结果的平均值。 test = test / 10 return test 调用模型的函数定义 def lgb_model(x_train, y_train, x_test): lgb_test = cv_model(lgb, x_train, y_train, x_test, \u0026#34;lgb\u0026#34;) return lgb_test def loss(y_p,y_t): y_p=np.array(y_p) y_t=np.array(y_t) loss=sum(sum(abs(y_p-y_t))) return loss lgb_test = lgb_model(X_train, y_train, X_test) ","permalink":"https://fishdel.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98lightgbm/","summary":"\u003ch2 id=\"lightgbm\"\u003eLightGBM\u003c/h2\u003e\n\u003ch3 id=\"介绍\"\u003e介绍\u003c/h3\u003e\n\u003cp\u003eLightGBM（Light Gradient Boosting Machine）：一个实现GBDT算法的框架，解决GBDT在海量数据遇到的问题。\u003c/p\u003e\n\u003ch5 id=\"两大技术\"\u003e两大技术：\u003c/h5\u003e\n\u003cp\u003e（1）GOSS(Gradient-based One-Side Sampling)：减少样本数\u003c/p\u003e","title":"机器学习实战(LightGBM)"},{"content":"集成学习与随机森林 更新权重 Adaboost AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None)\nbase_estimator:可选参数，默认为DecisionTreeClassifier。 algorithm： 可选参数，默认为SAMME.R 循环训练，实例权重不断更新（不是是成本函数最小化，而是加入更多预测器）\nGradient Boosting 新预测器针对前一个预测器的残差进行拟合\nGradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0,random_state=42)\n提前停止法\n训练完之后测量每个阶段的训练验证误差，找到树的最优数量后重新训练\nerrors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\nbst_n_estimators = np.argmin(errors) + 1\n验证误差在连续某次未改善时停止训练\nxgboost xgbc = XGBClassifier(max_depth=2, learning_rate=1, n_estimators=2, # number of iterations or number of trees slient=0, objective=\u0026#34;binary:logistic\u0026#34; ) 不更新权重 投票分类器 基于多分类器的结果聚合\nvoting_clf = VotingClassifier(estimators=[ (\u0026rsquo;log_clf\u0026rsquo;, LogisticRegression()), (\u0026lsquo;svm_clf\u0026rsquo;, SVC(probability=True)), (\u0026lsquo;dt_clf\u0026rsquo;, DecisionTreeClassifier(random_state=10)), ], voting=\u0026lsquo;soft\u0026rsquo;) voting_clf.fit(X_train, y_train) voting_clf.score(X_test, y_test)\nbagging./pasting 有放回抽样。在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。\n1.通过设置参数 bootstrap=False来切换为无放回采样。 2.n_estimators=500，表示有有500个相同的决策器。 3.max_samples=100，表示在数据集上有放回采样 100 个训练实例。 4.n_jobs=-1，n_jobs 参数告诉 sklearn 用于训练和预测所需要 CPU 核的数量。（-1 代表着 sklearn 会使用所有空闲核） 5.oob_score=True，表示包外评估bag_clf.oob_score_ 随机森林\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n重要参数\nn_estimators，random_state，boostrap和oob_score 重要属性\n.estimators_ .oob_score_ .feature_importances_ 接口\napply，fit，predict，score和predict_proba ","permalink":"https://fishdel.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/","summary":"\u003ch1 id=\"集成学习与随机森林\"\u003e集成学习与随机森林\u003c/h1\u003e\n\u003ch2 id=\"更新权重\"\u003e更新权重\u003c/h2\u003e\n\u003ch3 id=\"adaboost\"\u003eAdaboost\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAdaBoostClassifier(base_estimator=None, n_estimators=50,\nlearning_rate=1.0, algorithm=’SAMME.R’,\nrandom_state=None)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebase_estimator:可选参数，默认为DecisionTreeClassifier。\u003c/li\u003e\n\u003cli\u003ealgorithm： 可选参数，默认为SAMME.R\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e循环训练，实例权重不断更新（不是是成本函数最小化，而是加入更多预测器）\u003c/p\u003e","title":"机器学习实战(集成学习与随机森林)"},{"content":"降维 主要动机 加速，可视化数据，节省空间\n缺点：丢失信息，计算密集，转换过程难以理解\n什么时候用到降维 分类前，加速；聚类前，可视化数据\n维度诅咒 高维数据集——非常稀疏——训练实例彼此远离——容易过拟合\n主要动机：特征过多，训练变得缓慢，难以寻找更好的解决方案\n例子：MNIST数据集中图像边界的像素全是白色，删除这些像素也不会丢失太多信息，或者相邻两个像素合并。\n降维的主要方法 投影：适用于一个分布接近于2D子空间的3D数据集\n流形学习：瑞士卷数据集\n投影 投影的方法：PCA （主成分分析）寻找训练集中可获得最大方差的轴。\n如何寻找训练集的主成分 奇异值分解（SVD）\n使用numpy提供的svd（）函数获得训练集的主成分 如果不是用sklearn的PCA类，就要将数据集做中心化处理.\nX_centered=X-X.mean(axis=0)` `U,s,V=np.linalg.svd(X_centered)` `c1=V.T[:,0]` `c2=V.T[:,1] 有了奇异值分解得到的V，任意选取前d个主成分点乘原始数据集便可实现在d维的投影。\nW2 = V.T[:,:2]` `X2D=X_centered.dot(W2) sklearn实现PCA from sklearn.decomposition import PCA` `pca=PCA(n_components=2)` `X2D=pca.fit_transform(X) 使用components_访问每一个主成分\n`pca.components_.T[:,0]` 方差解释率 pca.explained_variance_ratio_` `pca = PCA(n_components=0.95) 可以将 n_components 设置为 0.0 到 1.0 之间的浮点数，表明希望保留的方差比率 cumsum = np.cumsum(pca.explained_variance_ratio_)` `d = np.argmax(cumsum \u0026gt;= 0.95)+1 也可以保留相加足够大的方差部分维度\n也可以画图寻找拐点\nplt.plot(cumsum,linewidth=3)\nQ：假设你对一个 1000 维的数据集应用 PCA，同时设置方差解释率为 95%，你的最终数据集将会有多少维？\n可能是1-1000之间的任何数字，取决于数据集，\nPCA数据压缩 `pca = PCA(n_components=154)` `X_reduced = pca.fit_transform(X_train)` `X_recovered = pca.inverse_transform(X_reduced)` 几种不同的PCA 随机PCA `rnd_pca = PCA(n_components=154,svd_solver=\u0026#39;randomized\u0026#39;,random_state=42)` `X_reduced = rnd_pca.fit_transform(X_train)` 增量PCA：对于大型数据集可以划分成小批量，但是要在每个小批量里调用partial_fit方法 from sklearn.decomposition import IncrementalPCA` `n_batches = 100` `inc_pca = IncrementalPCA(n_components=154)` `for X_batch in np.array_split(X_train,n_batches):` `inc_pca.partial_fit(X_batch)` `X_reduced = inc_pca.transform(X_train)` Kernel PCA `from sklearn.decomposition import KernelPCA rbf_pca=KernelPCA(n_components=2,kernel=\u0026#39;rbf\u0026#39;,gamma=0.04) X_reduced=rbf_pca.fit_transform(X)` 为kPCA调参方法 引入模型，通过最优化模型表现调参\n`from sklearn.model_selection import GridSearchCV` `from sklearn.linear_model import LogisticRegression` `from sklearn.pipeline import Pipeline` `clf = Pipeline([` `(\u0026#34;kpca\u0026#34;,KernelPCA(n_components=2)),` `(\u0026#34;log_reg\u0026#34;,LogisticRegression(solver=\u0026#34;lbfgs\u0026#34;))` `])` `param_grid = [{` `\u0026#34;kpca__gamma\u0026#34;:np.linspace(0.03,0.05,10), \u0026#34;kpca__kernel\u0026#34;:[\u0026#34;rbf\u0026#34;,\u0026#34;sigmoid\u0026#34;]` `}]` `grid_search = GridSearchCV(clf,param_grid,cv=3)` `grid_search.fit(X,y)` 2.基于重建功能算误差\n`best_score = 0.0` `for gamma in np.linspace(0.01, 0.05, 10):` `for kernel in [\u0026#34;rbf\u0026#34;, \u0026#34;sigmoid\u0026#34;]:` `kpca = KernelPCA(n_components=27,fit_inverse_transform=True)` `X_reduced = kpca.fit_transform(X)` `X_preimage = kpca.inverse_transform(X_reduced )` score = mean_squared_error(X, X_preimage) if score \u0026gt; best_score: best_score = score best_parameters = {\u0026#39;gamma\u0026#39;:gamma,\u0026#39;kernel\u0026#39;:kernel} print(best_parameters,best_score) 各种PCA的选择 首选常规PCA，不适合内存的大型数据集用增量PCA，想要大幅度降低维度并且追求速度用随机PCA，非线性数据集用内核PCA\n流形学习 LLE from sklearn.manifold import LocallyLinearEmbedding lle=LocallyLinearEmbedding(n_components=2,n_neighbors=10) X_reduced=lle.fit_transform(X) ","permalink":"https://fishdel.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E9%99%8D%E7%BB%B4/","summary":"\u003ch2 id=\"降维\"\u003e降维\u003c/h2\u003e\n\u003ch3 id=\"主要动机\"\u003e主要动机\u003c/h3\u003e\n\u003cp\u003e加速，可视化数据，节省空间\u003c/p\u003e\n\u003cp\u003e缺点：丢失信息，计算密集，转换过程难以理解\u003c/p\u003e\n\u003ch3 id=\"什么时候用到降维\"\u003e什么时候用到降维\u003c/h3\u003e\n\u003cp\u003e分类前，加速；聚类前，可视化数据\u003c/p\u003e\n\u003ch3 id=\"维度诅咒\"\u003e维度诅咒\u003c/h3\u003e\n\u003cp\u003e高维数据集——非常稀疏——训练实例彼此远离——容易过拟合\u003c/p\u003e","title":"机器学习实战(降维)"},{"content":"决策树 分类树 八个重要参数 criterion： 决定不纯度的计算方法： 1）”entropy“，使用信息熵（Entropy） 2）”gini“，使用基尼系数（Gini Impurity）\n信息熵对不纯度更加敏感，对不纯度的惩罚最强，计算更复杂 splitter：输入“best”优先选择更重要的分支进行分类；输入“random”更加随机防止过拟合\n剪枝策略参数：\nmax_depth:一般为3 min_samples_leaf：一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发。类别不多选1，一般为5 min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。 max_features ：限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃 min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。 可视化方法 import graphviz dot_data = tree.export_graphviz(clf #训练好的模型 ,out_file = None ,feature_names= feature_name ,class_names=[\u0026ldquo;琴酒\u0026rdquo;,\u0026ldquo;雪莉\u0026rdquo;,\u0026ldquo;贝尔摩德\u0026rdquo;] ,filled=True #进行颜色填充 ,rounded=True #树节点的形状控制 ) graph = graphviz.Source(dot_data) graph 一个属性四个接口 属性：feature_importances_，能够查看各个特征对模型的重要性\nfit（）predict（）apply（）score（）\napply中输入测试集返回每个测试样本所在的叶子节点的索引\n基本流程 from sklearn import tree #导入需要的模块 clf = tree.DecisionTreeClassifier() #实例化模型对象 clf = clf.fit(X_train,y_train) #用训练集数据训练模型 result = clf.score(X_test,y_test) 计算全部特征的不纯度指标-\u0026gt;选择不纯度指标最优的特征来分支-\u0026gt;在第一个特征下计算不纯度-\u0026gt;选取不纯度指标继续分支（直到没有更多特征可用或者整体不纯度达到最优）\n所有接口中要求输入X_train和X_test的部分，输入的特征矩阵必须至少是一个二维矩阵。如果你的数据的确只有一个特征，那必须用reshape(-1,1)来给矩阵增维；如果你的数据只有一个特征和一个样本，使用reshape(1,-1)来给你的数据增维。\n回归树 重要参数，属性和接口 criterion：mse，friedman_mse,mae 属性接口如上。接口score返回的是R平方，并不是MSE if xgboost is not None: # not shown in the book` xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2) y_pred = xgb_reg.predict(X_val) val_error = mean_squared_error(y_val, y_pred) print(\u0026#34;Validation MSE:\u0026#34;, val_error) ","permalink":"https://fishdel.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%E5%86%B3%E7%AD%96%E6%A0%91/","summary":"\u003ch1 id=\"决策树\"\u003e决策树\u003c/h1\u003e\n\u003ch2 id=\"分类树\"\u003e分类树\u003c/h2\u003e\n\u003ch3 id=\"八个重要参数\"\u003e八个重要参数\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ecriterion：\n决定不纯度的计算方法：\n1）”entropy“，使用信息熵（Entropy）\n2）”gini“，使用基尼系数（Gini Impurity）\u003c/p\u003e","title":"机器学习实战(决策树)"},{"content":"import pandas as pd import numpy as np import matplotlib.pyplot as plt 数据导入 观察数据的具体情况，可以发现年龄变量Age和Cabin有缺失，然后Name，sex，Ticket，cabin和Embark是object类型，在后续的数据处理中要进行调整。\ndata_train = pd.read_csv(r\u0026#39;C:/Users/train.csv\u0026#39;) data_train.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB 再看看测试集\ndata_test= pd.read_csv(r\u0026#39;test.csv\u0026#39;) data_test.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 418 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB 把索引设置为乘客编号\ntest_process = test_process.set_index([\u0026#39;PassengerId\u0026#39;]) test_process 现在测试集长这样\n数据处理 缺失值处理 本次数据的缺失应该是完全随机的，不依赖于其他完全变量，所以可以采取删除和填补两种方式。cabin缺失过多，直接删除这一特征，不放心的话可以计算一些相关度或者画图看看情况。\n# 删除cabin train_process = data_train.drop([\u0026#39;Cabin\u0026#39;],axis=1) # 年龄数据进行缺失值填补 from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import GridSearchCV Age_df = train_process[[\u0026#39;Age\u0026#39;,\u0026#39;Survived\u0026#39;,\u0026#39;Pclass\u0026#39;,\u0026#39;SibSp\u0026#39;,\u0026#39;Parch\u0026#39;,\u0026#39;Fare\u0026#39;]] UnknowAge = Age_df[Age_df.Age.isnull()].values KnowAge = Age_df[Age_df.Age.notnull()].values #y是目标年龄，x是已知属性 y_train = KnowAge[:,0] x_train = KnowAge[:,1:] rfr = RandomForestRegressor(n_estimators=500,random_state=42) rfr.fit(x_train,y_train) predictedAges = rfr.predict(UnknowAge[:,1::]) Age_df.loc[ (Age_df.Age.isnull()), \u0026#39;Age\u0026#39; ] = predictedAges train_process.Age=Age_df.Age.astype(int) 年龄缺失值使用随机森林进行填补，建立回归方程进行拟合。\n测试集也要删除cabin变量和进行年龄缺失值的填补。\n#测试集 test_process = data_test.drop([\u0026#39;Cabin\u0026#39;],axis=1) test_process.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 418 non-null float64 9 Embarked 418 non-null object dtypes: float64(2), int64(4), object(4) memory usage: 32.8+ KB Age_df = test_process[[\u0026#39;Age\u0026#39;,\u0026#39;Pclass\u0026#39;,\u0026#39;SibSp\u0026#39;,\u0026#39;Parch\u0026#39;,\u0026#39;Fare\u0026#39;]] UnknowAge = Age_df[Age_df.Age.isnull()].values KnowAge = Age_df[Age_df.Age.notnull()].values #y是目标年龄，x是已知属性 y_train = KnowAge[:,0] x_train = KnowAge[:,1:] rfr = RandomForestRegressor(n_estimators=500,random_state=42) rfr.fit(x_train,y_train) predictedAges = rfr.predict(UnknowAge[:,1::]) Age_df.loc[ (Age_df.Age.isnull()), \u0026#39;Age\u0026#39; ] = predictedAges test_process.Age=Age_df.Age.astype(int) 文本数据处理 对文本数据名字进行处理，把名字的称谓，长度，前名提取出来并舍弃名字变量。\ndef change(df): df[\u0026#39;Called\u0026#39;] = df[\u0026#39;Name\u0026#39;].str.findall(\u0026#39;Miss|Mr|Ms\u0026#39;).str[0].to_frame() df[\u0026#39;Name_length\u0026#39;] = df[\u0026#39;Name\u0026#39;].apply(lambda x:len(x)) df[\u0026#39;First_name\u0026#39;] = df[\u0026#39;Name\u0026#39;].str.split(\u0026#39;,\u0026#39;).str[0] df = df.drop([\u0026#39;Name\u0026#39;],axis=1) change(train_process) change(test_process) TargetEncoder 把其他object类型变量进行编码处理。sklearn有很多种编码方式，target适用于特征无内在顺序，category数量 \u0026gt; 4的情况 one-hot适用于特征无内在顺序，category数量 \u0026lt; 4的情况。\nimport category_encoders from category_encoders import TargetEncoder X_train = train_process.iloc[:,2:] y_train = train_process.iloc[:,1] tar_encoder1 = TargetEncoder(cols=[\u0026#39;Sex\u0026#39;,\u0026#39;Ticket\u0026#39;,\u0026#39;Embarked\u0026#39;,\u0026#39;Called\u0026#39;,\u0026#39;Name_length\u0026#39;,\u0026#39;First_name\u0026#39;], handle_missing=\u0026#39;value\u0026#39;, handle_unknown=\u0026#39;value\u0026#39;) tar_encoder1.fit(X_train,y_train) TargetEncoder(cols=[\u0026#39;Sex\u0026#39;, \u0026#39;Ticket\u0026#39;, \u0026#39;Embarked\u0026#39;, \u0026#39;Called\u0026#39;, \u0026#39;Name_length\u0026#39;, \u0026#39;First_name\u0026#39;]) X_train_encoded = tar_encoder1.transform(X_train) X_train_encoded.drop([\u0026#39;Name\u0026#39;],axis=1) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X_test = test_process X_test.drop([\u0026#39;Name\u0026#39;],axis=1) .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X_test_encoded = tar_encoder1.transform(X_test) 归一化 后面要多模型验证，所以要把数据归一化。\nimport sklearn.preprocessing as preprocessing scaler = preprocessing.StandardScaler() scaler.fit(X_train_encoded[[\u0026#39;Age\u0026#39;,\u0026#39;Fare\u0026#39;]]) scaler.fit(X_test_encoded[[\u0026#39;Age\u0026#39;,\u0026#39;Fare\u0026#39;]]) StandardScaler() X_train_encoded[[\u0026#39;Age\u0026#39;,\u0026#39;Fare\u0026#39;]] = scaler.transform(X_train_encoded[[\u0026#39;Age\u0026#39;,\u0026#39;Fare\u0026#39;]]) X_test_encoded[[\u0026#39;Age\u0026#39;,\u0026#39;Fare\u0026#39;]] = scaler.transform(X_test_encoded[[\u0026#39;Age\u0026#39;,\u0026#39;Fare\u0026#39;]]) 模型预测 from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.ensemble import VotingClassifier from sklearn.svm import SVC X_train_encoded X_test_encoded .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X_train_encoded.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Pclass 891 non-null int64 1 Sex 891 non-null float64 2 Age 891 non-null int32 3 SibSp 891 non-null int64 4 Parch 891 non-null int64 5 Ticket 891 non-null float64 6 Fare 891 non-null float64 7 Embarked 891 non-null float64 8 Called 891 non-null float64 9 Name_length 891 non-null float64 10 First_name 891 non-null float64 dtypes: float64(7), int32(1), int64(3) memory usage: 73.2 KB 投票法 先看看投票法\nlr_clf = LogisticRegression(penalty=\u0026#39;l1\u0026#39;,solver=\u0026#39;saga\u0026#39;,n_jobs=-1,max_iter=20000) rnd_clf = RandomForestClassifier(n_estimators=300,max_depth=8,min_samples_leaf=1,min_samples_split=5,random_state=42) svm_clf = SVC(C=2,kernel=\u0026#39;poly\u0026#39;,random_state=42,probability=True) voting_clf = VotingClassifier(estimators=[(\u0026#39;lr\u0026#39;,lr_clf),(\u0026#39;rf\u0026#39;,rnd_clf),(\u0026#39;scv\u0026#39;,svm_clf)],voting=\u0026#39;soft\u0026#39;) voting_clf.fit(X_train_encoded,y_train) VotingClassifier(estimators=[(\u0026#39;lr\u0026#39;, LogisticRegression(max_iter=20000, n_jobs=-1, penalty=\u0026#39;l1\u0026#39;, solver=\u0026#39;saga\u0026#39;)), (\u0026#39;rf\u0026#39;, RandomForestClassifier(max_depth=8, min_samples_split=5, n_estimators=300, random_state=42)), (\u0026#39;scv\u0026#39;, SVC(C=2, kernel=\u0026#39;poly\u0026#39;, probability=True, random_state=42))], voting=\u0026#39;soft\u0026#39;) y_test = pd.read_csv(r\u0026#39;C:/Users/gender_submission.csv\u0026#39;) y_test = y_test[\u0026#39;Survived\u0026#39;] from sklearn.metrics import accuracy_score for clf in (lr_clf,rnd_clf,svm_clf,voting_clf): clf.fit(X_train_encoded,y_train) y_pred = clf.predict(X_test_encoded) print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) LogisticRegression 0.6961722488038278 RandomForestClassifier 0.80622009569378 SVC 0.6363636363636364 VotingClassifier 0.8110047846889952 再试试XGBoost，果然效果比较好。\nXGBoost import xgboost from sklearn.metrics import mean_squared_error xgb_reg = xgboost.XGBRFRegressor(random_state=42) xgb_reg.fit(X_train_encoded,y_train) y_pred = xgb_reg.predict(X_test_encoded) val_error=mean_squared_error(y_test,y_pred) print(\u0026#34;Validation MSE:\u0026#34;, val_error) Validation MSE: 0.5023153196818051 ","permalink":"https://fishdel.github.io/posts/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E9%A2%84%E6%B5%8B/","summary":"\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003epandas\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003epd\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003enumpy\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003enp\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nn\"\u003eplt\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch1 id=\"数据导入\"\u003e数据导入\u003c/h1\u003e\n\u003cp\u003e观察数据的具体情况，可以发现年龄变量Age和Cabin有缺失，然后Name，sex，Ticket，cabin和Embark是object类型，在后续的数据处理中要进行调整。\u003c/p\u003e","title":"机器学习实战（泰坦尼克号预测）"},{"content":"训练模型 训练模型的方法 使用成本函数最小的参数 标准方程法 SVD奇异值分解 迭代优化，使用梯度下降 批量梯度下降 随机梯度下降 小批量梯度下降 训练模型方法的问题 训练集有数百万特征：（使用迭代优化）随机梯度下降和小批量梯度下降，若训练集可以容纳于内存，使用批量梯度下降\n训练集里特征的数值大小迥异 先放缩，再使用梯度下降 使用标准方程法或SVD 使用批量梯度下降，并在每个轮次绘制验证误差。如果验证误差持续上升 如果训练错误增加：学习率过高，如果训练错误没增加，可能过拟合——简化模型 使用多项式回归时，训练误差和验证误差之间差距大 验证误差远高于训练误差\n过拟合\n减少多项式阶数 添加L1或l2至成本函数 增加训练集的大小 使用岭回归时，训练误差和验证误差差不多且都相当高 欠拟合，减少正则化参数alpha 两种用于分类的模型 Logistic回归 对数几率回归：把线性回归的结果，通过sigmoid函数，从(-∞,∞)映射到(0,1) 成本函数为凸函数，梯度下降不会陷入局部最优 Softmax回归 逻辑回归只能用于二分类，通过softmax函数扩展到多分类问题 成本函数为交叉熵成本函数 多项式回归（可能会过拟合） 处理非线性关系的数据：将每个特征的幂次方添加为一个新特征，在扩展的特征集上训练一个线性模型 poly_features = PolynomialFeatures(degree=2, include_bias=False) “斜率”参数（w，也叫作权重或系数）被保存在 coef_ 属性中，而偏移或截距（b）被保 存在 intercept_ 属性中 若有（a，b）两个特征，使用degree=2的二次多项式则为（1，a, a^2, ab, b ,b^2)。 参数：\ndegree：度数，决定多项式的次数\ninteraction_only： 默认为False，字面意思就是只能交叉相乘，不能有a^2这种.\ninclude_bias: 默认为True, 这个bias指的是多项式会自动包含1，设为False就没这个1了\n欠拟合——两条曲线接近且都很高 过拟合——曲线之间存在很大的间隙\n正则化 正则化线性模型 岭回归\nalpha：正则化系数，float类型，默认为1.0。 fit_intercept：是否需要截距b，默认为True。 normalize：是否先进行归一化，默认为False。 copy_X：是否复制X数组，否则覆盖，默认为True。 max_iter：最大的迭代次数，int类型，默认为None。 solver：求解方法，str类型，默认为auto。可选参数为：auto、svd、cholesky、lsqr、sparse_cg、sag。\nridge_reg =Ridge(alpha=1, solver=\u0026ldquo;cholesky\u0026rdquo;, random_state=42)\nsgd_reg = SGDRegressor(penalty=\u0026lsquo;l2\u0026rsquo;,max_iter=50,tol=-np.infty,random_state=42)\n也可以这样用 Lasso回归\nlasso_reg = Lasso(alpha=0.1)\nSGDRegressor(penalty=\u0026lsquo;l1\u0026rsquo;)\n经常把特征的权重降低为0，只有很少的特征重要时选他\n弹性网络\nelastic_net = ElasticNet(alpha=0.1,l1_ratio=0.5,random_state=42) 二者的一个混合，容易产生一些异常 早期停止法\nsgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None, learning_rate=\u0026ldquo;constant\u0026rdquo;, eta0=0.0005, random_state=42)\n当 warm_start=True 时，调用 fit() 方法后，训练会从停下来的地方继续，而不是从头重新开始\n正则化 正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。损失函数由原来的E(X,Y)变为E(X,Y)+alpha||w||，w是模型系数组成的向量（有些地方也叫参数parameter，coefficients），||·||一般是L1或者L2范数，alpha是一个可调的参数，控制着正则化的强度。当用在线性模型上时，L1正则化和L2正则化也称为Lasso和Ridge。\n","permalink":"https://fishdel.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/","summary":"\u003ch1 id=\"训练模型\"\u003e训练模型\u003c/h1\u003e\n\u003ch2 id=\"训练模型的方法\"\u003e训练模型的方法\u003c/h2\u003e\n\u003ch3 id=\"使用成本函数最小的参数\"\u003e使用成本函数最小的参数\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e标准方程法\u003c/li\u003e\n\u003cli\u003eSVD奇异值分解\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"迭代优化使用梯度下降\"\u003e迭代优化，使用梯度下降\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e批量梯度下降\u003c/li\u003e\n\u003cli\u003e随机梯度下降\u003c/li\u003e\n\u003cli\u003e小批量梯度下降\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"训练模型方法的问题\"\u003e训练模型方法的问题\u003c/h2\u003e\n\u003cp\u003e训练集有数百万特征：（使用迭代优化）随机梯度下降和小批量梯度下降，若训练集可以容纳于内存，使用批量梯度下降\u003c/p\u003e","title":"机器学习实战(训练模型)"},{"content":"支持向量机 线性SVM分类 硬间隔分类：让所有实例都在正确的一边的分类。 硬间隔变成软间隔：引入松弛变量C。 C是调节间隔与准确率的因子，C值越大，越不愿放弃那些离群点；c值越小，越不重视那些离群点。（模型过拟合，C值调小进行正则化）\n软间隔分类：在“街道”的宽度和间隔违例（错误分类）之间找到良好的平衡的分类。 svm_clf = SVC(kernel=\u0026ldquo;linear\u0026rdquo;, C=float(\u0026ldquo;inf\u0026rdquo;)) 线性核函数\n非线性SVM分类 通过非线性变换，将非线性问题变为线性问题 添加特征\n指定kernel=“ploy”其中γ、r、d属于超参，需要调参定义 添加相似特征\n指定kernel=“rbf”，其中γ属于超参，要求大于0，需要调参定义（过拟合降低，欠拟合提升）\n增加gamma值，钟形曲线变得更窄会变得更窄，每个实例的影响范围更小，减小gamma变的更平坦\n核函数的选取 高维用线性，不行换特征；低维试线性，不行换高斯 SVM回归 参数 参数 解释 C 惩罚项参数 loss 损失函数。当值为epsilon_insensitive时损失函数为L（它是标准SVR的损失函数）；值为square_epsilon_insensitive时表示为L的平方 epsilon 浮点数，用于loss中的sigma参数 dual 布尔值。如果为True，则解决对偶问题，如果为False，则解决原始问题，当n_samples\u0026gt;n_features时，倾向于采用False tol 浮点数，指定终止迭代的阈值 fit_intercept 布尔值，如果为True，则计算截距，即决策函数中的常数项；否则忽略截距 属性 coef_：一个数组，给出了各个特征的权重。\nintercept_：一个数组，隔出了截距，即决定函数中的常数项。\n方法 fit（x, [,y]）:训练模型。 predict（x）:用模型进行预测，返回预测值 score（x,y[,sample_weight]）:返回(x,y)上预测的准确率 基本思想 在类之间拟合可能最宽的街道，寻找最大的决策边界 支持向量：决策边界位于“街道”边缘的实例 放缩的原因：支持向量机拟合类别之间可能的、最宽的“街道”，所以如果训练集不经缩放，SVM将趋于忽略值较小的特征。 当训练实例的数量小于特征数量时，解决对偶问题比原始问题更迅速。 Sklearn构建的SVM分类器 参数 解释 C 惩罚项 kernel 核函数类型，str类型，默认为’rbf’ degree 多项式核函数的阶数，int类型，可选参数，默认为3 gamma 核函数系数，float类型，可选参数，默认为auto。只对’rbf’ ,’poly’ ,’sigmod’有效。如果gamma为auto，代表其值为样本特征数的倒数，即1/n_features。 coef0 核函数中的独立项，float类型，可选参数，默认为0.0。只有对’poly’ 和,’sigmod’核函数有用，是指其中的参数c。 ","permalink":"https://fishdel.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","summary":"\u003ch1 id=\"支持向量机\"\u003e支持向量机\u003c/h1\u003e\n\u003ch2 id=\"线性svm分类\"\u003e线性SVM分类\u003c/h2\u003e\n\u003ch3 id=\"硬间隔分类让所有实例都在正确的一边的分类\"\u003e硬间隔分类：让所有实例都在正确的一边的分类。\u003c/h3\u003e\n\u003ch3 id=\"硬间隔变成软间隔引入松弛变量c\"\u003e硬间隔变成软间隔：引入松弛变量C。\u003c/h3\u003e\n\u003cp\u003eC是调节间隔与准确率的因子，C值越大，越不愿放弃那些离群点；c值越小，越不重视那些离群点。（模型过拟合，C值调小进行正则化）\u003c/p\u003e\n\u003ch3 id=\"软间隔分类在街道的宽度和间隔违例错误分类之间找到良好的平衡的分类\"\u003e软间隔分类：在“街道”的宽度和间隔违例（错误分类）之间找到良好的平衡的分类。\u003c/h3\u003e\n\u003cp\u003esvm_clf = SVC(kernel=\u0026ldquo;linear\u0026rdquo;, C=float(\u0026ldquo;inf\u0026rdquo;))\n线性核函数\u003c/p\u003e","title":"机器学习实战(支持向量机)"},{"content":"密码技术 1.密码体制分类 对称密码体制 优点 缺点 加/解密速度快 密钥分发需要安全通道 明文长度和密文一样 密钥量大难以管理 密钥比较短 无法解决不可否认性 非对称密码体制 优点 缺点 密钥分发相对容易 加/解密速度慢 密钥管理容易 密文长度较长 能实现数字签名 密钥长度长 2.密码体制的攻击 唯密文攻击：攻击者知道部分密文； 已知明文攻击：攻击者者知道部分密文和对应的明文； 选择明文攻击：攻击者可以选择任意明文，并在同一密钥下得到相应密文；（控制加密器） 选择密文攻击：攻击者可以选择任意密文，并得到对应解密的明文；（控制解密器） 选择文本攻击：同时控制加密器和解密器。 3.传统密码体制 置换密码（重排） 代换密码（替换） 单字母代换 多字母代换 单表代换 （移位，替换，仿射） 多表代换 仿射密码 e=（ax+b）mod26 d= a^-1(x-y)mod26\n多表代换vigenere 分组 -\u0026gt;模26-\u0026gt;加上密钥\n统计分析法\n4.现代对称密码体制 DES 分组长度64位 密钥长度56位 其中8位为奇偶校验位 流程：明文64bit——\u0026gt;初始IP变换——\u0026gt;(64bit密钥进行子密钥生成)乘积变换——\u0026gt;逆初始变换——\u0026gt;输出64比特密文 5.非对称密码体制 RSA基本原理： 1.密钥产生： 随机生成大素数p,q,n=p*q,欧拉函数O(n)=(p-1)(q-1) 随机选择密钥k,k和O(n)互质，若sk=k,pk就是sk的逆元 2.加密过程与解密过程 Ci=mi^sk mod n Mi=ci^pk mod n 安全性取决与大素数分解的难度和p,q的保密性 信息认证技术 1.哈希函数MD5 以512为分组来处理数据，分为16个32位子数组——\u0026gt;输出4个32位子数字级联成128位散列值\n数据填充与分组：最后一位为448位，将填充前的消息L转为64位，（保留最后64位）填充到后448位那里 2.消息认证技术 （检验消息是否真实）\n消息加密：将整个消息的密文作为认证码 对称加密，添加校验码，公钥加密 哈希函数：将哈希函数产生的散列值作为消息认证码 使用哈希函数，保证机密性的哈希函数，混合加密认证 消息认证码：将消息和密钥一起作为认证码 3.数字签名 直接数字签名：依赖于发送方私人密钥的安全性，若丢失或被盗用，签名被伪造。 仲裁数字签名： 方案1：对称加密算法（报文M以明文形式发送） 方案2：对称加密算法，密文传输（仲裁者两方勾结） 方案3：公开密钥算法，密文传输（） 4.身份认证 常见的重放攻击：1.简单重放；2.可检测的重放；3.不可检测的重复； 4.不加修改的逆向重放 重放攻击的预防：序列号，时间戳，随机数 基于对称密钥的身份认证 特点 Needham-Schroeder (随机数) 第三步易受攻击 Denning（时间戳） 无法安全同步网络时钟,抑制—重放攻击，报文中的时间戳快了 Numan-Stubblebine 解决抑制—重放攻击 基于公钥的消息认证 特点 Denning-Sacco 引入时间戳，但需严格时钟同步 Woo-lam 用随机数作为临时交互值 计算机病毒 1.计算机病毒特征 隐蔽性，寄生性，可触发性，传染性，破坏性\n2.常见的病毒类型 引导型病毒，文件型病毒，网络蠕虫，计算机木马\n3.病毒的检测 特征代码法：对每种病毒样本抽取特征代码，根据该代码特征进行病毒检测； 校验和法：计算文件内容的校验和； 行为检测法：利用病毒特有行为特征进行检测 软件模拟法：两次特征代码法，识别未知病毒\n网络攻击与防范 1.分类 分类 举例 窃听 网络监听，获取密码，网络监听 欺骗 获取密码，恶意代码，网络欺骗 拒绝服务 数据驱动攻击 缓冲区溢出 2.步骤 目标检测——端口扫描——网络监听——实施攻击——撤退\n防火墙技术 1. 体系结构 堡垒主机体系结构 堡垒主机是一种被强化的可以防御攻击的计算机，被暴露于因特网之上，作为进入内部网络的一个检查点（checkpoint），以达到把整个网络的安全问题集中在某个主机上解决。正是由于这个原因，防火墙的建造者和防火墙的管理者应尽力给予其保护，特别是在防火墙的安装和初始化的过程中应予以仔细保护\n双宿主主机体系结构 双宿主主机的防火墙系统由一台装有两个网卡的堡垒主机构成。两个网卡分别与外部网及内部网相连。堡垒主机上运行防火墙软件，可以转发数据、提供服务等。堡垒主机将防止在外部网络和内部系统之间建立任何直接的连接，可以确保数据包不能直接从外部网络到达内部网络。 双宿主主机有两个接口，具有以下特点。 （1）两个端口之间不能进行直接的IP数据包的转发。 （2）防火墙内部的系统可以与双宿主主机进行通信，同时防火墙外部的系统也可以与双宿主主机进行通信，但二者之间不能直接进行通信。 这种体系结构的优点是结构非常简单，易于实现，并且具有高度的安全性，可以完全阻止内部网络与外部网络通信 屏蔽主机体系结构 这种结构的堡垒主机位于内部网络，而过滤路由器按以下规则过滤数据包：任何外部网（如 Internet）的主机都只能与内部网的堡垒主机建立连接，甚至只有提供某些类型服务的外部网主机才被允许与堡垒主机建立连接。任何外部系统对内部网络的操作都必须经过堡垒主机，同时堡垒主机本身就要求有较全面的安全维护。包过滤系统也允许堡垒主机与外部网进行一些“可以接受（即符合站点的安全规则）”的连接\n屏蔽子网体系结构 在最简单的屏蔽子网体系结构中，有两台都与边界网络相连的过滤路由器，一台位于边界网络与内部网络之间，而另一台位于边界网络与外部网络之间。在这种结构下，入侵者要攻击到内部网必须通过两台路由器的安全控制，即使入侵者通过了堡垒主机，它还必须通过内部路由器才能抵达内部网，因此整个网络安全机制就不会因一个站点攻破而全部瘫痪。\n","permalink":"https://fishdel.github.io/posts/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/","summary":"\u003ch1 id=\"密码技术\"\u003e密码技术\u003c/h1\u003e\n\u003ch2 id=\"1密码体制分类\"\u003e1.密码体制分类\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e对称密码体制\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e优点\u003c/th\u003e\n          \u003cth\u003e缺点\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e加/解密速度快\u003c/td\u003e\n          \u003ctd\u003e密钥分发需要安全通道\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e明文长度和密文一样\u003c/td\u003e\n          \u003ctd\u003e密钥量大难以管理\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e密钥比较短\u003c/td\u003e\n          \u003ctd\u003e无法解决不可否认性\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e非对称密码体制\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e优点\u003c/th\u003e\n          \u003cth\u003e缺点\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e密钥分发相对容易\u003c/td\u003e\n          \u003ctd\u003e加/解密速度慢\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e密钥管理容易\u003c/td\u003e\n          \u003ctd\u003e密文长度较长\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e能实现数字签名\u003c/td\u003e\n          \u003ctd\u003e密钥长度长\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"2密码体制的攻击\"\u003e2.密码体制的攻击\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e唯密文攻击：攻击者知道部分密文；\u003c/li\u003e\n\u003cli\u003e已知明文攻击：攻击者者知道部分密文和对应的明文；\u003c/li\u003e\n\u003cli\u003e选择明文攻击：攻击者可以选择任意明文，并在同一密钥下得到相应密文；（控制加密器）\u003c/li\u003e\n\u003cli\u003e选择密文攻击：攻击者可以选择任意密文，并得到对应解密的明文；（控制解密器）\u003c/li\u003e\n\u003cli\u003e选择文本攻击：同时控制加密器和解密器。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"3传统密码体制\"\u003e3.传统密码体制\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e置换密码（重排）\u003c/li\u003e\n\u003cli\u003e代换密码（替换）\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003e单字母代换\u003c/th\u003e\n          \u003cth\u003e多字母代换\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e单表代换 （移位，替换，仿射）\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e多表代换\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e仿射密码\u003c/strong\u003e\ne=（ax+b）mod26\nd= a^-1(x-y)mod26\u003c/p\u003e","title":"数据安全"},{"content":"","permalink":"https://fishdel.github.io/about/","summary":"about","title":"关于"}]