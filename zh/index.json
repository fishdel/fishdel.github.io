[{"content":"1、音名与唱名，全音与半音\n音名和唱名就是两种不同的表示音的方式。\n音名是用英文字母C、D、E、F、G、A、B来表示音的名称，音名就像我们身份证上的名字一样，是不会改变的。我们吉他指板上每一根弦在每一个品格中就有一个固定的音名，是固定的，永远不会改变的。\n唱名在不同的调里面可以有不同的存在方式。例如，在C大调里，C音的唱名是do，但在D大调里，D音的唱名是do。唱名是用音名来表示音的相对位置，如用do、re、mi、fa、so、la、si或1、2、3、4、5、6、7来表示它们对应的C、D、E、F、G、A、B。\n我们需要记住一点就是，在自然大调中E音和F音，B音和C音永远是半音关系，7和1，3和4永远是半音关系。（至于为什么，这就涉及到了自然大调音阶的排列方式，即在自然大调中，音与音之间的关系是全全半全全全半的关系，也就是我们听着像是1、2、3、4、5、6、7这样的关系。）目前我们只需要记住它们是半音关系就可以了。所以，在自然大调中，只要没有其他的符号出现，比如升降音符号，没有这些符号出现的话，那么上面说的3和4，7和1，E和F，B和C，它们永远都是半音关系，除了上面四组，其他的音都是全音关系。比如1到2， 2到3，那在音名中就是C到D， D到E，这些全都是全音关系。\n有了这些全半音关系，那么全半音体现在吉他指板上是什么形式.\n全音就是吉他指板上间隔一个品，比如一弦的第三品这个音，那么从一弦的三品到一弦的五品，这两个音之间就是个全音关系，它们中间间隔了一个四品。\n半音在吉他指板上就是相邻的品，比如吉他一弦三品到一弦的四品，它们中间没有间隔音，也就是相邻的，所以叫半音。\n总结来说就是吉他指板上中间间隔一个品的这两个音之间就是全音关系，相邻的品就是半音关系。\n**注意：**每一根弦的空弦音到它的一品都是半音关系，比如说一弦的空弦到一弦的一品，这两个音之间就是半音关系；一弦的空弦到一弦的二品，这两个音之间就是全音关系。\n2、十二平均律和升降号 我们前面讲了音名和唱名已经全半音关系，比如吉他指板上六弦二品到六弦三品的音就是半音，从六弦二品到六弦四品的音就是全音。从空弦到这个弦的一品是半音，从空弦到这个弦的二品是全音。这是对全音和半音有了最初步的了解。\n现在，我们来说说十二平均率以及升降号的问题。我们在一些谱子中会看到升降号（♯或是♭），那升降号是怎么来的？有什么用呢？\n我们知道，在自然大调中，E和F，B和C，3和4，7和1永远是半音，其他的音名和唱名之间都是全音关系，在吉他指板上也能够体现，中间隔了一个品的两个音是全音，相邻的两个品的音是半音。有人肯定就会问，那中间隔的那个品是什么音？这个问题问得好。这就是我接下来要说的十二平均律。\n自然大调音阶是以全全半全全全半这样的音程关系来定义的，既然1和2，2和3，4和5，5和6，6和7之间都是全音，也就是两个半音，那么是不是在它们之间还有别的音呢？答案是对的，我们把全音都拆成半音来看，比如，我们可以把1升高半音，升高半音后的1就是♯1，也就是我们吉他指板上的间隔的那个品的音，既然是中间的品，那就是前后两个音共用的一个品，也就是说在1和2中间的这个音既是1升高半音也是2降低半音，所以2降低半音后就是♭2，在听感上，♯1和♭2是一样的，但是在不同的调里，它们的功能是不一样的。所以音名也一样，C和D是全音，在吉他指板上也是间隔一个品的，所以这个中间的品既是♯C，又是♭D，C大调音阶中的其他的音名和唱名也是一样的道理。所以我们就把1、2、3、4、5、6、7拆成了12份，把它们展开来就是1、（♯1/♭2）、2、（♯2/♭3）、3、4、（♯4/♭5）、5、（♯5/♭6）、6、（♯6/♭7）、7。音名也一样可以拆成C、（♯C/♭D）、D、（♯D/♭E）、E、F、（♯F/♭G）、G、（♯G/♭A）、A、（♯A/♭B）、B。这样我们来数一数，是不是把它们分成了12份，每一份都是半音关系。这就是十二平均律。\n我们可以通过使用升降号来改变一个音的高低，也可以改变两个音之间的距离。比如在自然大调中，E和F，3和4是半音，那我们把E降低半音，变成了♭E，那原本E和F是半音，现在把E降低半音后，♭E和F是不是就成了两个半音的关系，也就是全音了呢？同样，我们把F升高半音，变成了♯F，那原本E和F是半音，现在是不是E和♯F成了两个半音的关系，也就是全音了呢？\n再举个例子，C和D，它们之间是两个半音的关系，也就是全音，那我们把C升高半音，就是♯C，那♯C和D的距离是不是就缩了一半，从原来的C和D两个半音变成了现在的♯C和D的一个半音。我们也可以把D降低半音，变成♭D，那原本的C和D全音的关系是不是就在D降低半音后变成了C和♭D的一个半音的关系了？其他的音名和唱名都是同样的道理。\n3、音程 大三度、小三度、大二度、小二度，以及我们前面提到的全音、半音，这些都是音程。通俗来说音程就是指两个音之间的关系，也就是两个音之间的距离。\n音程分为自然音程和变化音程，自然音程就是从某一个音到另一个音，它们之间任何一个度都叫做自然音程。变化音程就是指带有升降号变化的音程，也就是说用升降号来体现的一个音程关系，这叫做变化音程。我们主要来讲自然音程。\n①音程的概念： 音程是从这个音开始，连同这个音在内，到另一个音，也包括那个音，有几个音，就是几度。\n②音程的种类： 一度音程\n从1到1，从2到2，从3到3……，就是从这个音到下一个音没有任何变化，就叫做纯一度。\n二度音程\n从1到2，从2到3，从3到4……。它们就叫做二度音。二度音分为两种，一种是大二度，一种是小二度。我们前面讲的全音和半音（其实一个全音等于两个半音，即全音=半音+半音）就是二度关系，所以两个音之间如果是两个半音音程，那就是大二度，比如1到2，它们之间就是全音关系，也就是两个半音，它们就是大二度关系。3到4，它们之间是半音关系，也就是它们之间只有一个半音，所以它们是小二度关系。其他的音与音之间的二度关系以次类推。\n三度音程\n从1到3，从2到4，从3到5……，这些音程是三个音，因为从1到3之间还有一个2，从2到4之间还有一个3，从3到5之间还有一个4，它们叫做三度音程。三度音程分大三度和小三度。从1到3中间是没有半音的，也就是1到2是两个半音，从2到3也是两个半音，也就是从1到3这之间有四个半音，即两个全音，所以说它叫做大三度；从3到5中间包含一个4，也就是3到4是一个半音，4到5是两个半音，也就是从3到5之间有三个半音，所以它叫做小三度。因此，大三度和小三度的区别在于包含了几个半音。如果是四个半音，那就是大三度，如果是三个半音，那就是小三度。\n四度音程\n从1到4，2到5，3到6，4到7，5到1，6到2，7到3，这些都是四度， 以上这些四度音有一组音中间是不包含半音的，那就是从4到7，也就是4、5、6、7，这四个音之间是没有半音的，也就是说只有这一组音是没有半音的，我们把它称为增四度，其余的四度音中只要包含半音的就叫做纯四度。\n五度音程\n从1到5，2到6，3到7，4到1，5到2，6到3，7到4，都是五个音，就是五度，这种五度叫做纯五度。7到4这五个音中包含两个半音，即7和1，3和4是半音，这一组叫做减五度，其余的五度音程叫做纯五度。\n六度音程\n从1到6，从2到7，从3到1，从4到2，从5到3，从6大4，从7到5。六度音程也分两种，一种叫做大六度，就是音程中包含了一个半音，比如1到6，2到7。另一种叫做小六度，包含两个半音，比如3到1，7到5\n七度音程\n从1到7，从2到1，3到2，4到3，5到4，6到5，7到6。七度音程也是跟六度音程是一样的，也分为大七度和小七度。从2到1，3到2，5到4，6到5，7到6，这几组音程中都包含了两个半音，即3和4，7和1，所以这几组音叫小七度。剩余1到7和4到3，这两组音程中只包含了一个半音，所以叫大七度。\n八度音程\n从中音1到高音1，从中音2到高音2，从中音3到高音3……这种就叫做八度，就是纯八度。\n4、和弦 和弦的概念 和弦是指由三个或三个以上的音按照三度关系叠加，依次或同时发生，叫做和弦。\n我们需要注意的是构成和弦的三个条件：①三个或三个以上的音②以三度关系叠加③同时或依次发声。\n和弦的种类 常见的和弦有三和弦、七和弦、九和弦、十一和弦、十三和弦，还有一些挂留和弦、add和弦、减三和弦、增三和弦等等。我们慢慢来讲这些和弦。\n三和弦\n三和弦是和弦系统里最基本的和弦，就是我们常听到的像C和弦、 G和弦、 F和弦等等，这些都叫做三和弦，还有Am和弦、 Em和弦也都叫做三和弦。三和弦分为两种，一种是大三和弦，一种是小三和弦，还有减三和弦、增三和弦，因为我们用得少，所以暂时不讲。\n","permalink":"https://fishdel.github.io/zh/posts/%E4%B9%90%E7%90%86%E5%9F%BA%E7%A1%80/","summary":"\u003cp\u003e\u003cstrong\u003e1、音名与唱名，全音与半音\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e音名和唱名就是两种不同的表示音的方式。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e音名\u003c/strong\u003e是用英文字母C、D、E、F、G、A、B来表示音的名称，音名就像我们身份证上的名字一样，是不会改变的。我们吉他指板上每一根弦在每一个品格中就有一个固定的音名，是固定的，永远不会改变的。\u003c/p\u003e","title":"乐理基础"},{"content":"金色冬天 广州的冬天 是倒叙的秋 让人忍不住 想谈谈风花雪月 想谈谈江湖沧桑 但我不能谈 因为我的工位终日不见阳光 在这里 无论是文学还是音乐 都被视为精神水货 只剩虚无和沮丧\n","permalink":"https://fishdel.github.io/zh/posts/%E9%87%91%E8%89%B2%E5%86%AC%E5%A4%A9/","summary":"\u003ch3 id=\"金色冬天\"\u003e金色冬天\u003c/h3\u003e\n\u003cp\u003e广州的冬天\n是倒叙的秋\n让人忍不住\n想谈谈风花雪月\n想谈谈江湖沧桑\n但我不能谈\n因为我的工位终日不见阳光\n在这里\n无论是文学还是音乐\n都被视为精神水货\n只剩虚无和沮丧\u003c/p\u003e","title":"金色冬天"},{"content":"装不下 下班回家 但我无家可归 除非五小时高铁或者两小时直飞 于是逼仄的出租屋 也成为港湾 同样的十几平米 装得下 楼上一对情侣和一只柯基 却装不下 楼下一个25岁的人和她隐秘的梦 而她和她的梦 不知何去何从\n","permalink":"https://fishdel.github.io/zh/posts/%E8%A3%85%E4%B8%8D%E4%B8%8B/","summary":"\u003ch3 id=\"装不下\"\u003e装不下\u003c/h3\u003e\n\u003cp\u003e下班回家\n但我无家可归\n除非五小时高铁或者两小时直飞\n于是逼仄的出租屋\n也成为港湾\n同样的十几平米\n装得下\n楼上一对情侣和一只柯基\n却装不下\n楼下一个25岁的人和她隐秘的梦\n而她和她的梦\n不知何去何从\u003c/p\u003e","title":"装不下"},{"content":"表現原則 １．非典型名词谓语句 問題は山積みだ。\nこのビルはいつ完成（ですか）しますか？\n来年予定です。\n京都で（の）（撮影は）写真を撮るのが久々だ（った）。\nあなたの意見に賛成だ\n２．モダリティ表現 转述他人信息\n转述他人感觉\n对事情的认识和判断\n对听话人的语气\n「でも、し、とか、たり…」\n明日は晴れでしょう\nこのカレーはなんか変な味がする（【词义4】表示主观心理感受和印象。）\n時間も時間だし、今日はここまでにしよう\nそろそろ（もう十分）飲んだし、（そろそろ）帰ろう（か）。\n幽霊なん（か）てあるいるものか。\n彼は噓など言えない（言ったことはなかった）\n今夜には会議があるそうだ（ようだ、らしい）。\n今日はお忙しい（そう）ようで（すから）、（また）明後日お伺いいたします。\n３．話者中心性 誰かに傘が間違えられた。\nみんなに君のプレゼントが好きだ。（お土産はみんなに喜ばれた）\n大家さんに(から)家賃が増額の請求られたをされてしまった。\n喧嘩を解決することが頼まれたので、午後出かける必要です。\n(僕は午後外出しなければならない。争議の調停を頼まれてね)\n晩ご飯後、彼に（は）そのこと（を問われたのである）が聞かれた。\n４．非情の受け身 明日教会で結婚式があります（行われます）。\n大統領の専用機は~~三つの専機（~~戦闘機三機）~~で（~~に）守られて空港に到着した。\n中国茶はもともと薬として使われ~~った（~~ていた）。\n北京の夏はとても暑いので、熱中症に心配されている。\nコルナ~~により（~~なので）、オリンピックは無観客で行われる\n５．授受表现 先生、色々なことを教えてくれたのはありがとうございます。\n勘弁してくれよ\n昨日、田中さんに会って（さ）、駅前に送ってくれた。（駅まで送ってもらったんだ）\n隣の人に教科書を見せてもらった。（もらいました）\nお店にオリジナル衣服を作れてもらった（もらいました）。\n6．无生物主语 こ（あ）の映画に感動して泣いた（てしまった）。\n病気で（のため）学校に行けなかった。（登校できなかった）\n彼の涙に（はある種の期待が込められている）なんか期待を含む。\n洪水で町が壊られた。\n遠距離恋愛（なの）~~で彼らはとても（~~二人とも）つらい。\n表达方式 ～たらどうするんだ 無理をして、体が壊されたらどうすんだ。\n体を壊したら\n何があったらどうするんだ。責任できるか？\n何かあったら\n責任をとれるのか？\n止めてください！ボールが子供に当たったらどうするんだ？\n毎日遊ぶばかり、試験が落ちたらどうするんだ？\n毎日遊んでばかりで\n試験に落ちたら\n走るな！転んだらどうするんだ。\nもし誰か聞きたらどうするんだ。\n誰かに聞かれたら\n情報が漏れたらどうするんだ。\n妻と子供はに何があったらどうするんだ。\n2.　とは限らない 高いものは必ずいいとは限らない。\n天気予報がずっと当たるとは限らない。\nいつも正しい\n美味しそうな食べ物が美味しいとは限らない。\n美味しそうに見える食べ物\n彼の言葉が真実だとは限らない。\n彼の言っていることが本当だ\n美人だったらと言って、モテルとは限らない。\n（并不是所有的放弃都是消极的）\n子供に誰がどの子供も林檎が好きだとは限らない。\nお金持ちの人はすべて幸せだがみんな幸せだとは限らない。\n有名でもな人はみんなに知っているとは限らない。\n3.　にもほどがある おいおい、どんな態度ですか？礼儀正しくないにもほどがある\nなんだその態度は　失礼にも\n仕事中にほっといたら、責任がないにもほどがある\n途中で仕事を投げ出すなんて　無責任にも\nこんなミスをしたをするなんて、（ぞんざい）いかげんにもほどがある\nまた噓だ。噓をつくにもほどがある\n図々しいにもほどがある\nわがままにもほどがある\n身勝手にも\n常識欠にもほどがある\n非常識\n冗談にもほどがある。\n4.　べきだった。べきじゃなかった 傘を持つ持ってくるべきだった\n夜にコーヒーを飲むべきじゃなかった、全然眠れない。\n早く起きるべきだった。\n彼にを誘うべきじゃなかった。\n早く意識べきだった。\nもっと早く気付くべきだった。\nもっと早く言うべきだった。\n5.　たら～てください ほかの予定に競合したら、話してください\nもしこれが何か別の予定とぶつかっていたら、教えてね\n駅についたら電話をしてください、向かいて（迎えに行くよ）\n家に帰ったら（メールしてね）メッセージを届けてください。\n私（に）できることがあたっら、話してください（言ってね）\n会議を終わったら、電話（して）をかけてください。\n知っていたら、教えてください。\n決めたら話してください\n決まったら\n何か私（に）できることがあたっら、話してください（遠慮なく言ってください）\n決める　他动词\n決まる　自动词\n决定了马上就干。\n6.　気はさらさらない 行くの気はさらさらない\n辞めるの気はさらさらない\n教授になる気はさらさらない\n人生がすべて仕事にかかるの気にさらさらない\n人生を仕事にかける気はさらさらない\n庇う気はさらさらない\n石川さんに使われる気にさらさらない\n石川さんのために骨を折る気はさらさらない　無駄骨を折る\n喧嘩を売る気はさらさらない\nまだ結婚する気はさらさらない\n7.　さほど、ない 勝つことはさほどそんなに難しくありません。\n日本語はさほど難しくない。\n今日はさほど熱くない。\nそんなにさほど面白くない。\n最近そんなにさほど忙しくない。\nそんなにさほど特別なものじゃない。\n8.　としか言いようがない 運が悪い（ない）としか言いようがない\n自業自得としか言いようがない\n（これはもうミラクル）奇跡だとしか言いようがない\n天才だとしか言いようがない\nこんな事をしった、バーガーとしか言いようがない\n不思議としか言いようがない\n頭が悪い（おかしい）としか言いようがない。\n頭がおかしい\n見事としか言いようがない\n9.　とは言い切れない 進捗が順調だ（うまくいく）とは言い切れない。\nこんな事がなかったとは言い切れない\n完全に治った（また全快）とは言い切れない\n全然関係ないとは言い切れない\n完全に無関係とは言い切れない\nそうなんだ（はっきりそうだ）とは言い切れない\n絶対に間違えない（間違いない）とは言い切れない\nほかの可能性が全然（全く）ないとは言い切れない。\n彼がやったとは言い切れない\n間違う：自他同体\n間違って隣へ行った。\n間違ったことをする\n間違いない　＝必ず\n間違える：弄错\n意味を間違えった。\nコンピューターなら計算を間違えないだろう\n10.　かどうか分からない 行ってくるかどうか分からない\n今夜のパーティーに行けるかどうか分からない。\nこれは正しいかどうか分からない\nできるかどうか分からない\n順調に進む（うまくいく）かどうか分からない、でもしてみるやりがいがある（やってみる価値がある）\n彼女に喜ばれる（気に入ってもらえる）かどうか分からない\n今日は定時で（帰れる）退勤して家に帰るかどうか分からない\n食べたことがなかった（）ないので、美味しいかどうか分からない\n11.　数だけ、分だけ 涙の数だけ、人は強くなれる\n人の数だけ、人生がある\n国の数だけ、習慣がある\n真実は一つわけではない、人の数だけある。\n12.　べきだ・べきじゃない 人を見るにより（人を見た目で）判断するべきじゃない。\n分からないなら、分からないとはっきり言うべきだ。\n目上の人に話す時敬語で使うべきだ。\nお金を借りたら、きちんと返すべきだ。\n人の悪口を言うべきじゃない。\n日本に住んでいるなら、日本のルールを守るべきだ。\n急いで焦って決めるべきじゃない。\n今すぐ向かうべきだ。\n13．気がする・ような気がする 最近彼女はちょっとイライラ気がする。\n次は勝てる気がする。\nどこかで彼女に会ったような気がする。\n誰かに言われたような気がする。\n生まれ変わったような気がする。\n誰かに見られるような見られている気がする。\nむちゃくちゃだが、行ける気がする。\n空気感がいい気がする。\n14．私はＢに（から）～するように頼まれた 中国の娘からこれを渡すように頼まれた。\n友達にタピオカを買ってくるように頼まれた。\n家族にお土産を買ってくるように頼まれた。\n妹に宿題を手伝うように頼まれた。\n昨日、映画を見に行くつもりだったが、母さんに買い物に行くように頼まれた。\n先生にこれを貼るように頼まれた。\n15．するつもりです・つもりだったのに・つもりだったが（けど） 明日自転車で会社に行くつもりです。\n何かつもりですか？\nもともとずっと隠す隠していくつもりでしただったが。\nどんな手を使っても、あなたに撮ってもらうつもりだ。\n李さんと（もう親友のつもりだったが、）友達になったつもりだが、彼女はそう思って考えていないようだ。\n電気を消したつもりだったが、ついていた。\n16．～（する）気ですか 殺す気ですか\n逃げる気ですか\n売る気ですか\n何をする気ですか\n横取りする気ですか\n（）找死\n死ぬ気がするか\n寝る気ですか？\n私を食べる気ですか？\n17．場合か・場合じゃない 前接　ている\nもう迷っていけない。今は迷ってる場合じゃない。\n喜んでる場合じゃない。\n落ち着けなさい！争ってる場合じゃない。\n今は笑ってる場合じゃない。\n照れてる場合じゃない。\nふざけてる場合じゃない。\nもう（一意孤行）意地張ってる\nさすが私の友達。今は（佩服）。感心してる\n内輪もめる\nビビってる\nもやもやしてる\nかっこうつける\n18.~立場じゃない 言える立場じゃない\n指図する立場じゃない。\nこんな言葉を言える立場じゃない。\nそんなのんきな立場じゃない。\n19．ことはない 怯えることはない。\n遠慮することはない。\n心配することはない。\n小さいな地震なので、慌てることはない。\n一つ電話により解決できること、わざわざ行くことはない。\n時間があるので、急ぐことはない。\n悪いのは向こうなんだから、謝ることはない。\nそんなに怒ることはないでしょう。\n20．ずっと～したかった ずっとあなたに謝りたかった。\nずっとこうしたかった。\nずっと言いたかった。\nずっと彼女に会いたかった。\nずっと一人占めしたかった。\nあれからずっと確認したかった。\n以前この本を見たかった。\nずっとお会いしてお礼を言いたかったの\n21. ～ばよかった；～なければよかった 君を連れてこなければよかった。\nあんな子供、生まれなければよかった。\n先輩のこと、知らなければよかった。\n買う前にちゃんと考えればよかった。\n（作罢）やめればよかった。\nいい景色、カメラをつければよかった。（持ってくればよかった。）\n先帰ればよかった。\nそんな時もっと健康を気を付ければよかった。\n22. 下手をすれば；下手すりゃ；下手したら；下手すると 下手すれば死ぬ。死んじゃうよ\nそのような言い方、下手すれば喧嘩になっていく。\n下手すれば怒られるよ。\n下手すればここから戻れなくなる。\n下手すれば巻き込まれる。\n下手したらこの男と結婚していたかもしれません。\n結婚していく表变化\n下手したらこのチャンスを失うかも.\n下手すれば大怪我になるかも。\nしていく、てくる\n買ってきた\n行ってきます\n傘を持っていこう\n乗せてきました\n連れていって\nロングドレスを着ていきます。\nサングラスをかけてきました\n見に行く　見ていく\n23．てもおかしくない どんな時雨が降ってもおかしくない（天気）。\nいつ雨が降り出して\nどんな（いつ）時感染られてもおかしくない。\n絶交してもらってもおかしくない。（絶交されても）\n怒られてもおかしくない。\n記憶をに残してもおかしくない。\n何か起こして起こってももおかしくない。\nあのカップルはずっと（いつも）喧嘩ばかりでしているので、どんな時別れてもおかしくない状態だ。\n紧绷的弦緊張の糸はいつ切れてもどんな時切れちゃってもおかしくない。\n24．～ば～はずだ 頑張ればついていけるはずだ。\n行けばわかっていくはずだ。それがわかる\n基本的な作法通りにやればできる行けるはずだ。\n考えれば何か思い出すはずだ。浮かんでくる\n今から頑張れば、明日前完成できるはずだ。明日までに終わる\nちゃんと話せば、父に理解られるはずだ。わかってくるはずだ。\n寝れば回復するはずだ。一晩寝れば良くなるはずだ\nここで置いておけば、出掛ける時に忘れないはずだ。\n25．どうしても～たい どうしても別れたいの？\n私は子供が欲しい。どうしても生まれたいの。\n高校時代どうしても叶えたい夢。叶えておきたい\n（有些话）どうしても言いたいことがある。\nどうしても皆さんにこの感謝の気持ちを伝えたい。\nどうしても先生に見せたい。見てもらいたい\nどうしても体重を減らしたい。\n26．～ば大丈夫 パンダに頼まれば大丈夫。任せとけば\nここに到着すれば大丈夫。ここまで来れば\n君がいれば大丈夫。\nここまで逃げれば大丈夫はずだ。\nほっておけば大丈夫。\n急がない、（一点一点记就好）焦らないで、少しずつ覚えていけば大丈夫。\nここだけを気を付けておけば丈夫。\nここがあれば大丈夫。\n27．覚えはない こんなことがあった覚えはない。\nそんな覚えはないな。\n君に頼まれた覚えはない。別に頼んだ覚えはない\nルールを違反した（破った）覚えはない。\nそんな酷い言葉を言った覚えはない。\nあの衣服を付けた覚えはない。そんな格好した覚えはない。\n育てられた（育ててもらった）覚えはない。\nこんな約束（密約を交わした）覚えはない。\n承認（許可）した覚えはない。\n（そんな風に）言われる覚えはない。\n説教られる覚えはないことをした覚えはない。お前に説教される覚えはない。\n抱怨（）文句を付けられる覚えはない。\n（君にそんなひどい）こんなひどい言葉を言われる覚えはない。\n28．（ちょうど）～ようと思っていたところだ ちょうど一息入れようと思っていたところだ\n父上、呼びに行こうと思っていたところだ\nこれから（打招呼）ご挨拶に回ろう\n新しい水着を買おうと思っていたところだ。\n李さんとに相談しようと思っていたところだ。\nちょうど出かけようと思っていたところだ。\n（来てくれてよかった）くるのがよかった、呼ぼうと思っていたところだ。（正想叫你）\n電話しようと思っていたところだ。\n29．～ぶりをする た形动词＋ふり 小句或连用形＋ぶり\n知らないふりをする。\n怒ったふりをする\n病気なのふりをする。\n何も起こっていないぶりをする。平気なふりをしている\n分からないのに、分かったふりをする。\n知らないふりをするな。\nじゃあ、死ぬふりをしたらどうですか。\n落ち込んでいるふりをするな。落ち込んだふり\n30．予想以上に；想像以上に 一般后接过去式\nなんか…予想以上にヤバい。\n予想以上に面白い。\n想像以上に時間がかかる（かかった）。\n知名度は予想以上に高い。\n相手は想像以上に難しい。想像以上に厄介な相手だ\n对手比预想更难搞——\u0026gt;比预想更难搞的对手\n想像以上に傷つける。傷ついたみたい。\n予想以上に成長した。\n","permalink":"https://fishdel.github.io/zh/posts/%E5%B8%B8%E7%94%A8%E6%97%A5%E8%AF%AD%E8%A1%A8%E8%BE%BE/","summary":"\u003ch2 id=\"表現原則\"\u003e表現原則\u003c/h2\u003e\n\u003ch3 id=\"１非典型名词谓语句\"\u003e\u003cstrong\u003e１．非典型名词谓语句\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e問題は山積みだ。\u003c/p\u003e\n\u003cp\u003eこのビルはいつ完成（\u003cem\u003eですか\u003c/em\u003e）\u003cdel\u003eしますか\u003c/del\u003e？\u003c/p\u003e\n\u003cp\u003e来年予定です。\u003c/p\u003e\n\u003cp\u003e京都で（の）（撮影は）\u003cdel\u003e写真を撮るのが\u003c/del\u003e久々だ（った）。\u003c/p\u003e\n\u003cp\u003eあなたの意見に賛成だ\u003c/p\u003e\n\u003ch3 id=\"２モダリティ表現\"\u003e\u003cstrong\u003e２．モダリティ表現\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003e转述他人信息\u003c/p\u003e","title":"常用日语表达"},{"content":"24岁结尾一点感想 总结自己总是离不开几件事，生活，友谊，工作，感情，健康和对人类的贡献\u0026hellip;\u0026hellip;每个人心中的排序都不同，同龄人里，有的忙于工作， 有的还在读书，甚至有的已经在家庭和婚姻中历练自我。\n而我的2024年过的太冗长，杂乱且难以整理。\n感情方面上并没有诞生出什么恋情。 想到四叔说的，一直一个人就犹如建立了一个个人的王国， 这个王国的律法系统且保守，国王专制，接受他人往往带着审视和批判，是一个精神上的独裁者，最后走向一个人的朝圣。 而随着年龄增大，周围的朋友开始有自己的生活，开始谈恋爱，见家长，考公考事业单位，努力赚钱，自己就会有一种走入孤独的感觉。 因为一旦加入到这样一个集体，必然要交付一部分个性，而这个交付的过程是一个允许自己主体性被侵犯的过程。 而大部分人选择的谈恋爱来对抗孤独的方式，也正是如此。 虽然来到了一个新的城市，但是好在还有几个交心的老朋友，他们也不属于以上的集体，虽然分别甚远，但也能消解寂寞。\n今年的工作也许是最颠簸的一年吧。新的工作说不上喜欢，但也说不上讨厌。 离开了之前的象牙塔，没有了之前的部长，同事，才发现之前一直被保护的太好，一个人的探险之路并不简单。 没有太过幸福的事情发生，也没有遇到什么毁灭性打击，我只能去捏造一些有趣的事情让自己略有活力地活着，来逃避荒谬的生活。\n下半年也不怎么玩游戏了，今年也就玩了山河旅探，bangbang，重温了两部刺客信条。买了一年的王国之泪到现在也没通关。 被生活的疲惫感渐渐侵蚀，为了抵抗这种虚无，为了心里的那一点愿望，买了一部四块二，开始了吉他之旅，买了一辆手波小车，开始驾驶之路。 在干这些事的时候，物理意义上扎实的接触，操作与协调感，能对抗虚无，抽离和格格不入。\nAnyway，今年还是做了很多不一样的事情，一个人去日本出差，旅行，给好朋友介绍工作，一个人跑到广州上班，开始了新的生活 也做了很多没想到的事，比如撞车了，比如买车了。 也看了不少精彩的演出，去年年初不晚封箱演出时世界一分为二的感动，美好药店的新奇，平野绫的回忆满满，藤井风的精彩，还和新认识的同事看了康姆士。\n我的心境从大三开始确确实实改变，到现在扎扎实实地坚定下来，现在的我不允许任何无意义的事情对我制造焦虑干扰我的生活，也不会在这样不知要流向何处的工作里白白消耗自己的才能和激情。 新的一岁在狭窄的出租屋里码字，但是无妨，麻雀虽小但五脏俱全，又何陋之有呢。\n","permalink":"https://fishdel.github.io/zh/posts/%E5%B2%81%E6%9A%AE%E7%BA%B7%E5%A4%9A%E6%80%9D-%E5%A4%A9%E6%B6%AF%E6%B8%BA%E6%9C%AA%E5%BD%92/","summary":"\u003ch3 id=\"24岁结尾一点感想\"\u003e24岁结尾一点感想\u003c/h3\u003e\n\u003cp\u003e总结自己总是离不开几件事，生活，友谊，工作，感情，健康和对人类的贡献\u0026hellip;\u0026hellip;每个人心中的排序都不同，同龄人里，有的忙于工作，\n有的还在读书，甚至有的已经在家庭和婚姻中历练自我。\u003c/p\u003e","title":"岁暮纷多思 天涯渺未归"},{"content":" 生成可执行文件的主要流程：预编译、编译（C—\u0026gt;汇编—\u0026gt;binary）、链接。\n编译只是把各个.c和.s文件编译成对应的.o文件。然后需要链接器将各个.o文件链接为一个可执行文件。\n.lsl,.ld文件：属于一种linkfile，规定如何把输入文件内的section放到输出文件，以及控制输出文件内各部分在程序地址空间内的布局。\n.text：是程序代码段，用于存放函数代码\n.data: 全局变量并且初始值不为0\n.bss：未初始化的全局变量或者初始化为0的全局变量。\n标准C语言的 section 前缀主要包括： .bss/.data/.rodata/.text。 英飞凌芯片对应的将数据划分为 near类型/far类型。对应的前缀为：\n1）带 “z”的near类型数据：.zbss/.zdata/.zrodata 2）标准的 far 类型数据： .bss/.data/.rodata\n16位系统中会有近指针，远指针。\nnear定义的标号表示段内近跳转，近调用的地址。near指针的长度是16位的，所以可指向的地址范围是64K字节，通常说near指针的寻址范围是64K。\nfar定义的标号表示段间远跳转，远调用的地址。far指针的长度是32位，含有一个16位的基地址和16位的偏移量，将基地址乘以16后再与偏移量相加，实际上far指针是20位，取值范围为0x00000～0xFFFFF。所以far指针的寻址范围是1M字节，超过了一个段64K的容量。\n编译器将.c文件编译成了对应的.o文件后，每个.o文件中会包含了数量不同的段，.text段、.data段、.bss段、.vectors段、.ram_code段。链接器将各个.o文件中的代码按照不同的段，链接成一个文件。所有的.text链接到一起，所有的.data链接到一起，所有的.bss段链接到一起，所有的.vectors段链接到一起，所有的.ram_code段链接到一起。\nENTRY(_START) ，ENTRY语法说明，用户程序最先从START处开始运行，定义应用程序的入口点，相当于告诉连接器的启动地址，即输出文件中的第一条可执行指令，_START一般在.s文件定义。\nFlash地址分配可以用链接脚本来控制，代码共分为三个部分，Tricore的APP，HSM的BootLoader和HSM的APP，三个工程的链接脚本要相互配合好，这样编译出来的代码不会互相覆盖。\n例子：\n代码内声明为Static类型的变量，其运行空间在RAM区域加载空间在ROM区域，编译后属性为.data/.bss,其运行空间可通过以下方式设定去运行地址与加载地址。\n设定运行地址\ngroup P_REW_DIS_TSK2MS (ordered, align = 4, run_addr=P_REW_DIS_TSK2MS_ORG) { section \u0026#34;P_REW_DIS_TSK2MS\u0026#34; (size=P_REW_DIS_TSK2MS_LEN, attributes=r, fill=0x00) { select \u0026#34;.text.*.P_REW_DIS_TSK2MS.text\u0026#34;; } } 设定加载地址\ngroup P_REW_EXE_EXD (ordered, contiguous, align=4, fill=0, load_addr=P_REW_EXE_EXD_ORG) { group (ordered, contiguous, align=4) { section \u0026#34;P_REW_EXE_EXD\u0026#34; (size=P_REW_EXE_EXD_LEN, attributes=r, fill=0x00) { select \u0026#34;.text.*.P_REW_EXE_EXD.text\u0026#34;; } } } ","permalink":"https://fishdel.github.io/zh/posts/%E5%B5%8C%E5%85%A5%E5%BC%8F.ld.lsl%E7%AD%89%E9%93%BE%E6%8E%A5%E6%96%87%E4%BB%B6%E4%BD%9C%E7%94%A8/","summary":"\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e生成可执行文件的主要流程：预编译、编译（C—\u0026gt;汇编—\u0026gt;binary）、链接。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e编译只是把各个.c和.s文件编译成对应的.o文件。然后需要链接器将各个.o文件链接为一个可执行文件。\u003c/p\u003e","title":"嵌入式.ld,.lsl等链接文件作用"},{"content":"函数指针与回调函数 函数指针 作用：硬件驱动程序和用户应用程序相互分开，硬件驱动程序提供API函数，用户应用程序将函数作为回调函数的方式进行使用。 回调机制的好处是，在程序执行期间可以动态更改被调用 回调函数：作为参数传递给另一个函数的函数，接受回调作为参数的函数预计会在某个时间点执行它。\n使用方法：函数名带括号就是函数指针，没括号是指针函数\n把函数A的地址赋给一个函数指针p，以p为参数，赋值给函数B，函数B通过p调用A\n函数指针是一个指向函数的指针变量，32位单片机中存放大小为4字节的地址\nint *(*pfunc)(int,int*,void*)*; typedef int *(*pfunc)(int,int*,void*)*; 一般用typedef定义函数指针类型。 typedef和define区别：typedef 语句是在编译过程中被解析的，而#define是在编译之前的预处理过程中被解析的\ntypedef uint8_t (*func_ptr) (void);，就相当于把uint8_t (*) (void); 定义成了另一个别名 func_ptr了。这个func_ptr就表示了函数指针类型。\ntypedef int *(*pfunc)(int,int*,void*)*;相当于把int（*）(int,int*,void*)；定义成别名pfunc\n相当于把 int(*)(int，int*，void*)取一个别名pfunc\n回调函数 作用: 虽然只是函数指针的应用，但通过函数指针的当时区分传入不同的函数入口地址去执行不同的函数，可以节省单片机的ram和rom的开支。\n实现： 例如将callback函数看做底层函数，main函数看成上层应用函数，现在有上层方法A,上层方法B,在使用底层callback函数进行功能设计时，转回去拿到具体的方法A,方法B，再把结果返回main,此时方法A,B就是回调函数。\n举例： EcuM_AL_Reset是回调函数，属于对象外的，该函数被调用后，会MCAL标准函数Mcu_PerformReset来重启CPU\n钩子函数 hook函数实际也是函数指针，因为也是用户定义的，也可以理解为回调函数，二者之间区别主要是回调函数主要是目的处理，hook函数主要是过程监控。\n定义函数fun1，fun2，再定义一个pfun函数指针，在main函数里通过pfun指向fun1，fun2，这个过程称为挂钩子。在不确定main函数的功能的情况下，留下函数指针作为接口，挂上不同的函数就可以完成不同的功能。\nHOOK函数相当于一个监视器，捕获消息队列中需要的内容去处理\n挂钩子的过程也称为注册。在注册函数中，使用者把自己编写的钩子函数挂在已经声明的函数指针上，这个注册函数的参数是要挂上的钩子函数的地址，即函数指针。\n举例：Autosar中的hook函数机制\n1）由操作系统调用，在特定的context中取决于操作系统的实现 2）高于所有task 3）不被第二类中断程序打断。 4）属于操作系统的一部分 5）可以由用户定义功能\n截获行为的函数调用（相当于监控器）。所有特定于应用程序的Hook函数（Startup, Shutdown and Error）必须返回（不接受阻塞或无限循环）。\n当应用程序或操作系统在出现严重错误时请求系统关闭时调用 (ShutdownHook)\n在Shutdown OS时，操作系统将调用钩子函数ShutdownHook，勾到EcuM_ShutDown那边，然后关闭（如下图： 多核系统关闭过程）。用户通常可以在ShutdownHook中自由定义任何系统行为\nFUNC(void, OS_SHUTDOWNHOOK_CODE) ShutdownHook(StatusType Fatalerror) { if( GetCoreID() == OS_CORE_ID_MASTER ) { EcuM_Shutdown(); } } 位操作 1. 移位操作 ①\u0026laquo; 左移 ：左移几位就把左边的数去掉几位，右边补0；相当于2^5-\u0026gt;2^6\n源操作数 * 2的N次方（N取决于移动的位数） = 移动后的结果。\n②\u0026raquo;右移：右移几位就把右边的数去掉几位，左边补0\n源操作数 / 2的N次方（N取决于移动的位数） = 移动后的结果(*只取整数部分*)\n算术左移和逻辑左移相同\n​ 算术右移符号位要一起移动，左边补符号位，11100算术右移一位为11110\n2.逻辑运算 \u0026amp;(与) 和0一起使用 可以清零 |(或) 可以置1 ^(异或) 3.举例 将特定数置1 0xf8 把第2位到第6位置1\n1111 _1000\n2到6 1_1111 =0x1f\n从第二位开始置1 0x1f\u0026laquo;2 = 111_1100\n第a位到第b位置1，（b-a+1）转为16进制\n从哪一位开始置1 则左移多少位，\u0026laquo;a\n把第4位到第8位和第23到25位同时置1\n((0x1f \u0026laquo;4) | (0x7\u0026laquo;23))\n将特定数置0 先置1 再按位取反\n~((0x1f \u0026laquo;3)\n操作 表达式 设置整型数a的bit4 a|=(1\u0026laquo;4) 设置整型数a的bit4~bit7 a|=（(0x1f）\u0026laquo;4） 清除整型数a的bit15 a\u0026amp;=((~a\u0026laquo;15)) 代码举例 清除特定位的值 VOID\tClear_data( const UCHAR indat ) { UCHAR\tbyte_id;\tUCHAR\tbit_dat;\tUCHAR\tbit_id;\tUCHAR\tbit_dat_inv;\t/* indat/8 取得其数组id*/ byte_id\t= (UCHAR)If_Shift_LR( (ULONG)indat, ZSHIFT3 ); /* indat%8 取得其bit位*/ bit_id\t= (UCHAR)( indat \u0026amp; ZMASK_07 );\tbit_dat\t= (UCHAR)If_Shift_LL( ZMASK_BIT0, (ULONG)bit_id ); bit_dat_inv = ~bit_dat; /*\u0026amp;= 清除该位*/ array[ byte_id ]\t\u0026amp;= bit_dat_inv;\t} memcpy函数 3.以2Byte为单位进行复制处理，从起始地址开始每2个Byte拷贝到目标地址，n是拷贝次数，如果16个字节，以2Byte为单位复写，n=8，以4Byte为单位复写，n=4 VOID\tIf_Memcpy2( VOID* const dst, const VOID* const src, SIZE_T n ) { USHORT*\tdst_p = (USHORT*)dst; const\tUSHORT*\tsrc_p = (const USHORT*)src; /*n = n/2 如果是4个byte为单位进行复写，IfCfc_Shift_LR( (ULONG)n, 2 );*/ n = (SIZE_T)If_Shift_LR( (ULONG)n, 1 ); if ( n != 0 ) { do { *(dst_p) = *(src_p); dst_p++; src_p++; n--; } while ( n \u0026gt; 0 ); } } 数据类型与关键字 enum枚举 注意点：\n1.在同一个作用域能不能出现重名的枚举常量名；\n虽然定义了两个枚举类型 enum1，enum2，但如果其成员常量名相同则会报错\n2.不可以定义相同的变量但是可以定义相同的值；\n`typedef enum { enumA=0, enumB=4, enumC=5,\tenmuD=6, enmuE=6, }A_enum;` union联合体 注意点\n内存空间相同但储存不同的数据类型，并不是同时储存，而是一次只能存储一种数据类型。 联合体的大小都相等，每个联合可以储存各种数据类型。共用体的长度为其最大的成员的长度。 用途： 数据的格式在不同场合下不同时，节省内存。 const关键字 可以保护被修饰的东西，防止意外修改，增强程序的健壮性\n编译器通常不为普通const常量分配存储空间，而是将它们保存在符号表中，这使得它成为一个编译期间的常量，没有了存储与读内存的操作，使得它的效率也很高。\ninline关键字 关键字inline 必须与函数定义体放在一起\n只适合函数体内代码简单的函数数使用\nstatic inline inline函数不能在两个不同的文件中出现，一个.h不能被两个不同的文件包含，一个inline在不同的.C里面生成了不同的实例\nc文件中的仅inline函数是不内联的，因为没有static，编译会认为它是全局的，因此像普通函数一样编译了。 加入static，这样内部调用函数时，会内联，而外部调用该函数时，则不会内联。\nstatic关键字 修饰变量：\n修饰全局变量：仅对当前文件可见，其他文件不可访问，其他文件可以定义与其同名的变量\n修饰局部变量：普通局部变量存储在栈空间，编译器不会初始化；修饰之后的局部变量，即使在声明时未赋初值，编译器也会把它初始化为0。且静态局部变量存储于进程的全局数据区，即使函数返回，它的值也会保持不变\n修饰函数：\n静态函数只能在声明它的文件中可见，其他文件不能引用该函数\nvolatile关键字 每次读取数据，必须在内存上取，而不是使用保存在寄存器或者cache里的备份（直接取内存原始地址）\n没有声明的话，对一个变量进行多次赋值，没有生成之间汇编代码，直接取最后的值寻址赋值给该变量；声明之后每个变量的赋值都形成了汇编代码，没有被优化。\n易变的 多线程的程序，共同访问内存时多个程序可以操纵这个变量；和外部设备的状态对应，通过驱动程序和中断改变该变量的值，程序并不知道\n用途：\n并行设备的硬件寄存器（状态寄存器），每次对它的读写都可能有不同意义\n一个中断服务子程序中会访问到的非自动变量；当变量在触发某中断程序中修改，对于编译器，主函数里没有修改这个变量，可能只执行一次从内存到某寄存器的读操作，而后每次只会从该寄存器中读取变量副本，使得中断程序的操作做了跟没做一样。\n多线程任务中的共享变量。\n一个参数可以既是const又是volatile——只读的状态寄存器。它是volatile因为它可能被意想不到地改变。它是const因为程序不应该试图去修改它。\n一个volatile的指针——当一个中断服务子程序修改一个指向一个主函数的指针时\n宏定义 用途\n防止头文件被重复包含\n重新定义一些类型\n得到指定地址\n得到指定地址上的一个字节\n#define MEM_B( x ) ( *( (byte *) (x) ) ) #define MEM_W( x ) ( *( (word *) (x) ) ) Translation unit 在函数块外部名字声明（函数和变量）若只能在一个已知的translation unit是可见的，称为内部链接。他们对于linker（连接器）是不可见。若声明的函数或变量对于其他的目标文件是看见的，则称为具有外部连接，对于linker是可见的。\n编译的基本单元是.c文件\n","permalink":"https://fishdel.github.io/zh/posts/%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88%E4%BD%8D%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","summary":"\u003ch1 id=\"函数指针与回调函数\"\u003e函数指针与回调函数\u003c/h1\u003e\n\u003ch2 id=\"函数指针\"\u003e函数指针\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e作用\u003c/strong\u003e：硬件驱动程序和用户应用程序相互分开，硬件驱动程序提供API函数，用户应用程序将函数作为回调函数的方式进行使用。\n\u003cem\u003e回调机制的好处是，\u003cem\u003e\u003cstrong\u003e在程序执行期间可以动态更改被调用\u003c/strong\u003e\u003c/em\u003e\u003c/em\u003e\n\u003cem\u003e回调函数：作为参数传递给另一个函数的函数，接受回调作为参数的函数预计会在某个时间点执行它。\u003c/em\u003e\u003c/p\u003e","title":"函数指针，位操作和数据类型"},{"content":"CAN Interface (MCMCAN) 1.模块 有三个模块CAN0,CAN1,CAN2，一般只使用CAN0,CAN0的模块比较全面。\nCAN0有4个CAN node，Message RAM 一共32Kbyte，FIFO,buffer，register在此处开辟。\n每个node通过M_CAN来实现，并且都支持CAN FD；都有两个引角，TXD和RXD\n2.初始化 具体步骤\n设置CCCRi.INIT开始初始化（软硬件复位或通过bus off）此时配置寄存器不会被改变\n当该位被置起，即CCCRi.INIT (i=0-3) 置1，can总线之间的通信停止，CAN node停止收发，CAN总线输出TXD为隐性（高）。错误管理逻辑EML的计数器保持不变。\n重置 CCCRi (i=0-3).INIT 完成软件初始化\n只有当位CCCRi.INIT和CCCRy.CCE都设置为1时，才能启用对M_CAN配置寄存器的访问\nCCCRi.CCE只有在CCCRi.INIT=\u0026lsquo;1\u0026rsquo;时才能被set/reset。\n当CCCRi.INIT被清除时，CCCRi.CCE自动复位。\n3. 时钟控制单元 MCMCAN模块时钟输入连接到时钟控制单元（CCU）。并且MCMCAN有两种时钟源，fsyn用于寄存器和RAM接口，fasyn用在CAN FD上。CLC设置为全局模块寄存器提供其时钟。\n为了向M_CAN节点提供相应的时钟，必须设置MCR.CLKSELi寄存器。异步时钟以及每个M_CAN节点的同步时钟可以通过MCR.CLKSELi寄存器位字段打开/关闭。\n4. 中断 每个module都有16个中断线INT_O0~INT_O15\n中断分组已经设置，存在16个中断节点。中断组可以通过GRINT1i（i=0-3），**GRINT2i (i=0-3)**自由分配给节点\n中断脉冲的生成是基于 寄存器 (IRi (i=0-3) TTIR0) 和 (**IEi (i=0-3)**and TTIE0)，中断标志和中断使能之间的关系是\u0026amp;。中断flag会通过向 CANn_IRi bit.写‘1’进行reset。\n如果相应中断使能寄存器中的相关中断使能位使能（IEi（i＝0-3）、NTRTRi（i＝0-2）、TEIE和TTIE0），MCMCAN 模块里16条中断输出线INT_On的其中一条使用GRINT1i (i=0-3) 和 GRINT2i (i=0-3)可以产生中断脉冲。\n也就是说通过配置interrupt line选择对应的SRC节点（line和SRC_INT一一对应）配置CAN中断\nIfx_SRC_SRCR INT[16]; } Ifx_SRC_CAN_CAN; volatile Ifx_SRC_SRCR *IfxCan_getSrcPointer(Ifx_CAN *can, IfxCan_InterruptLine interruptLine) { IfxCan_Index canIndex = IfxCan_getIndex(can); Ifx_SRC_CAN_CAN *const srcCanBaseAddress[IFXCAN_NUM_MODULES] = { \u0026amp;MODULE_SRC.CAN.CAN[0], \u0026amp;MODULE_SRC.CAN.CAN[1], }; return \u0026amp;(srcCanBaseAddress[canIndex]-\u0026gt;INT[interruptLine]); } 5. CAN FD 区别：传输速率不同、数据长度不同、帧格式不同、ID长度不同\nCCCRi.FDOE (i=0-3) 是收发CAN FD frame功能启用位\n时间延迟补偿：为了实现比发射机延迟更短的数据相位比特时间，引入了延迟补偿。在没有变送器延迟补偿的情况下，CAN FD帧的数据阶段的比特率受到变送器延迟的限制\nCAN收发器的信号从TX出发到总线到RX有时延。CAN FD速率可变，BRS位进行控制，波特率更高\n6. CAN node的收发 接受到的Rx Frame以Rx buffer的形式存放在Message RAM，最多存放64个\n首地址通过寄存器进行配置\n\\#define SMCMCAN0_RXBC0_INIT ( 0x00000000UL )\ntypedef volatile struct _Ifx_CAN_N_RX { Ifx_CAN_N_RX_F0C F0C; Ifx_CAN_N_RX_F0S F0S; Ifx_CAN_N_RX_F0A F0A; Ifx_CAN_N_RX_BC BC; Ifx_CAN_N_RX_F1C F1C; Ifx_CAN_N_RX_F1S F1S; Ifx_CAN_N_RX_F1A F1A; Ifx_CAN_N_RX_ESC ESC; } Ifx_CAN_N_RX; 单个Rx Buffer进行管理时，每个Rx Buffer称为Dedicated Rx Buffer，当写入数据后就会被锁住，不会再从CAN总线上写入新数据，直到CPU访问完后解锁\n多个连续的Rx Buffer 可以组成Rx FIFO进行管理，一个CAN Node可以设置2个Rx FIFO,FIFO0和FIFO1.同样首地址通过寄存器进行配置。FIFO是一种先进先出的数据缓存区域。\n在节点的初始化过程中\n①对于Tx,\n（1）首先设置buffer大小\n（2）判断config-\u0026gt;txConfig.txMode的类型是FIFO还是QUEUE设置为对应的模式并且设置其buffers的大小\n（3）在所选中的buffer使能中断\n​\t②对于Rx，主要是设置其Rx buffer 的数据长度和Message RAM的起始地址，然后设置Rx FIFO的数据长度，Message RAM的起始地址，大小，操作模式和watermark level\n对于FIFO,可容纳的元素数量称为Deepth，每个元素大小为Size，进入full状态后写入会溢出，所以要设置水线（Watermark)，Watermark\u0026lt;Deepth,当已被读取的Element达到Watermark时降低读取速度或者提高写入速度 读写指针：总指向下一个要读取/当前要被读出的单元。\n读空：当读写指针相同时，表示FIFO为空， 复位操作时 或者当读指针读出FIFO中最后一个字 后，追赶上写指针时，此时读空信号有效\n写满：当读写指针再次相等时，表明FIFO为满，这种情况发生在，当写指针转了一圈折回来（wrapped around）又追上了读指针\n7. Message RAM CAN0 起始地址：0xF0200000\nCAN1起始地址：0xF0210000\nCAN Node会将准备发送的Tx Frame以Tx Buffer的形式存放在Message RAM中，最多可以存放32个Tx Buffer数据\n8. 工作模式 Restricted Operation Mode 固定工作模式\n模式进入：处理器无法从消息RAM读取数据\n模式退出：host CPU重置CCCRi.ASM位\n在固定工作模式下，节点能够接收数据和远程帧，并对有效帧进行确认，但不发送数据帧、远程帧、活动错误帧或过载帧。出现错误或者过载情况等待总线空闲，然后同步CAN通信，\nBus Monitoring Mode 总线监控模式\n模式进入：CCCRi.MON置1，或者error level S3，即(TTOST0.EL = “11”)\n在总线监控模式下，点能够接收数据和远程帧，但是不启动传输，可以分析CAN总线上信号量\nPower Down (Sleep Mode) 睡眠模式\n模式进入：输入时钟信号或者控制寄存器\n具体过程：时钟停止请求信号active，CCCRi.CSR读取为1；\n1.当所有pending的传输请求都已经完成，等待总线空闲状态。\n2.然后M_CAN将CCCRi.INIT设置为1，以防止任何进一步的CAN传输；\n3.通过将CCCRi（i=0-3）.CSA设置为1来确认其已准备好Power Down。在这种状态下，在时钟被关闭之前，可以进行进一步的寄存器访问。对CCCRi.INIT写访问将无效。\n4.时钟将被关闭\nTest Modes 测试模式\n测试模式应仅用于生产测试或自检。针脚TXD的软件控制会干扰所有CAN协议功能。不建议使用测试模式进行应用\n3）.CSA设置为1来确认其已准备好Power Down。在这种状态下，在时钟被关闭之前，可以进行进一步的寄存器访问。对CCCRi.INIT写访问将无效。\n4.时钟将被关闭\nTest Modes 测试模式\n测试模式应仅用于生产测试或自检。针脚TXD的软件控制会干扰所有CAN协议功能。不建议使用测试模式进行应用\n","permalink":"https://fishdel.github.io/zh/posts/%E8%8B%B1%E9%A3%9E%E5%87%8C-tc3xx-can-interface-mcmcan-%E6%A8%A1%E5%9D%97/","summary":"\u003ch3 id=\"can-interface-mcmcan\"\u003e\u003cstrong\u003eCAN Interface (MCMCAN)\u003c/strong\u003e\u003c/h3\u003e\n\u003ch4 id=\"1模块\"\u003e1.模块\u003c/h4\u003e\n\u003cp\u003e有三个模块CAN0,CAN1,CAN2，一般只使用CAN0,CAN0的模块比较全面。\u003c/p\u003e\n\u003cp\u003eCAN0有4个CAN node，Message RAM 一共32Kbyte，FIFO,buffer，register在此处开辟。\u003c/p\u003e","title":"英飞凌 TC3XX CAN Interface (MCMCAN)模块"},{"content":"事件触发操作系统，通过定时器届满，错误检出等event触发os task的调度运行\n1.TASK种类与状态 task的种类分为两种，基本task和扩展的task，处理器在同一时间只能运行一个task指令，os会负责保存和恢复task的状态切换时的数据\n1.1.Basic task 状态：Running，Ready，Suspended 单次任务模型，当基本任务中止或被强占时才会释放处理器 自动开始任务在startOS（）时被自动激活，可以开启需要等待事件的扩展任务。基本task在进入中止状态前只执行依次\n1.2.Extended task 状态：Running，Ready，Waiting，Suspended 多一个waiting状态，适合于需要中间执行同步的功能，使任务具有同步点，（当处理过程中发现缺少信息时，会切换到Waiting状态）时间响应较快，但会一直占用Ram资源，相当于以空间换时间\n1.3.状态与栈工作的对应 1.Basic task：当进入running状态时直接依次入栈，加到栈顶，然后依次出栈；2.Extended task：最差情况：在waiting过程中其他低优先级的任务都被激活并全被打断，扩展task的入栈位置需要确定，即知道实际任务占用的栈空间当扩展任务的栈管理异常时，会进入shutdownOS（）\n抢占式调度会增加任务context的切换时间和内存消耗，不可抢占式会降低实时性，但是节省context切换的时间\n2.调度方式 2.1.任务调度方式 FirstComeFirstServed（FCFS） 2.2.切换机制与调度策略 抢占式并需要设置优先级\n静态调度：在调度前就已经配置好；\n动态调度：根据负载率自动去调度 autosar支持静态调度，单个处理器一次处理一个任务，不同任务通过alarm进行切换\n抢占策略的选择：如果应用程序并行的task比较少，可以选择全抢占式，如果有明确的执行时间的短期任务则是和混合抢占式。\n若有三个task，优先级task1\u0026lt;task2\u0026lt;task3,task1,3,可以被抢占，task2不可被抢占，所以在运行task2时，task3不会抢占task2，优先级其实是相同的。好处是可以让task2运行完，节省了一次任务切换时间，并且task3的栈不会压到task2上增加栈空间。\n2.3.资源管理 死锁 1.概念：当两个（进程）task同时访问一个资源，在无外力作用时会导致无法推进 2.产生原因：竞争了不可剥夺资源；进程间的非法推进 3.产生条件：互斥条件 请求和保持 不剥夺 环路等待 4.解决方法：通过os resources使用优先级上限协议机制 优先级反转 优先级反转：低优先级延迟了高优先级的执行顺序\n举例：假设有task1-task4，优先级递减，task1强占task4对信号x的访问，此时task1在等待访问信号x的过程中（等待信号x被task4释放），中途会执行优先级比task4更高的task2和task3，当执行完task2,3后才会执行task4，即释放信号x，所以task1的执行时间被task2和task3延迟，即优先级反转。\n优先级上限协议：即上文的静态分配\n内容：在系统初始化时对每个资源静态分配一个上限优先级，当任务使用这个资源时任务的优先级就自动提升到该资源的优先级上限，使得其他访问该资源的task的优先级都低。 作用：相当于获得这个资源的最高级别优先级，其他试图访问该进程的优先级都低于它，所以不会发生倒挂\n3.task的激活 启动操作系统后，所有任务默认均为Suspended状态，用户去触发然后激活后进入Ready状态，os根据task的优先级进行调度\n3.1.Task的激活方式： 1.ActivateTask()—直接激活\n2.Alarm届满：对Alarm指定任务，该任务在每次Alarm届满时激活—间接激活\n对于一些重复的事件，通过定时器进行设置\n3.2 event和alarm机制 Alarm作用：以设置定时周期的方式\n1.激活一个task（通过硬件定时器产生tick time ，使system counter++，加到预设的值触发task） 2.设置一个event（event仅仅提供给Extended task） event为扩展任务提供同步点，每个event可以关联多个task，一个扩展任务ye可以用很多event，但是该event只能由其接受的那个扩展任务进行清除。\n当某个task正在运行时突然需要一个event，此时task进入waiting状态，释放cpu资源，而后去执行优先级低的task，等event来了，再执行改优先级高的task。\n3.设置回调函数 4.task的登录 Runnables可运行实体,SWC中的函数，再被达芬奇生成的时候手动添加实际功能，可以被定时器或者操作调用以及接受数据触发\nRunnables需要os中的task作为载体，需要放在操作系统的任务中来执行这个函数\nRunnables到task的映射需要RTE实现\n5.中断 优先级 说明 0类中断 不受OS管理 Timing protect 防止时间失效导致死锁，阻塞，错误同步等 1类中断 不与OS内部交互，1类中断不能被2类中断打断，开销小 2类中断 其中断向量表指向OS内部，开销大 Task 中断的周期性比task更严格 1类中断和2类中断的区别在于当有两个不同优先级的task1，task2时，在执行低优先级的task1时，如果1类中断被触发，退出中断后仍然回到task1，如果2类中断被触发，会进行os调度，中断退出后执行高优先级的task2\nAUTOSAR有两种时间保护机制，一种是执行时间的保护，一种是在bsw里的看门狗的保护\n5.HOOK机制 类似中断的机制，在OS里一般使用在StartupHook（操作系统启动后并且在调度程序运行前），ShutdownHook（系统被应用或是操作系统出错要求关闭），调试以及出错管理\nHOOK程序的优先级比task高，因为属于操作系统的一部分，所以不会被2类中断打断\n6.OS运行流程 硬件代码初始化\ncall startOS\nOS执行操作初始化代码\nOS执行StartupHook，用户将初始化程序放置在这里，此时所有中断无效\n操作程序激活中断和调度程序\n执行用户的中断和 task。\n","permalink":"https://fishdel.github.io/zh/posts/autosar-os%E5%8E%9F%E7%90%86/","summary":"\u003cp\u003e事件触发操作系统，通过定时器届满，错误检出等event触发os task的调度运行\u003c/p\u003e\n\u003ch4 id=\"1task种类与状态\"\u003e1.TASK种类与状态\u003c/h4\u003e\n\u003cp\u003etask的种类分为两种，基本task和扩展的task，处理器在同一时间只能运行一个task指令，os会负责保存和恢复task的状态切换时的数据\u003c/p\u003e","title":"Autosar OS原理"},{"content":"MCU系统结构 整体结构 因为学习的是STM32，所以按照手册进行理解。\n哈佛结构和冯诺依曼结构 首先我们在编写代码的时候，可以将代码分为两部分，一部分是逻辑代码部分，另一部分是定义的变量，逻辑代码是不用改变的，而变量会改变，哈佛结构和冯诺依曼结构就是对于这个两部分代码的存储方式有着一些区别。 冯诺依曼结构将程序存储器和数据存储器合并在一起的处理器架构设计，他的特点是使用同一个存储器，经由同一个总线传输。 哈佛结构将程序指令存储和数据存储分开存储，在嵌入式编程中一般使用这种方式，因为可以只修改数据不用修改逻辑代码。\nSTM32采用的就是这种哈佛结构 它分为四个驱动单元和四个从动单元 驱动单元：CM3，系统总线和数据总线，DMA 从动单元: SRAM，FLASH，FSMA，AHB到APB的桥以及连接的所有设备。\n可以从这个总线矩阵来看，它的前级是CM3和DMA，它的后级是从设备——也就是存储器以及各个外设控制器。\n各部分间联系 CM3内核 内核里面有NVIC嵌套中断向量控制器：ALU，寄存器组，内存保护单元，总线的内连接等。 NVIC:关于中断的内容可以看这一篇： 中断的概念与机制 寄存器组：其中寄存器组的R0-R12为通用寄存器。R13为堆栈指针寄存器，有2个寄存器，当时同一时间只能用一个，一个是主堆栈指针（MSP）：复位后缺省使用的堆栈指针，用于操作系统内核以及异常处理例程（包括中断服务例程）；另一个是进程堆栈指针（PSP）：由用户的应用程序代码使用。 R14为连接寄存器，当呼叫一个寄存器，R14存储返回地址；R15是程序计数器PC。 其中还有特殊功能寄存器组：\n程序状态字寄存器组（PSRs），记录ALU的标志。 中断屏蔽寄存器组（PRIMASK, FAULTMASK, BASEPRI） PRIMASK相当于中断总开关，关闭后只有NMI和硬fault才会响应；FAULTMASK打开后只有NMI才会响应； BASEPRI定义被屏蔽优先级的阈值。 控制寄存器（CONTROL）：选择堆栈的指针和线程模式。 总线 Icode指令总线：和FLASH的接口相连，用于取指令，它不经过总线矩阵，不需要切换，速度更快。 Dcode数据总线：主要用来读取存储在SRAM和Flash里的数据。 哈佛结构在这里我理解为SRAM是处理变量和堆栈记录，而Flash是我们烧进去的代码，SRAM是由锁存器构成，掉电之后内容就会没有，而Flash相当于逻辑代码。 DMA总线：可以访问Flash，SRAM，数据寄存器，然后三者间交换数据并且不占用CPU。 而总线矩阵在这里协调CM3内核和DMA的竞争。 系统总线：也叫外设总线，连接CM3的内核和外设。AHB通过桥接的方式进行了分频，APB2为72HZ上面搭载着GPIO口，ADC，TIM1等外设，而APB1为36HZ,上面挂着串口，看门狗等慢速设备。\n时钟 STM32F10x时钟源 HSI：（高速内部）RC振荡器，频率8MHz，精度不高 HSE：（高速外部）外接石英/陶瓷晶振（4MHz——16MHz） LSI：（低速内部）RC振荡器，频率40KHz，LSI是作为看门狗时钟源和RTC时钟源而独立使用 LSE：（低速外部）外接晶振，32.768KHz石英晶振 PLL：锁相环倍频输出，其时钟输入源可选择为HSI/2、HSE或者HSE/2。倍频可选择为2~16倍，但是其输出频率最大不得超过72MHz\n多个时钟源是因为兼容不同速度的外设，有些高速，有些低速，不同的时钟对应不同的模块。\n系统时钟SYSCLK可来源于三个时钟源：\nHSI振荡器时钟 HSE振荡器时钟 PLL时钟 系统时钟\u0026mdash;\u0026gt;AHB分频器\u0026mdash;\u0026gt;各个外设分频倍频器 \u0026mdash;\u0026gt; 外设时钟的设置\nstm32上电过程 1. 选择启动方式 STM32 上电复位后代码从0x00000000开始，选择不同的启动模式就是将不同的地址映射到0x00000000 从Flash启动，将0x80000000映射到0x00000000； 从系统存储器启动，一般是将stm32里带的Bootloader的代码映射到0x00000000，这个Bootloader就是将用户的代码通过串口下载到Flash，再从Flash启动； 从SRAM启动，将0x20000000映射到0x00000000。\n2. 根据复位中断向量表设置SP和PC 假定从Flash启动 向量表的存储位置是可以设置的，通过 NVIC 中的一个重定位寄存器来指出向量表的地址。在复位后，该寄存器的值为 0。因此，在地址 0 处必须包含一张向量表，用于初始时的异常分配。 起始地址存放堆顶指针，而第二个地址则必须存放复位中断入口向量地址，这样在Cortex-M3内核复位后，会自动从起始地址的下一个32位空间取出复位中断入口向量，跳转执行复位中断服务程序。\n3. 初始化系统时钟 在复位中断函数中调用 SystemInit 函数，初始化时钟，配置中断向量表等。\n4. 软件设置SP 在复位中断服务程序中会跳转__main函数，在这里面加载.data.bss,初始化栈区，在__main函数中调用C函数main。\n","permalink":"https://fishdel.github.io/zh/posts/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0mcu%E7%BB%84%E6%88%90%E7%BB%93%E6%9E%84%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%E4%B8%89/","summary":"\u003ch1 id=\"mcu系统结构\"\u003eMCU系统结构\u003c/h1\u003e\n\u003ch2 id=\"整体结构\"\u003e整体结构\u003c/h2\u003e\n\u003cp\u003e因为学习的是STM32，所以按照手册进行理解。\u003c/p\u003e\n\u003ch3 id=\"哈佛结构和冯诺依曼结构\"\u003e哈佛结构和冯诺依曼结构\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e首先我们在编写代码的时候，可以将代码分为两部分，一部分是逻辑代码部分，另一部分是定义的变量，逻辑代码是不用改变的，而变量会改变，哈佛结构和冯诺依曼结构就是对于这个两部分代码的存储方式有着一些区别。\n\u003cstrong\u003e冯诺依曼结构\u003c/strong\u003e将程序存储器和数据存储器合并在一起的处理器架构设计，他的特点是使用同一个存储器，经由同一个总线传输。\n\u003cstrong\u003e哈佛结构\u003c/strong\u003e将程序指令存储和数据存储分开存储，在嵌入式编程中一般使用这种方式，因为可以只修改数据不用修改逻辑代码。\u003c/p\u003e","title":"嵌入式系统——MCU组成结构与程序运行机制"},{"content":"1. CPU的运行原理 1.1 CPU最基本的工作单元——MOSFET 二极管的工作原理 SI原子外层有4个电子，P原子外层有5个电子，B原子外层有3个电子；如果SI和P结合，就会多一个电子，导电性上升；如果SI和B结合，就会有一个空穴，会有电子过来，导电性也会上升；电子和空穴都叫载流子，载流子就是电流的载体。 增加空穴的掺杂——P型掺杂；增加电子的掺杂——N型掺杂。 如果在一块硅晶体左边和右边进行N型掺杂和P型掺杂，中间区域就会出现电子从N区扩散到P区和空穴结合；交界处N区域失去电子显正电，P区域得到电子带负电，中间产生一个电场，叫耗尽层。 如果在外面接一个电池，如果电池提供的电场和中间电场方向相反进行抵消，此时电路导通；如果反过来接，耗尽层加宽，则不能导通。\nMOSFET的工作原理 在一块纯硅的两个肩膀处进行N型掺杂，其余部分P型掺杂，交界处产生耗尽层。 在两个N区域中间下层接绝缘层上层放金属板，P区域也充当金属板（类似于一个电容），此时通电很多电子填到下层P区域的空穴中，然后下面又出现一个耗尽层，两个N区域被联通起来，这个区域称为N沟道。\n如果施加一个能使沟道产生的电压，电路被导通，这个电压叫做阈值电压，中间电极称为栅极，左边称为源极，右边称为漏极。\n高于阈值电压被导通，低于阈值电压不导通：NMOS； 如果NP掺杂时相反，高于阈值电压不导通，低于阈值电压导通：PMOS；\n把NMOS和PMOS的漏极连起来得到一个CMOS。 对于NMOS远离源头。 对于PMOS指向源头 1.2 逻辑门 门电路 以非门为例： 以或门为例： 假设A=0,B=1,那么A上的PMOS导通，B上的NMOS导通，PMOS不导通，中间的主线相当于接了VSS，为0，再经过非门，得到值为1。 之后以此类推。 所以，或非门就是不要后面的非门。 同样，与非门： 可以得出： 与非门+非门=与门 或非门+非门=或门\n异或门： 逻辑的组合 异或门+与门可以构成半加法器，但是不能进位（后面会解释） 所以两个半加器级联，构成全加法器，四个串联起来四位全加法器。\n时序的逻辑 D锁存器 C端如果是0，D端不起作用 C端为1的时候，Q和Q~被D刷新成新值 锁存器是一种存储数据形式，SRAM是用这种形式，所以掉电之后代码会丢失。\nD触发器 一对D锁存器构成，控制信号相反 clk为高，主关从开；clk为低，主开从关；低电平前面导通，高电平后面导通，从而实现一个节拍结构。\n1.3 CPU计算加法的原理 本位和（Sum），进位（Carry） 10进制 3+9=12里2是本位和，1是进位数； 2进制 1+1=10里0是本位和，1是进位数 A B S C 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 本位和S符合异或的逻辑，进位C符合与的逻辑，用这两个门就可以计算一位二进制数得到一个半加法器，但是它的输入只有A,B，就不能输入上一次计算的进位数，所以需要再来一个可以计算上一位进位数的加法器。 本位S还要和Cin再异或一次；进位条件是A,B,Cin有大于等于两个以上的1就行了,这三个分别做与运算，结果再分别做或运算。 四个全加法器Cin和Cout首位串起来，就是4位串行进位全加法器。\n1.4 总结 逻辑门由MOSFET组成，把它们刻蚀到芯片上，按电路图连接，CPU内部全是门电路构成。\n2. 概念CPU之微控制器MCU和ARM CPU是什么 CPU是计算机/微控制器的核心，进行算术逻辑运算，通用CPU需要大量外围辅助. MUC 微控制器：完成的计算机系统，单个芯片包含了处理器，存储器和所有外设I/O模块. 其组成包括： CPU 输入输出接口 外设接口 RAM ROM 时钟单元 MCU优势在于：小巧，低成本，低功耗。\nARM是什么 1 一家公司，全球领先的半导体知识产权(IP)提供商，ARM设计了大量高性价比、耗能低的RISC处理器、相关技术及软件。 2 ARM指的也是一门技术，具有性能高、成本低和能耗省的特点。在智能机、平板电脑、嵌入控制、多媒体数字等处理器领域拥有主导地位。 3 ARM还是一类微型处理器的统称，其微型处理器包含多个系列，每个系列各自的特点和应用领域。\n3.\tCPU的基本结构和运行机制 3.1 基本结构 运算逻辑单元（Arithmetic Logic Unit） 操作数operands 运算operation 结果\tresult 标志\tflag 寄存器组（register file） 作用：用于临时保存/获取操作数 程序状态寄存器（program status register/CCR） 特点：1. 执行单元产生的标志通常放在PSR中；2. 每执行一条指令，相应的状态位更新；3.\t每条指令影响的状态位不同 条件码：Zero，Negative，Overflow，Carry 寄存器与CPU的关系：\t任何CPU都包含通用/专用寄存器；\t寄存器的数目和宽度是衡量CPU的重要指标。 寄存器和内存的关系：内存并不在CPU上访问比较慢，寄存器相当于衣服上的口袋，内存相当于包包。在单片机中，寄存器相当于要操作的外设的别名，通过操作寄存器对外设进行控制。 ARM五个寄存器编程模型 xPSR——保存cpu各种状态（32位） 通过别名访问，只关注和访问特定的字段实现特定功能 APSR—高四位：N Z C V IPSR—后几位：发生异常时的中断号 EPSR—T：记录是否发生异常和中断 PRIMASK：PM 控制中断的总开关 FAULTMASK BASEPRI CONTROL 控制寄存器：实现控制堆栈指针的选择和切换到用户级 程序计数器（Program Counter） 作用：保存下一条待执行的指令 控制单元（Control Unit） 指令解析 分析该指令需要执行何种操作 原理：程序由指令序列构成，保存在程序存储器中，这些指令序列依次进入CPU执行 数据流向 3.2堆栈的概念 栈 概念：一段连续的存储空间 工作方式：后入先出，只能从顶部加入或取出数据 特点：堆栈能保持数据的顺序 操作方式：PUSH，PULL 栈的实际使用：分支调用，嵌套调用，顺次保存函数地址，逆序取用。 栈的作用\nC语言编译器使用堆栈来完成参数的传递和返回值传递 汇编语言使用堆栈来保存局部变量，寄存器值 CPU硬件使用堆栈来保存返回地址和寄存器上下文（中断） 栈与寄存器的关系 栈的顶端位置通过CPU内的堆栈指针寄存器确定（stack pointer） 初始位置由程序代码确定，指向预先划定的堆栈空间底部 地址 变量地址从低地址向高地址划分 堆栈空间从高地址向低地址增长 堆 概念：一个进程开启后，系统分配给它的一个全局的空间，系统中所有动态分配的对象（指针）都在这个空间分配。 注意：堆里的数据是有数据结构的，空间占用不连续 堆栈溢出：堆栈溢出的产生是由于过多的函数调用，导致调用堆栈无法容纳这些调用的返回地址，一般在递归中产生。\n3.3 运行机制 该程序的功能：A和B做对换 具体步骤 1 开始什么也不做，随后加载一个栈指针 2 A入栈，B入栈 3 跳到函数subfunc 3.1 do nothing 3.2 返回 4 A出栈，B出栈\n","permalink":"https://fishdel.github.io/zh/posts/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0cpu%E4%B8%80/","summary":"\u003ch1 id=\"1-cpu的运行原理\"\u003e1. CPU的运行原理\u003c/h1\u003e\n\u003ch2 id=\"11-cpu最基本的工作单元mosfet\"\u003e1.1 CPU最基本的工作单元——MOSFET\u003c/h2\u003e\n\u003ch3 id=\"二极管的工作原理\"\u003e二极管的工作原理\u003c/h3\u003e\n\u003cp\u003eSI原子外层有4个电子，P原子外层有5个电子，B原子外层有3个电子；如果SI和P结合，就会多一个电子，导电性上升；如果SI和B结合，就会有一个空穴，会有电子过来，导电性也会上升；电子和空穴都叫载流子，载流子就是电流的载体。\n增加空穴的掺杂——P型掺杂；增加电子的掺杂——N型掺杂。\n如果在一块硅晶体左边和右边进行N型掺杂和P型掺杂，中间区域就会出现电子从N区扩散到P区和空穴结合；交界处N区域失去电子显正电，P区域得到电子带负电，中间产生一个电场，叫耗尽层。\n如果在外面接一个电池，如果电池提供的电场和中间电场方向相反进行抵消，此时电路导通；如果反过来接，耗尽层加宽，则不能导通。\u003c/p\u003e","title":"嵌入式系统——CPU"},{"content":"中断的概念和机制 中断与轮询 中断： 由硬件判断外部事件并通知CPU；专用的中断服务程序来处理事件 处理对响应要求非常高的事件 处理持续事件非常短的事件 低功耗的应用 程序设计复杂 通常把CPU内部的紧急时间叫做异常，比如地址访问越界； 把CPU外部的片上外设产生的紧急时间叫做中断,比如GPIO口引脚的电平变化。 中断和异常都是停下当前任务去执行紧急事件，所以一般统称位中断。\n轮询：周期/连续检查外部事件是否发生 消耗大量CPU处理时间 在CM3的内核中1~15号是系统异常，16 ~ 256是外部中断，有内核中的NVIC（嵌套向量中断控制器）\n中断控制器（NVIC） 作用\n中断管理 支持异常及中断向量化处理 支持嵌套中断 中断管理 全局中断控制 CRR寄存器中的一个特殊位 在复位和中断后该位置\nDedicated IE 在复位后所有中断被禁止\n中断标志位 IF 中断源对应的标志 引发请求 读写操作清除中断标志位\n寄存器AIRCR（中断和复位寄存器） 0-7位：设置抢占优先级和子优先级的级数 8-10位：优先级分组 中断先看抢占优先级，抢占优先级越高越先触发；抢占优先级相同，子优先级高的先触发。 抢占优先级高可以打断低的，即中断的嵌套，实现中断的嵌套\n中断优先级特点\n多个中断同时出现，高优先级中断先得到响应。 中断优先级可以是固定的或者是编程指定的 固定优先级：根据中断向量表顺序（比如S12内核） 设定优先级：每个中断都有优先级设置位（比如ARM Cortex M0+支持4个优先级） 每一个部件也有一个自己的中断控制器，而NVIC相当于总管家。\n中断和异常向量表 内部异常和外部中断按照优先级进行排列形成一张中断向量表，一般数字越小优先级越高。当发生中断和异常的时候，处理器将PC指向表中的相应地址，这个地址叫做异常向量。 前三个优先级是最高的。 中断向量表的特点：\n一段连续的存储空间 ； 复位后有默认起始位置 ； 每个中断在向量表中都有相应的表项，该表项的值为该中断对应的服务程序的地址； 中断向量表里的内容赋值给PC指针，程序相应的就会发生跳转 由程序代码确定中断向量表的每个表项； 中断向量表的位置是可以通过改写中断向量基址寄存器重新定位的； 在系统上电之后，即缺省情况下，会执行这样一张表，后面会讲到。 工作流程 中断的过程 一个中断的产生可以归纳为打开中断总开关，使能中断，配置发生条件：\nstep1 设置自己的控制单元； step2 在NVIC里打开总中断； step3 中断源送到CPU； step4 CPU查找中断向量表，找到处理函数的地址去执行函数。 以GPIO口电平引起的中断为例，有一个EXIT外部中断控制器，管理所有GPIO的中断。EXTI进行使能，将请求通知给NVIC，再通知CPU。\n堆栈情况 通过栈结构进行现场恢复的步骤\nstep1 主程序执行的时候，发生中断 step2 压栈保留一个现场执行中断函数 step3 中断函数没有执行完，又来了一个更高优先级的中断 step4 再次打断它，在保留一个CPU的现场 step5 再占一部分堆栈去执行更高优先级的中断 step6 依次退出 注意点\n中断的寄存器入栈由CPU硬件自动完成 在中断时寄存器在堆栈中的保存顺序是在其手册中指明的。 开发者需要手动的把发生变化的寄存器压入堆栈。 中断服务子程 中断服务子程也叫中断服务函数，在一些CPU中，中断服务子程不同于一般的C函数，函数退出时的返回汇编指令有所区别。\t在ARM Cortex M0+平台上，中断服务子程与一般C语言写法没有区别，使用同样的汇编返回指令即可\n中断子程序函数特点：\n被CPU硬件自动调用 在ISR执行前、后，CPU自动进行了堆栈出入等操作 写成C语言的参数和返回值都应该为void ","permalink":"https://fishdel.github.io/zh/posts/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%AD%E6%96%AD%E4%BA%8C/","summary":"\u003ch2 id=\"中断的概念和机制\"\u003e中断的概念和机制\u003c/h2\u003e\n\u003ch3 id=\"中断与轮询\"\u003e中断与轮询\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e中断\u003c/strong\u003e： 由硬件判断外部事件并通知CPU；专用的中断服务程序来处理事件\n\u003cul\u003e\n\u003cli\u003e处理对响应要求非常高的事件\u003c/li\u003e\n\u003cli\u003e处理持续事件非常短的事件\u003c/li\u003e\n\u003cli\u003e低功耗的应用\u003c/li\u003e\n\u003cli\u003e程序设计复杂\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003e通常把CPU内部的紧急时间叫做异常，比如地址访问越界；\n把CPU外部的片上外设产生的紧急时间叫做中断,比如GPIO口引脚的电平变化。\n中断和异常都是停下当前任务去执行紧急事件，所以一般统称位中断。\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e","title":"嵌入式系统—中断"},{"content":"1.数据导入 df=pd.read_csv(\u0026#39;Pokemon.csv\u0026#39;,encoding=\u0026#34;ISO-8859-1\u0026#34;) df.head() 2.查看数据 df.shape df.shape[0] df.shape[1] df.columns #列名 df.index #行名 df.dtypes df.head() df.tail() df.sample() df.describe() pd.set_option(\u0026#39;max_colwidth\u0026#39;,8) #设置每一行的最大宽度，恢复原设置方法 pd.reset_option(\u0026#39;max_colwidth\u0026#39;) loc操作与iloc loc通过label定位；iloc通过position定位\ndf.loc[[0,5],[\u0026#39;名称\u0026#39;,\u0026#39;生命点数\u0026#39;]] df.iloc[0:10,[0,1]] df[:3] 3.数据筛选与操作 df[df[\u0026#39;综合能力\u0026#39;]\u0026gt;400].head() df[df[\u0026#39;世代数\u0026#39;]==1] df.insert(4,\u0026#39;能力600\u0026#39;,df[\u0026#39;综合能力\u0026#39;]\u0026gt;=600) 4.读取数据 pickle文件，可以将python中的数据类型进行序列化 compression参数指定了压缩类型，\u0026lsquo;zip\u0026rsquo;, \u0026lsquo;gzip\u0026rsquo;, \u0026lsquo;bz2\u0026rsquo;, \u0026lsquo;zstd\u0026rsquo;\npandas.read_pickle(filepath_or_buffer, compression=\u0026#39;infer\u0026#39;, storage_options=None) 5.处理数据 （1）取特定的行列 利用标签取列：\ndata[\u0026#39;xxx\u0026#39;] data.at #访问单个值 data.loc[\u0026#39;行名\u0026#39;,\u0026#39;列名\u0026#39;] #访问值组 data.loc[[\u0026#39;A行\u0026#39;,\u0026#39;B行\u0026#39;]] # 取多行 （2）对需要的行列进行处理 时间处理 #转换回日期格式,默认是毫秒 data[\u0026#39;time\u0026#39;] = pd.to_datetime(data[\u0026#39;time\u0026#39;],unit=\u0026#39;s\u0026#39;) #生成一个时间戳 now = pd.Timestamp.now() 某一列数据进行处理 data[\u0026#39;xxx\u0026#39;] = [\u0026#39;这里用一个列表表达式\u0026#39;] data[\u0026#39;xxx\u0026#39;] = data[\u0026#39;xxx\u0026#39;].apply(lamdba x: function(x)) 对于字符串的处理 #去掉首位的a data[\u0026#39;a\u0026#39;] = data[\u0026#39;a\u0026#39;].str.strip(\u0026#39;a\u0026#39;) # 替换 data[\u0026#39;a\u0026#39;] = data[\u0026#39;a\u0026#39;].str.replace(\u0026#39;a\u0026#39;，\u0026#39;b\u0026#39;) 将特定的列拆分后合并 data = pd.concat([data, data[\u0026#39;tempvaluearray\u0026#39;].str.split(\u0026#39;,\u0026#39;, expand=True)], axis=1).drop(\u0026#39;tempvaluearray\u0026#39;, axis=1) pd.concat()#可以按照指定的轴将dataframe或者series拼接，而merge只能拼接两个表 axis #0：上下拼接 1：左右拼接 #drop()的参数axis在删除特定的一行为1 批量修改列名称 data.rename(columns=dic)#可以写一个字典进行替换 计算时保留小数 \u0026#39;%.3f\u0026#39;%0.0065 #保留3位小数 lambda x:\u0026#39;%.3f\u0026#39;%((int(x)-1000)/1000)) 调整列顺序 #先取出来再插入 A = data[\u0026#39;A\u0026#39;] data.drop(labels=[\u0026#39;A\u0026#39;],axis = 1,inplace=True) data.insert(0, \u0026#39;A\u0026#39;, A) 一个字符串划分的函数 # 对于数据为[12，23，45，12]类似的数据，其中不能是字符串 def split_col(data, columns): \u0026#34;\u0026#34;\u0026#34;拆分成列 :param data: 原始数据 :param columns: 拆分的列名 :type data: pandas.core.frame.DataFrame :type columns: list \u0026#34;\u0026#34;\u0026#34; for c in columns: new_col = data.pop(c) max_len = max(list(map(len, new_col.values))) # 最大长度 new_col = new_col.apply(lambda x: x + [None]*(max_len - len(x))) # 补空值，None可换成np.nan new_col = np.array(new_col.tolist()).T # 转置 for i, j in enumerate(new_col): data[c + str(i)] = j 去重 df_unique = df.drop_duplicates([\u0026#39;A\u0026#39;], keep=\u0026#39;last\u0026#39;) # 根据A去重只留下最后一次出现的 隔行取数据 方法1：每隔20行取数,把每20行的id取出来\na=[] for i in range(0,len(df),20): a.append(i) new_df= df.iloc[a] 方法2：直接调用read_csv()里的参数skiprows\ndf = pd.read_csv(\u0026#39;test.csv\u0026#39;,header = None,skiprows=lambda x: x \u0026gt; 0 and x % 20 != 0) df 6.遍历和求值 遍历\nfor name,group in grouped_single: print(name) display(group.head()) 组内求均值，最大值，最小值等等\ndf.groupby(\u0026#39;A\u0026#39;)[\u0026#39;B\u0026#39;].mean() agg自定义内置函数 df.groupby(\u0026#39;A\u0026#39;)[\u0026#39;B\u0026#39;].agg(fun()) 整合（Aggregation）——即分组计算统计量（如求均值、求每组元素个数）\n变换（Transformation）——即分组对每个单元的数据进行操作（如元素标准化）\n过滤（Filtration）——即按照某些规则筛选出一些组（如选出组内某一指标小于50的组）\n7.举例：分组统计次数或者频数 ①先排序，再分组，加上first()就是取最大值。\n若B是种类，C是时间，就是先按种类分组，再按时间分组，取A最大的情况。\ndf1=df.sort_values(\u0026#39;A\u0026#39;, ascending=False).groupby([\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;], as_index=False).first() ②apply方法(groupby+apply这样的写法比之前写循环要快很多)\n先分组再对A属性进行数量统计，加上head（1）取最大值\n需要加上reset_index,时期扁平化，原来的index变成数据列，保留下来\ndf2=df.groupby(\u0026#39;B\u0026#39;)[\u0026#39;A\u0026#39;].apply(lambda x: x.value_counts(normalize=True).head(1)).to_frame().reset_index() 举个例子：统计数学成绩前五名的学生\ndf = df[\u0026#39;math\u0026#39;].groupby(\u0026#39;class\u0026#39;).apply(lambda x: x.sort_values(ascending = False)[:5])` 1. value_counts()方法 需要加上unstack()将其展开，否则是一个序列。\ndata.groupby(\u0026#39;A\u0026#39;)[\u0026#39;B\u0026#39;].value_counts().unstack() 2.DataFrame.plot函数 画在一个子图里\npd.DateFrame.plot(kind = \u0026lsquo;scatter\u0026rsquo;)\nax=v1.plot.scatter(x=\u0026#39;C\u0026#39;,y=\u0026#39;D\u0026#39;,label=\u0026#39;D\u0026#39;,title=i,alpha=0.4,) v1.plot.scatter(x=\u0026#39;C\u0026#39;,y=\u0026#39;E\u0026#39;,c=\u0026#39;r\u0026#39;,title=i,alpha=0.4,ax=ax,label=\u0026#39;E\u0026#39;) ","permalink":"https://fishdel.github.io/zh/posts/pandas%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/","summary":"\u003ch3 id=\"1数据导入\"\u003e1.数据导入\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003epd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eread_csv\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;Pokemon.csv\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003eencoding\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;ISO-8859-1\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ehead\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2查看数据\"\u003e2.查看数据\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecolumns\u003c/span\u003e \u003cspan class=\"c1\"\u003e#列名\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eindex\u003c/span\u003e \u003cspan class=\"c1\"\u003e#行名\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edtypes\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ehead\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etail\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esample\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edescribe\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_option\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;max_colwidth\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"c1\"\u003e#设置每一行的最大宽度，恢复原设置方法\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epd\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereset_option\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;max_colwidth\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eloc操作与iloc\nloc通过label定位；iloc通过position定位\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eloc\u003c/span\u003e\u003cspan class=\"p\"\u003e[[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e],[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;名称\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;生命点数\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eiloc\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e,[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[:\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"3数据筛选与操作\"\u003e3.数据筛选与操作\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;综合能力\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e\u003cspan class=\"mi\"\u003e400\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ehead\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;世代数\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e==\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einsert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;能力600\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\u003cspan class=\"n\"\u003edf\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;综合能力\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;=\u003c/span\u003e\u003cspan class=\"mi\"\u003e600\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"4读取数据\"\u003e4.读取数据\u003c/h3\u003e\n\u003cp\u003epickle文件，可以将python中的数据类型进行序列化\ncompression参数指定了压缩类型，\u0026lsquo;zip\u0026rsquo;, \u0026lsquo;gzip\u0026rsquo;, \u0026lsquo;bz2\u0026rsquo;, \u0026lsquo;zstd\u0026rsquo;\u003c/p\u003e","title":"Pandas基本用法"},{"content":"基本概念 1.docker 应用容器引擎，用于运行容器\nimage：可执行程序\ncontainer：运行起来的进程\ndockerfile：image的源代码， 是一个用**来构建镜像的文本文件，**文本内容包含了一条条构建镜像所需的指令和说明\n部署流程 上传项目文件到服务器\n进去项目文件夹 使用 Dockerfile 创建镜像\ndocker build -t server:v1\n运行容器之前先停止老版本\ndocker stop server-container:v0\n基于镜像运行容器\ndocker run -d name server-container:v1 server:v1\n暂停一个容器\ndocker pause haicoder\n删除之前版本的容器\ndocker rmi server:v0\n若有问题恢复之前版本\ndocker stop server-container:v1\ndocker run -d --name server-container:v0 server:v0\n基于dockerfile创建实例 创建一个Dockerfile 创建一个文件夹mkdir TestDockerfile 进入文件夹cd TestDockerfile 创建一个文件touch Dockerfile 写入echo \u0026quot;hello world\u0026quot; \u0026gt; index.html创建一个测试文件 在相应的目录下 docker build -t = “beyond/test：v1” 镜像名为beyond/test，标签为v1 Dockerfile文件 FROM ubuntu：16.04 基于哪个镜像进行构建\nMAINTAINER beyond ubuntu ”111@qq.com\u0026quot;\nADD ./index.html /var/www/html/index.html把和Dockerfile文件在同一个目录里面的文件添加到要构建的镜像\nWORKDIR /var/www/html 设置工作目录\nrun和cmd的区别：run在构建镜像时运行，cmd在容器启动时运行，每次启动都执行一次\n常用命令 docker build：构建镜像。\n常用格式：docker build -t 镜像名 Dockerfile所在路径。 示例： dcoker build -t image-name .。 docker run：基于镜像启动容器。\n常用格式：docker run -id -t --name 容器名 镜像名/镜像ID。 示例：docker run -id -t --name container-name image-name。 docker ps：显示所有容器信息。\ndocker images：显示所有镜像信息。\ndocker stop：停止容器运行。\ndocker start：重新运行一个已停止的容器。\ndocker rm：删除容器，删除之前要先确保容器已经停止运行。可以指定多个容器。\n示例：docker rm container-name。 docker rmi：删除镜像，删除之前要确保没有基于该镜像的容器存在。可以指定多个镜像。\n示例：docker rmi image-name/image-id docker exec：在容器中执行命令。\n示例：docker exec -it container-name /bin/bash。这命令可以启动容器内的 bash，其中 -i 表示以交互的方式运行命令，-t 表示在终端（tty）中运行。 2.一些概念 集群 服务器集群，使多台服务器能够像一台服务器那样工作或者是看起来好像一台机器，提高数据处理能力以及服务能力。\n负载均衡 把工作分到多个服务器，防止哪个服务器宕机其他服务器能提供相同内容。\n负载均衡器：选择能正常做出响应的后端服务器。\n分布式 分散的物理和逻辑资源通过计算机网络实现信息的交换。\n例：计算1+2+\u0026hellip;+100 一台电脑处理1+2+3\u0026hellip;+50,另一台处理51+52\u0026hellip;+100，再进行汇总\n区别 集群是将几台服务器集合到一起，来实现同一业务，分布式是处理不同任务。\nElyra 一个AI项目：数据预处理— 特征抽取— 训练— 模型评估— 部署 提供一个pipeline可视化编辑器，将多个Notebook转换为批处理或工作流 kubernetes(k8s) 一个容器集群的管理系统\n快速部署应用\n快速扩展应用\n无缝对接新的应用功能\n节省资源，优化硬件资源的使用\nKubeflow kubernetes的机器学习工具包\nkubeflow的组建：\tPipeline Pipeline：（Pipelines是一个基于Argo实现了面向机器学习场景的流水线项目，提供机器学习流程的创建、编排调度和管理，提供了一个Web UI）提供一个ui来定义机器学习的过程，整个过程运行在k8s集群上。\npipeline是一个机器学习工作流的抽象概念，可以是一个函数过程，也可以是数据加载，变换，清洗等环节。\n在pipelines构建各流程组件前，需要将对应流程的业务代码打包成docker镜像文件（因为kubeflow中运行的代码均以容器方式实现）\n构建pipeline的步骤 安装专门的sdk：打包Docker镜像，镜像是组件的依赖，每一个组件运行就是一个容器。\npython定义好pipeline：python函数描述组件的输入输出信息，有几个节点输入有几个节点输出。\nsdk构建pipeline的包，通过ui上传\nk8s命令和kubeflow平台 查看版本kubectl version\n查看命名空间 kubectl get ns\n查看命名空间下的资源 kubectl get pods --namespece (名字)\n运行pipeline需新建experiment，即将同一类的pipeline 运行放在同一个experiment中（KFP）\n运行一个pipelines：上传yaml文件或者通过url上传\n","permalink":"https://fishdel.github.io/zh/posts/%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%B9%8Bdocker+kubeflow/","summary":"\u003ch2 id=\"基本概念\"\u003e基本概念\u003c/h2\u003e\n\u003ch4 id=\"1docker\"\u003e1.docker\u003c/h4\u003e\n\u003cp\u003e应用容器引擎，用于运行容器\u003c/p\u003e\n\u003cp\u003eimage：可执行程序\u003c/p\u003e\n\u003cp\u003econtainer：运行起来的进程\u003c/p\u003e\n\u003cp\u003edockerfile：image的源代码， 是一个用**来构建镜像的文本文件，**文本内容包含了一条条构建镜像所需的指令和说明\u003c/p\u003e","title":"工程化之docker+kubeflow"},{"content":"LightGBM 介绍 LightGBM（Light Gradient Boosting Machine）：一个实现GBDT算法的框架，解决GBDT在海量数据遇到的问题。\n两大技术： （1）GOSS(Gradient-based One-Side Sampling)：减少样本数\n（2）EFB (Exclusive Feature Bundling ):减少特征数\nXGBoost的缺点：先预排序再找分割点，空间消耗大\nXGBoost与LightGBM的区别：\nlightGBM XGBoost 分裂方式 leaft-wise选择分裂收益最大的节点，要限制深度容易过拟合 level-wise无差别分裂 输入 lightgbm支持直接输入categorical 的feature 需要one-hot编码 时间复杂度 基于直方图的决策树算法，直方图的优化算法只需要计算K次，时间复杂度为O(Kfeature) 基于预排序的决策树算法，每遍历一个特征就需要计算一次特征的增益，时间复杂度为O(datafeature) 特征捆绑转化为图着色问题，减少特征数量 两种使用形式 sklearn接口形式 导包\nimport lightgbm as lgb from sklearn.metrics import mean_squared_error from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split gbm = lgb.LGBMRegressor(objective=\u0026#39;regression\u0026#39;, num_leaves=31, learning_rate=0.05, n_estimators=20) gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric=\u0026#39;l1\u0026#39;, early_stopping_rounds=5) 原生形式 数据集切分与转换\nlgb_train = lgb.Dataset(X_train, y_train) #If this is Dataset for validation, training data should be used as reference. lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train) 将参数写为字典形式\nparams = { \u0026#39;task\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;boosting_type\u0026#39;: \u0026#39;gbdt\u0026#39;, # 设置提升类型 \u0026#39;objective\u0026#39;: \u0026#39;regression\u0026#39;, # 目标函数 \u0026#39;metric\u0026#39;: {\u0026#39;l2\u0026#39;, \u0026#39;auc\u0026#39;}, # 评估函数 \u0026#39;num_leaves\u0026#39;: 31, # 叶子节点数 \u0026#39;learning_rate\u0026#39;: 0.05, # 学习速率 \u0026#39;feature_fraction\u0026#39;: 0.9, # 建树的特征选择比例 \u0026#39;bagging_fraction\u0026#39;: 0.8, # 建树的样本采样比例 \u0026#39;bagging_freq\u0026#39;: 5, # k 意味着每 k 次迭代执行bagging \u0026#39;verbose\u0026#39;: 1 # \u0026lt;0 显示致命的, =0 显示错误 (警告), \u0026gt;0 显示信息 } 交叉验证与预测评估\ngbm = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5) y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration) Epoch、Iteration、Batchsize相关理解 Epoch 一个完整的数据集通过了神经网络一次并且返回了一次。梯度下降的方法来优化学习过程，随着epoch\nIteration batch需要完成一个epoch的次数\n一个迭代=一个正向通过+一个反向通过。\nBatchsize 不能将数据一次性通过神经网络的时候，就需要将数据集分成几个batch。\nbatchsize的选择：Batch：数据集较小选择全批次\nmini batch：选定后以batch的大小输入网络，计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均\nstochastic：每次修正方向以各自样本的梯度方向修正，难收敛\n如果batchsize过小，训练数据难以收敛容易欠拟合，增加batchsize相对处理速度加快但是占用内存增加\n参数调整 预先固定的参数 调整策略 learning_rate 0.05~0.1 学习率较小比较稳定。默认0.1 n_estimators 100~1000。可以设置一个较大的值配合early_stopping_round来让模型根据性能自动选择最好的迭代次数。默认100 min_split_gain 执行节点分裂的最小增益。默认为0。不建议去调整。增大这个数值会得到相对浅的树深。可调整其他参数得到类似效果。 min_child_sample 一个叶子上的最小数据量。默认设置为20.数据量大适当增加 min_child_weight 一个叶子上的最小hessian和。默认设置为0.001，一般设置为1。不建议调整，增大数值会得到较浅的树深 通过算法来搜索的参数 调整策略 max_depth 3，4，5（过大容易过拟合） num_leaves 小于2^max_depth-1 subsample 大致的搜索范围[0.8, 0.9, 1.0] colsample_bytree 大致的搜索范围[0.8, 0.9, 1.0] reg_alpha 服务于L1正则化，一般取0-1000的范围。通过特征筛选该数值由大变小可以增加模型信心 reg_lambda 服务于L2正则化，一般0-1000的范围。如果有非常强势的特征，可以人为加大一些reg_lambda使得整体特征效果平均一些，一般会比reg_alpha的数值略大一些，但如果这个参数大的夸张也需要再查看一遍特征是否合理 代码实例 训练模型并求最优参数的函数定义 import lightgbm as lgb def cv_model(clf, train_x, train_y, test_x, clf_name): 划分100折并进行数据打乱\nfolds = 10 seed = 2022 kf = KFold(n_splits=folds, shuffle=True, random_state=seed) ​ 设置测试集的输出矩阵。每一组数据输出：[0,0,0,0]以概率值填入\ntest = np.zeros((test_x.shape[0],4)) #交叉验证分数 cv_scores = [] onehot_encoder = OneHotEncoder(sparse=False) 根据折数进行划分，i值代表第（i+1）折。每一个K折都进行「数据混乱：随机」操作 train_index：10折里9折在train_index valid_index：剩下1折样本索引值，作为验证集用于给出「训练误差」\nfor i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)): if i \u0026lt; 9: #打印第（i+1）个模型结果 print(\u0026#39;模型:\u0026#39;i+1) #将训练集分为：真正训练的数据（K-1折），和 训练集中的测试数据（1折） trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y.iloc[train_index], train_x.iloc[valid_index], train_y.iloc[valid_index] 模型的设置和调用 #LGB模型 if clf_name == \u0026#34;lgb\u0026#34;: #训练样本 train_matrix = clf.Dataset(trn_x, label=trn_y) #训练集中测试样本 valid_matrix = clf.Dataset(val_x, label=val_y) #参数设置 params = { \u0026#39;boosting_type\u0026#39;: \u0026#39;gbdt\u0026#39;, #boosting方式 \u0026#39;objective\u0026#39;: \u0026#39;multiclass\u0026#39;, #任务类型为「多分类」 \u0026#39;num_class\u0026#39;: 4, #类别个数 \u0026#39;num_leaves\u0026#39;: 2 ** 5, #最大的叶子数，树模型的复杂度 \u0026#39;feature_fraction\u0026#39;: 0.8, #每次迭代中随机选择特征的比例（0.5-0.9之间调整） \u0026#39;bagging_fraction\u0026#39;: 0.8, #不进行重采样的情况下随机选择部分数据（0.5-0.9之间调整） \u0026#39;bagging_freq\u0026#39;: 5, #每5次迭代，进行一次bagging（3-5之间调整） \u0026#39;learning_rate\u0026#39;: 0.05, #学习率 \u0026#39;seed\u0026#39;: seed, #seed值，保证模型复现 \u0026#39;nthread\u0026#39;: 28, \u0026#39;n_jobs\u0026#39;:24, #多线程 \u0026#39;verbose\u0026#39;: 1, \u0026#39;lambda_l1\u0026#39;: 0.4, # L1正则化 \u0026#39;lambda_l2\u0026#39;: 0.5, #L2正则化 \u0026#39;min_data_in_leaf\u0026#39;:100, #叶子可能具有的最小记录数 } #模型 model = clf.train(params, train_set=train_matrix, #训练样本 valid_sets=valid_matrix, #测试样本 num_boost_round=2000, #迭代次数 verbose_eval=100, early_stopping_rounds=200) #如果数据在200次内没有提高，停止计算 ​\nval_pred = model.predict(val_x, num_iteration=model.best_iteration) test_pred = model.predict(test_x, num_iteration=model.best_iteration) val_y = np.array(val_y).reshape(-1, 1) val_y = onehot_encoder.fit_transform(val_y) print(\u0026#39;预测的概率矩阵：\u0026#39;) print(test_pred) test += test_pred #验证集计算训练误差 score = loss(val_y, val_pred) cv_scores.append(score) print(cv_scores) print(\u0026#34;%s_scotrainre_list:\u0026#34; % clf_name, cv_scores) print(\u0026#34;%s_score_mean:\u0026#34; % clf_name, np.mean(cv_scores)) print(\u0026#34;%s_score_std:\u0026#34; % clf_name, np.std(cv_scores)) #i个模型输出结果的平均值。 test = test / 10 return test 调用模型的函数定义 def lgb_model(x_train, y_train, x_test): lgb_test = cv_model(lgb, x_train, y_train, x_test, \u0026#34;lgb\u0026#34;) return lgb_test def loss(y_p,y_t): y_p=np.array(y_p) y_t=np.array(y_t) loss=sum(sum(abs(y_p-y_t))) return loss lgb_test = lgb_model(X_train, y_train, X_test) ","permalink":"https://fishdel.github.io/zh/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98lightgbm/","summary":"\u003ch2 id=\"lightgbm\"\u003eLightGBM\u003c/h2\u003e\n\u003ch3 id=\"介绍\"\u003e介绍\u003c/h3\u003e\n\u003cp\u003eLightGBM（Light Gradient Boosting Machine）：一个实现GBDT算法的框架，解决GBDT在海量数据遇到的问题。\u003c/p\u003e\n\u003ch5 id=\"两大技术\"\u003e两大技术：\u003c/h5\u003e\n\u003cp\u003e（1）GOSS(Gradient-based One-Side Sampling)：减少样本数\u003c/p\u003e","title":"机器学习实战(LightGBM)"},{"content":"集成学习与随机森林 更新权重 Adaboost AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None)\nbase_estimator:可选参数，默认为DecisionTreeClassifier。 algorithm： 可选参数，默认为SAMME.R 循环训练，实例权重不断更新（不是是成本函数最小化，而是加入更多预测器）\nGradient Boosting 新预测器针对前一个预测器的残差进行拟合\nGradientBoostingRegressor(max_depth=2,n_estimators=3,learning_rate=1.0,random_state=42)\n提前停止法\n训练完之后测量每个阶段的训练验证误差，找到树的最优数量后重新训练\nerrors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\nbst_n_estimators = np.argmin(errors) + 1\n验证误差在连续某次未改善时停止训练\nxgboost xgbc = XGBClassifier(max_depth=2, learning_rate=1, n_estimators=2, # number of iterations or number of trees slient=0, objective=\u0026#34;binary:logistic\u0026#34; ) 不更新权重 投票分类器 基于多分类器的结果聚合\nvoting_clf = VotingClassifier(estimators=[ (\u0026rsquo;log_clf\u0026rsquo;, LogisticRegression()), (\u0026lsquo;svm_clf\u0026rsquo;, SVC(probability=True)), (\u0026lsquo;dt_clf\u0026rsquo;, DecisionTreeClassifier(random_state=10)), ], voting=\u0026lsquo;soft\u0026rsquo;) voting_clf.fit(X_train, y_train) voting_clf.score(X_test, y_test)\nbagging./pasting 有放回抽样。在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。\n1.通过设置参数 bootstrap=False来切换为无放回采样。 2.n_estimators=500，表示有有500个相同的决策器。 3.max_samples=100，表示在数据集上有放回采样 100 个训练实例。 4.n_jobs=-1，n_jobs 参数告诉 sklearn 用于训练和预测所需要 CPU 核的数量。（-1 代表着 sklearn 会使用所有空闲核） 5.oob_score=True，表示包外评估bag_clf.oob_score_ 随机森林\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n重要参数\nn_estimators，random_state，boostrap和oob_score 重要属性\n.estimators_ .oob_score_ .feature_importances_ 接口\napply，fit，predict，score和predict_proba ","permalink":"https://fishdel.github.io/zh/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/","summary":"\u003ch1 id=\"集成学习与随机森林\"\u003e集成学习与随机森林\u003c/h1\u003e\n\u003ch2 id=\"更新权重\"\u003e更新权重\u003c/h2\u003e\n\u003ch3 id=\"adaboost\"\u003eAdaboost\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAdaBoostClassifier(base_estimator=None, n_estimators=50,\nlearning_rate=1.0, algorithm=’SAMME.R’,\nrandom_state=None)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebase_estimator:可选参数，默认为DecisionTreeClassifier。\u003c/li\u003e\n\u003cli\u003ealgorithm： 可选参数，默认为SAMME.R\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e循环训练，实例权重不断更新（不是是成本函数最小化，而是加入更多预测器）\u003c/p\u003e","title":"机器学习实战(集成学习与随机森林)"},{"content":"降维 主要动机 加速，可视化数据，节省空间\n缺点：丢失信息，计算密集，转换过程难以理解\n什么时候用到降维 分类前，加速；聚类前，可视化数据\n维度诅咒 高维数据集——非常稀疏——训练实例彼此远离——容易过拟合\n主要动机：特征过多，训练变得缓慢，难以寻找更好的解决方案\n例子：MNIST数据集中图像边界的像素全是白色，删除这些像素也不会丢失太多信息，或者相邻两个像素合并。\n降维的主要方法 投影：适用于一个分布接近于2D子空间的3D数据集\n流形学习：瑞士卷数据集\n投影 投影的方法：PCA （主成分分析）寻找训练集中可获得最大方差的轴。\n如何寻找训练集的主成分 奇异值分解（SVD）\n使用numpy提供的svd（）函数获得训练集的主成分 如果不是用sklearn的PCA类，就要将数据集做中心化处理.\nX_centered=X-X.mean(axis=0)` `U,s,V=np.linalg.svd(X_centered)` `c1=V.T[:,0]` `c2=V.T[:,1] 有了奇异值分解得到的V，任意选取前d个主成分点乘原始数据集便可实现在d维的投影。\nW2 = V.T[:,:2]` `X2D=X_centered.dot(W2) sklearn实现PCA from sklearn.decomposition import PCA` `pca=PCA(n_components=2)` `X2D=pca.fit_transform(X) 使用components_访问每一个主成分\n`pca.components_.T[:,0]` 方差解释率 pca.explained_variance_ratio_` `pca = PCA(n_components=0.95) 可以将 n_components 设置为 0.0 到 1.0 之间的浮点数，表明希望保留的方差比率 cumsum = np.cumsum(pca.explained_variance_ratio_)` `d = np.argmax(cumsum \u0026gt;= 0.95)+1 也可以保留相加足够大的方差部分维度\n也可以画图寻找拐点\nplt.plot(cumsum,linewidth=3)\nQ：假设你对一个 1000 维的数据集应用 PCA，同时设置方差解释率为 95%，你的最终数据集将会有多少维？\n可能是1-1000之间的任何数字，取决于数据集，\nPCA数据压缩 `pca = PCA(n_components=154)` `X_reduced = pca.fit_transform(X_train)` `X_recovered = pca.inverse_transform(X_reduced)` 几种不同的PCA 随机PCA `rnd_pca = PCA(n_components=154,svd_solver=\u0026#39;randomized\u0026#39;,random_state=42)` `X_reduced = rnd_pca.fit_transform(X_train)` 增量PCA：对于大型数据集可以划分成小批量，但是要在每个小批量里调用partial_fit方法 from sklearn.decomposition import IncrementalPCA` `n_batches = 100` `inc_pca = IncrementalPCA(n_components=154)` `for X_batch in np.array_split(X_train,n_batches):` `inc_pca.partial_fit(X_batch)` `X_reduced = inc_pca.transform(X_train)` Kernel PCA `from sklearn.decomposition import KernelPCA rbf_pca=KernelPCA(n_components=2,kernel=\u0026#39;rbf\u0026#39;,gamma=0.04) X_reduced=rbf_pca.fit_transform(X)` 为kPCA调参方法 引入模型，通过最优化模型表现调参\n`from sklearn.model_selection import GridSearchCV` `from sklearn.linear_model import LogisticRegression` `from sklearn.pipeline import Pipeline` `clf = Pipeline([` `(\u0026#34;kpca\u0026#34;,KernelPCA(n_components=2)),` `(\u0026#34;log_reg\u0026#34;,LogisticRegression(solver=\u0026#34;lbfgs\u0026#34;))` `])` `param_grid = [{` `\u0026#34;kpca__gamma\u0026#34;:np.linspace(0.03,0.05,10), \u0026#34;kpca__kernel\u0026#34;:[\u0026#34;rbf\u0026#34;,\u0026#34;sigmoid\u0026#34;]` `}]` `grid_search = GridSearchCV(clf,param_grid,cv=3)` `grid_search.fit(X,y)` 2.基于重建功能算误差\n`best_score = 0.0` `for gamma in np.linspace(0.01, 0.05, 10):` `for kernel in [\u0026#34;rbf\u0026#34;, \u0026#34;sigmoid\u0026#34;]:` `kpca = KernelPCA(n_components=27,fit_inverse_transform=True)` `X_reduced = kpca.fit_transform(X)` `X_preimage = kpca.inverse_transform(X_reduced )` score = mean_squared_error(X, X_preimage) if score \u0026gt; best_score: best_score = score best_parameters = {\u0026#39;gamma\u0026#39;:gamma,\u0026#39;kernel\u0026#39;:kernel} print(best_parameters,best_score) 各种PCA的选择 首选常规PCA，不适合内存的大型数据集用增量PCA，想要大幅度降低维度并且追求速度用随机PCA，非线性数据集用内核PCA\n流形学习 LLE from sklearn.manifold import LocallyLinearEmbedding lle=LocallyLinearEmbedding(n_components=2,n_neighbors=10) X_reduced=lle.fit_transform(X) ","permalink":"https://fishdel.github.io/zh/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E9%99%8D%E7%BB%B4/","summary":"\u003ch2 id=\"降维\"\u003e降维\u003c/h2\u003e\n\u003ch3 id=\"主要动机\"\u003e主要动机\u003c/h3\u003e\n\u003cp\u003e加速，可视化数据，节省空间\u003c/p\u003e\n\u003cp\u003e缺点：丢失信息，计算密集，转换过程难以理解\u003c/p\u003e\n\u003ch3 id=\"什么时候用到降维\"\u003e什么时候用到降维\u003c/h3\u003e\n\u003cp\u003e分类前，加速；聚类前，可视化数据\u003c/p\u003e\n\u003ch3 id=\"维度诅咒\"\u003e维度诅咒\u003c/h3\u003e\n\u003cp\u003e高维数据集——非常稀疏——训练实例彼此远离——容易过拟合\u003c/p\u003e","title":"机器学习实战(降维)"},{"content":"决策树 分类树 八个重要参数 criterion： 决定不纯度的计算方法： 1）”entropy“，使用信息熵（Entropy） 2）”gini“，使用基尼系数（Gini Impurity）\n信息熵对不纯度更加敏感，对不纯度的惩罚最强，计算更复杂 splitter：输入“best”优先选择更重要的分支进行分类；输入“random”更加随机防止过拟合\n剪枝策略参数：\nmax_depth:一般为3 min_samples_leaf：一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发。类别不多选1，一般为5 min_samples_split限定，一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。 max_features ：限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃 min_impurity_decrease限制信息增益的大小，信息增益小于设定数值的分枝不会发生。 可视化方法 import graphviz dot_data = tree.export_graphviz(clf #训练好的模型 ,out_file = None ,feature_names= feature_name ,class_names=[\u0026ldquo;琴酒\u0026rdquo;,\u0026ldquo;雪莉\u0026rdquo;,\u0026ldquo;贝尔摩德\u0026rdquo;] ,filled=True #进行颜色填充 ,rounded=True #树节点的形状控制 ) graph = graphviz.Source(dot_data) graph 一个属性四个接口 属性：feature_importances_，能够查看各个特征对模型的重要性\nfit（）predict（）apply（）score（）\napply中输入测试集返回每个测试样本所在的叶子节点的索引\n基本流程 from sklearn import tree #导入需要的模块 clf = tree.DecisionTreeClassifier() #实例化模型对象 clf = clf.fit(X_train,y_train) #用训练集数据训练模型 result = clf.score(X_test,y_test) 计算全部特征的不纯度指标-\u0026gt;选择不纯度指标最优的特征来分支-\u0026gt;在第一个特征下计算不纯度-\u0026gt;选取不纯度指标继续分支（直到没有更多特征可用或者整体不纯度达到最优）\n所有接口中要求输入X_train和X_test的部分，输入的特征矩阵必须至少是一个二维矩阵。如果你的数据的确只有一个特征，那必须用reshape(-1,1)来给矩阵增维；如果你的数据只有一个特征和一个样本，使用reshape(1,-1)来给你的数据增维。\n回归树 重要参数，属性和接口 criterion：mse，friedman_mse,mae 属性接口如上。接口score返回的是R平方，并不是MSE if xgboost is not None: # not shown in the book` xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2) y_pred = xgb_reg.predict(X_val) val_error = mean_squared_error(y_val, y_pred) print(\u0026#34;Validation MSE:\u0026#34;, val_error) ","permalink":"https://fishdel.github.io/zh/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%E5%86%B3%E7%AD%96%E6%A0%91/","summary":"\u003ch1 id=\"决策树\"\u003e决策树\u003c/h1\u003e\n\u003ch2 id=\"分类树\"\u003e分类树\u003c/h2\u003e\n\u003ch3 id=\"八个重要参数\"\u003e八个重要参数\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ecriterion：\n决定不纯度的计算方法：\n1）”entropy“，使用信息熵（Entropy）\n2）”gini“，使用基尼系数（Gini Impurity）\u003c/p\u003e","title":"机器学习实战(决策树)"},{"content":"训练模型 训练模型的方法 使用成本函数最小的参数 标准方程法 SVD奇异值分解 迭代优化，使用梯度下降 批量梯度下降 随机梯度下降 小批量梯度下降 训练模型方法的问题 训练集有数百万特征：（使用迭代优化）随机梯度下降和小批量梯度下降，若训练集可以容纳于内存，使用批量梯度下降\n训练集里特征的数值大小迥异 先放缩，再使用梯度下降 使用标准方程法或SVD 使用批量梯度下降，并在每个轮次绘制验证误差。如果验证误差持续上升 如果训练错误增加：学习率过高，如果训练错误没增加，可能过拟合——简化模型 使用多项式回归时，训练误差和验证误差之间差距大 验证误差远高于训练误差\n过拟合\n减少多项式阶数 添加L1或l2至成本函数 增加训练集的大小 使用岭回归时，训练误差和验证误差差不多且都相当高 欠拟合，减少正则化参数alpha 两种用于分类的模型 Logistic回归 对数几率回归：把线性回归的结果，通过sigmoid函数，从(-∞,∞)映射到(0,1) 成本函数为凸函数，梯度下降不会陷入局部最优 Softmax回归 逻辑回归只能用于二分类，通过softmax函数扩展到多分类问题 成本函数为交叉熵成本函数 多项式回归（可能会过拟合） 处理非线性关系的数据：将每个特征的幂次方添加为一个新特征，在扩展的特征集上训练一个线性模型 poly_features = PolynomialFeatures(degree=2, include_bias=False) “斜率”参数（w，也叫作权重或系数）被保存在 coef_ 属性中，而偏移或截距（b）被保 存在 intercept_ 属性中 若有（a，b）两个特征，使用degree=2的二次多项式则为（1，a, a^2, ab, b ,b^2)。 参数：\ndegree：度数，决定多项式的次数\ninteraction_only： 默认为False，字面意思就是只能交叉相乘，不能有a^2这种.\ninclude_bias: 默认为True, 这个bias指的是多项式会自动包含1，设为False就没这个1了\n欠拟合——两条曲线接近且都很高 过拟合——曲线之间存在很大的间隙\n正则化 正则化线性模型 岭回归\nalpha：正则化系数，float类型，默认为1.0。 fit_intercept：是否需要截距b，默认为True。 normalize：是否先进行归一化，默认为False。 copy_X：是否复制X数组，否则覆盖，默认为True。 max_iter：最大的迭代次数，int类型，默认为None。 solver：求解方法，str类型，默认为auto。可选参数为：auto、svd、cholesky、lsqr、sparse_cg、sag。\nridge_reg =Ridge(alpha=1, solver=\u0026ldquo;cholesky\u0026rdquo;, random_state=42)\nsgd_reg = SGDRegressor(penalty=\u0026lsquo;l2\u0026rsquo;,max_iter=50,tol=-np.infty,random_state=42)\n也可以这样用 Lasso回归\nlasso_reg = Lasso(alpha=0.1)\nSGDRegressor(penalty=\u0026lsquo;l1\u0026rsquo;)\n经常把特征的权重降低为0，只有很少的特征重要时选他\n弹性网络\nelastic_net = ElasticNet(alpha=0.1,l1_ratio=0.5,random_state=42) 二者的一个混合，容易产生一些异常 早期停止法\nsgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None, learning_rate=\u0026ldquo;constant\u0026rdquo;, eta0=0.0005, random_state=42)\n当 warm_start=True 时，调用 fit() 方法后，训练会从停下来的地方继续，而不是从头重新开始\n正则化 正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。损失函数由原来的E(X,Y)变为E(X,Y)+alpha||w||，w是模型系数组成的向量（有些地方也叫参数parameter，coefficients），||·||一般是L1或者L2范数，alpha是一个可调的参数，控制着正则化的强度。当用在线性模型上时，L1正则化和L2正则化也称为Lasso和Ridge。\n","permalink":"https://fishdel.github.io/zh/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/","summary":"\u003ch1 id=\"训练模型\"\u003e训练模型\u003c/h1\u003e\n\u003ch2 id=\"训练模型的方法\"\u003e训练模型的方法\u003c/h2\u003e\n\u003ch3 id=\"使用成本函数最小的参数\"\u003e使用成本函数最小的参数\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e标准方程法\u003c/li\u003e\n\u003cli\u003eSVD奇异值分解\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"迭代优化使用梯度下降\"\u003e迭代优化，使用梯度下降\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e批量梯度下降\u003c/li\u003e\n\u003cli\u003e随机梯度下降\u003c/li\u003e\n\u003cli\u003e小批量梯度下降\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"训练模型方法的问题\"\u003e训练模型方法的问题\u003c/h2\u003e\n\u003cp\u003e训练集有数百万特征：（使用迭代优化）随机梯度下降和小批量梯度下降，若训练集可以容纳于内存，使用批量梯度下降\u003c/p\u003e","title":"机器学习实战(训练模型)"},{"content":"支持向量机 线性SVM分类 硬间隔分类：让所有实例都在正确的一边的分类。 硬间隔变成软间隔：引入松弛变量C。 C是调节间隔与准确率的因子，C值越大，越不愿放弃那些离群点；c值越小，越不重视那些离群点。（模型过拟合，C值调小进行正则化）\n软间隔分类：在“街道”的宽度和间隔违例（错误分类）之间找到良好的平衡的分类。 svm_clf = SVC(kernel=\u0026ldquo;linear\u0026rdquo;, C=float(\u0026ldquo;inf\u0026rdquo;)) 线性核函数\n非线性SVM分类 通过非线性变换，将非线性问题变为线性问题 添加特征\n指定kernel=“ploy”其中γ、r、d属于超参，需要调参定义 添加相似特征\n指定kernel=“rbf”，其中γ属于超参，要求大于0，需要调参定义（过拟合降低，欠拟合提升）\n增加gamma值，钟形曲线变得更窄会变得更窄，每个实例的影响范围更小，减小gamma变的更平坦\n核函数的选取 高维用线性，不行换特征；低维试线性，不行换高斯 SVM回归 参数 参数 解释 C 惩罚项参数 loss 损失函数。当值为epsilon_insensitive时损失函数为L（它是标准SVR的损失函数）；值为square_epsilon_insensitive时表示为L的平方 epsilon 浮点数，用于loss中的sigma参数 dual 布尔值。如果为True，则解决对偶问题，如果为False，则解决原始问题，当n_samples\u0026gt;n_features时，倾向于采用False tol 浮点数，指定终止迭代的阈值 fit_intercept 布尔值，如果为True，则计算截距，即决策函数中的常数项；否则忽略截距 属性 coef_：一个数组，给出了各个特征的权重。\nintercept_：一个数组，隔出了截距，即决定函数中的常数项。\n方法 fit（x, [,y]）:训练模型。 predict（x）:用模型进行预测，返回预测值 score（x,y[,sample_weight]）:返回(x,y)上预测的准确率 基本思想 在类之间拟合可能最宽的街道，寻找最大的决策边界 支持向量：决策边界位于“街道”边缘的实例 放缩的原因：支持向量机拟合类别之间可能的、最宽的“街道”，所以如果训练集不经缩放，SVM将趋于忽略值较小的特征。 当训练实例的数量小于特征数量时，解决对偶问题比原始问题更迅速。 Sklearn构建的SVM分类器 参数 解释 C 惩罚项 kernel 核函数类型，str类型，默认为’rbf’ degree 多项式核函数的阶数，int类型，可选参数，默认为3 gamma 核函数系数，float类型，可选参数，默认为auto。只对’rbf’ ,’poly’ ,’sigmod’有效。如果gamma为auto，代表其值为样本特征数的倒数，即1/n_features。 coef0 核函数中的独立项，float类型，可选参数，默认为0.0。只有对’poly’ 和,’sigmod’核函数有用，是指其中的参数c。 ","permalink":"https://fishdel.github.io/zh/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","summary":"\u003ch1 id=\"支持向量机\"\u003e支持向量机\u003c/h1\u003e\n\u003ch2 id=\"线性svm分类\"\u003e线性SVM分类\u003c/h2\u003e\n\u003ch3 id=\"硬间隔分类让所有实例都在正确的一边的分类\"\u003e硬间隔分类：让所有实例都在正确的一边的分类。\u003c/h3\u003e\n\u003ch3 id=\"硬间隔变成软间隔引入松弛变量c\"\u003e硬间隔变成软间隔：引入松弛变量C。\u003c/h3\u003e\n\u003cp\u003eC是调节间隔与准确率的因子，C值越大，越不愿放弃那些离群点；c值越小，越不重视那些离群点。（模型过拟合，C值调小进行正则化）\u003c/p\u003e\n\u003ch3 id=\"软间隔分类在街道的宽度和间隔违例错误分类之间找到良好的平衡的分类\"\u003e软间隔分类：在“街道”的宽度和间隔违例（错误分类）之间找到良好的平衡的分类。\u003c/h3\u003e\n\u003cp\u003esvm_clf = SVC(kernel=\u0026ldquo;linear\u0026rdquo;, C=float(\u0026ldquo;inf\u0026rdquo;))\n线性核函数\u003c/p\u003e","title":"机器学习实战(支持向量机)"},{"content":"","permalink":"https://fishdel.github.io/about/","summary":"about","title":"关于"}]